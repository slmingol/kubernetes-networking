{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kubernetes Networking \u00b6 The Kubernetes Networking series consists of the following topics: Kubernetes Networking 101 (60 mins), you will use different ways to control traffic on a Kubernetes cluster with Service types, Ingress, Network Policy and Calico. Start here . Add Security to Routes on OpenShift (coming soon), you will use different types of TLS termination to secure Routes on OpenShift: edge, passthrough and reencrypt. Start here . Kubernetes Network Security using a Virtual Private Cloud (VPC) (90 mins), you will deploy a guestbook application to a Kubernetes cluster in a Virtual Private Cloud (VPC) Gen2, you will create the VPC, add a subnet, attach a public gateway, and update a security group with rules to allow inbound traffic to the guestbook application. Start here . Istio , use Istio to manage network traffic, load balance across microservices, enforce access policies, verify service identity, and more. This series of workshops on Kubernetes Networking is accompanied by a lecture on Kubernetes Networking . Kubernetes Networking is part of the series Kubernetes Security , which includes: Kubernetes Security, Kubernetes Networking, Kubernetes Storage, Kubernetes Automation (DevOps, IaC, CI/CD). Next steps \u00b6 Continue your learning by visiting the Istio workshop . With Istio, you can manage network traffic, load balance across microservices, enforce access policies, verify service identity, and more. Prerequirementss \u00b6 Free IBM Cloud account, to create a new IBM Cloud account go here . Free Pay-As-You-Go account. To upgrade a free IBM Cloud account, go here . CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, go here . Kubernetes standard cluster v1.18 with at least 2 worker nodes on a VLAN, a subnet with public IPs, external LoadBalancer (for details about VLAN, subnets and IPs, see here ). Go to Setup for more details. Labs \u00b6 Lab1 Kubernetes Networking 101 Setup Services ClusterIP NodePort Loadbalancer NLB ExternalName Ingress ALB Lab2 Secured Routes Secured Routes and TLS Termination Lab3 Network Policies Network Policy and Calico Lab4 Secure a Cluster using VPC Setup VPC Gen2 Technologies \u00b6 This workshop was tested using the following technologies: IBM Cloud Kubernetes Service (IKS) version 1.19, 2 worker nodes, flavor u3c.2x4 Calico client version v3.17.1 Calico cluster version v3.16.5 ibmcloud version 1.3.0 ibmcloud container-service/kubernetes-service 1.0.28 vpc-infrastructure/infrastructure-service 0.7.5 kubectl version 1.19 Contributors \u00b6 Remko de Knikker, remkohdev , Masa Abushamleh, nerdingitout Tim Robinson, timroster ,","title":"About the workshop"},{"location":"#kubernetes-networking","text":"The Kubernetes Networking series consists of the following topics: Kubernetes Networking 101 (60 mins), you will use different ways to control traffic on a Kubernetes cluster with Service types, Ingress, Network Policy and Calico. Start here . Add Security to Routes on OpenShift (coming soon), you will use different types of TLS termination to secure Routes on OpenShift: edge, passthrough and reencrypt. Start here . Kubernetes Network Security using a Virtual Private Cloud (VPC) (90 mins), you will deploy a guestbook application to a Kubernetes cluster in a Virtual Private Cloud (VPC) Gen2, you will create the VPC, add a subnet, attach a public gateway, and update a security group with rules to allow inbound traffic to the guestbook application. Start here . Istio , use Istio to manage network traffic, load balance across microservices, enforce access policies, verify service identity, and more. This series of workshops on Kubernetes Networking is accompanied by a lecture on Kubernetes Networking . Kubernetes Networking is part of the series Kubernetes Security , which includes: Kubernetes Security, Kubernetes Networking, Kubernetes Storage, Kubernetes Automation (DevOps, IaC, CI/CD).","title":"Kubernetes Networking"},{"location":"#next-steps","text":"Continue your learning by visiting the Istio workshop . With Istio, you can manage network traffic, load balance across microservices, enforce access policies, verify service identity, and more.","title":"Next steps"},{"location":"#prerequirementss","text":"Free IBM Cloud account, to create a new IBM Cloud account go here . Free Pay-As-You-Go account. To upgrade a free IBM Cloud account, go here . CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, go here . Kubernetes standard cluster v1.18 with at least 2 worker nodes on a VLAN, a subnet with public IPs, external LoadBalancer (for details about VLAN, subnets and IPs, see here ). Go to Setup for more details.","title":"Prerequirementss"},{"location":"#labs","text":"Lab1 Kubernetes Networking 101 Setup Services ClusterIP NodePort Loadbalancer NLB ExternalName Ingress ALB Lab2 Secured Routes Secured Routes and TLS Termination Lab3 Network Policies Network Policy and Calico Lab4 Secure a Cluster using VPC Setup VPC Gen2","title":"Labs"},{"location":"#technologies","text":"This workshop was tested using the following technologies: IBM Cloud Kubernetes Service (IKS) version 1.19, 2 worker nodes, flavor u3c.2x4 Calico client version v3.17.1 Calico cluster version v3.16.5 ibmcloud version 1.3.0 ibmcloud container-service/kubernetes-service 1.0.28 vpc-infrastructure/infrastructure-service 0.7.5 kubectl version 1.19","title":"Technologies"},{"location":"#contributors","text":"Remko de Knikker, remkohdev , Masa Abushamleh, nerdingitout Tim Robinson, timroster ,","title":"Contributors"},{"location":"networking-extra/","text":"Kubernetes Networking Extra \u00b6 Kubernetes Networking in-depth \u00b6 The following image demonstrates how Kubernetes forwards public network traffic through kube-proxy and NodePort, LoadBalancer, or Ingress services in IBM Cloud Kubernetes Service. The following image shows what features each of the different ServiceTypes supports on IBM Cloud. The services network is implemented by a kubernetes component called kube-proxy collaborating with a linux kernel module called netfilter to trap and reroute traffic sent to the cluster IP so that it is sent to a healthy pod instead Connections and requests operate at OSI layer 4 (tcp) or layer 7 (http, rpc, etc). Netfilter rules are routing rules, and they operate on IP packets at layer 3. All routers, including netfilter, make routing decisions based more or less solely on information contained in the packet; generally where it is from and where it is going. So to describe this behavior in layer 3 terms: each packet destined for the service at10.3.241.152:80 that arrives at a node\u2019s eth0 interface is processed by netfilter, matches the rules established for our service, and is forwarded to the IP of a healthy pod. External clients that call into our pods has to make use of this same routing infrastructure. The cluster IP and port is the \u201cfront end\u201d. The cluster IP of a service is only reachable from a node\u2019s ethernet interface. How can we forward traffic from a publicly visible IP endpoint to an IP that is only reachable once the packet is already on a node? One way is to examine the netfilter rules using the iptables utility. Any packet from anywhere that arrives on the node\u2019s ethernet interface with a destination of 10.3.241.152:80 is going to match and get routed to a pod. So you could give clients the cluster IP or a friendly domain name, and then add a route to get the packets to one of the nodes. But routers operating on layer 3 packets don\u2019t know healthy services from unhealthy. Kube-proxy\u2019s role is to actively manage netfilter. We can\u2019t easily create a stable static route between the gateway router and the nodes using the service network (cluster IP). Kube-Proxy \u00b6 Kubernetes relies on proxying to forward inbound traffic to backends. When you use the name of the service, it looks up the name in the cluster DNS provider and routes the request to the in-cluster IP address of the service. A Kubernetes cluster runs a local Kubernetes network proxy, kube-proxy , as a daemon on each worker node in the kube-system namespace, which provides basic load balancing for services. The default load balancing mode in Kubernetes is iptables and rules, a Linux kernel feature, to direct requests to the pods behind a service equally. The native method for load distribution in iptables mode is random selection . An older kube-proxy mode is userspace , which uses round-robin load distribution, allocating the next available pod on an IP list, then rotating the list. Proxy modes: userspace iptables (default) (NLB 1.0 uses iptables) IPVS (NLB 2.0 uses IP Virtual Server (IPVS)) DNS \u00b6 A cluster-aware DNS server, such as CoreDNS , watches the Kubernetes API for new Services and creates a set of DNS records for each service. If DNS has been enabled throughout your cluster then all Pods should automatically be able to resolve Services by their DNS name. Kubernetes also supports DNS SRV (DNS Service) records for named ports. If the \"my-service.my-ns\" Service has a port named \"http\" with the protocol set to TCP, you can do a DNS SRV query for _http._tcp.my-service.my-ns to discover the port number for \"http\", as well as the IP address. The Kubernetes DNS server is the only way to access ExternalName Services. Kubernetes offers a DNS cluster addon, which most of the supported environments enable by default. In Kubernetes version 1.11 and later, CoreDNS is recommended and is installed by default with kubeadm. To verify if the CoreDNS deployment is available, kubectl get deployment -n kube-system | grep dns coredns 3 /3 3 3 7h51m coredns-autoscaler 1 /1 1 1 7h51m kubectl get configmap -n kube-system | grep dns coredns 1 11d coredns-autoscaler 1 11d node-local-dns 1 11d The node-local-dns is the NodeLocal DNSCaching agent for improved cluster DNS performance. NodeLocal DNSCache is a feature introduced as stable in v1.18. Next, go back to continue. Resources \u00b6 Choosing an app exposure service ExternalName Service supported protocols","title":"Kubernetes Networking Extra"},{"location":"networking-extra/#kubernetes-networking-extra","text":"","title":"Kubernetes Networking Extra"},{"location":"networking-extra/#kubernetes-networking-in-depth","text":"The following image demonstrates how Kubernetes forwards public network traffic through kube-proxy and NodePort, LoadBalancer, or Ingress services in IBM Cloud Kubernetes Service. The following image shows what features each of the different ServiceTypes supports on IBM Cloud. The services network is implemented by a kubernetes component called kube-proxy collaborating with a linux kernel module called netfilter to trap and reroute traffic sent to the cluster IP so that it is sent to a healthy pod instead Connections and requests operate at OSI layer 4 (tcp) or layer 7 (http, rpc, etc). Netfilter rules are routing rules, and they operate on IP packets at layer 3. All routers, including netfilter, make routing decisions based more or less solely on information contained in the packet; generally where it is from and where it is going. So to describe this behavior in layer 3 terms: each packet destined for the service at10.3.241.152:80 that arrives at a node\u2019s eth0 interface is processed by netfilter, matches the rules established for our service, and is forwarded to the IP of a healthy pod. External clients that call into our pods has to make use of this same routing infrastructure. The cluster IP and port is the \u201cfront end\u201d. The cluster IP of a service is only reachable from a node\u2019s ethernet interface. How can we forward traffic from a publicly visible IP endpoint to an IP that is only reachable once the packet is already on a node? One way is to examine the netfilter rules using the iptables utility. Any packet from anywhere that arrives on the node\u2019s ethernet interface with a destination of 10.3.241.152:80 is going to match and get routed to a pod. So you could give clients the cluster IP or a friendly domain name, and then add a route to get the packets to one of the nodes. But routers operating on layer 3 packets don\u2019t know healthy services from unhealthy. Kube-proxy\u2019s role is to actively manage netfilter. We can\u2019t easily create a stable static route between the gateway router and the nodes using the service network (cluster IP).","title":"Kubernetes Networking in-depth"},{"location":"networking-extra/#kube-proxy","text":"Kubernetes relies on proxying to forward inbound traffic to backends. When you use the name of the service, it looks up the name in the cluster DNS provider and routes the request to the in-cluster IP address of the service. A Kubernetes cluster runs a local Kubernetes network proxy, kube-proxy , as a daemon on each worker node in the kube-system namespace, which provides basic load balancing for services. The default load balancing mode in Kubernetes is iptables and rules, a Linux kernel feature, to direct requests to the pods behind a service equally. The native method for load distribution in iptables mode is random selection . An older kube-proxy mode is userspace , which uses round-robin load distribution, allocating the next available pod on an IP list, then rotating the list. Proxy modes: userspace iptables (default) (NLB 1.0 uses iptables) IPVS (NLB 2.0 uses IP Virtual Server (IPVS))","title":"Kube-Proxy"},{"location":"networking-extra/#dns","text":"A cluster-aware DNS server, such as CoreDNS , watches the Kubernetes API for new Services and creates a set of DNS records for each service. If DNS has been enabled throughout your cluster then all Pods should automatically be able to resolve Services by their DNS name. Kubernetes also supports DNS SRV (DNS Service) records for named ports. If the \"my-service.my-ns\" Service has a port named \"http\" with the protocol set to TCP, you can do a DNS SRV query for _http._tcp.my-service.my-ns to discover the port number for \"http\", as well as the IP address. The Kubernetes DNS server is the only way to access ExternalName Services. Kubernetes offers a DNS cluster addon, which most of the supported environments enable by default. In Kubernetes version 1.11 and later, CoreDNS is recommended and is installed by default with kubeadm. To verify if the CoreDNS deployment is available, kubectl get deployment -n kube-system | grep dns coredns 3 /3 3 3 7h51m coredns-autoscaler 1 /1 1 1 7h51m kubectl get configmap -n kube-system | grep dns coredns 1 11d coredns-autoscaler 1 11d node-local-dns 1 11d The node-local-dns is the NodeLocal DNSCaching agent for improved cluster DNS performance. NodeLocal DNSCache is a feature introduced as stable in v1.18. Next, go back to continue.","title":"DNS"},{"location":"networking-extra/#resources","text":"Choosing an app exposure service ExternalName Service supported protocols","title":"Resources"},{"location":"archive/hello-java-app/","text":"How the HelloWorld App was Created \u00b6 Do not The following step is only explaining how the HelloWorld application was created. The Kubernetes resources will actually pull an existing image from Docker Hub. Create the Spring Boot scaffolding, $ spring init --dependencies=web,data-rest,thymeleaf helloworld-api $ cd helloworld-api $ mvn clean install $ mvn test $ mvn spring-boot:run Create the APIController, $ echo 'package com.example.helloworldapi; import java.util.List; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.ResponseBody; import org.springframework.web.bind.annotation.RestController; import org.springframework.http.MediaType; import org.springframework.http.ResponseEntity; import org.springframework.http.HttpStatus; @RestController public class APIController { @Autowired private MessageRepository repository; @GetMapping(\"/api\") public String index() { return \"Welcome to Spring Boot App\"; } @RequestMapping(value = \"/api/hello\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE) @ResponseBody public String hello(@RequestParam String name) { String message = \"Hello \"+ name; String responseJson = \"{ \\\"message\\\" : \\\"\"+ message + \"\\\" }\"; repository.save(new Message(name, message)); for (Message msg : repository.findAll()) { System.out.println(msg); } return responseJson; } @RequestMapping(value = \"/api/messages\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE) @ResponseBody public ResponseEntity<List<Message>> getMessages() { List<Message> messages = (List<Message>) repository.findAll(); return new ResponseEntity<List<Message>>(messages, HttpStatus.OK); } } ' > src/main/java/com/example/helloworldapi/APIController.java Create the Message class, $ echo 'package com.example.helloworldapi; import org.springframework.data.annotation.Id; public class Message { @Id private String id; private String sender; private String message; public Message(String sender, String message) { this.sender = sender; this.message = message; } public String getId() { return id; } public String getSender() { return sender; } public String getMessage() { return message; } @Override public String toString() { return \"Message [id=\" + id + \", sender=\" + sender + \", message=\" + message + \"]\"; } }' > src/main/java/com/example/helloworldapi/Message.java Create the MessageRepository class, echo 'package com.example.helloworldapi; import java.util.List; import org.springframework.data.mongodb.repository.MongoRepository; public interface MessageRepository extends MongoRepository<Message, String> { public List<Message> findBySender(String sender); }' > src/main/java/com/example/helloworldapi/MessageRepository.java Add a new file \u2018~/src/main/resources/application.properties\u2019, whose configuration should match those when the MongoDB server was deployed, echo 'pring.data.mongodb.username=user1 spring.data.mongodb.password=passw0rd spring.data.mongodb.database=mydb spring.data.mongodb.port=27017 spring.data.mongodb.host=mongodb.default' > src/main/resources/application.properties Add the following dependency to the Maven build file in pom.xml , <dependencies> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-data-mongodb</artifactId> </dependency> </dependencies> Test Java App on localhost: \u00b6 Create local Mongodb, docker run --name mongo -d -p 27017:27017 -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=passw0rd mongo Get IP address on mac osx, $ ifconfig en0 | grep inet 192.168.86.45 Configure application.properties and change the host to your local IP Address to test the app on localhost. On the localhost we did not create a separate user, but you can use the admin user. spring.data.mongodb.username=user1 spring.data.mongodb.password=passw0rd spring.data.mongodb.database=mydb spring.data.mongodb.port=27017 spring.data.mongodb.host=192.168.86.45 Run the application $ mvn clean install $ mvn spring-boot:run test $ curl -X GET 'http://192.168.86.45:8080/api/hello?name=one' { \"message\" : \"Hello one\" } Dockerize \u00b6 echo 'FROM openjdk:8-jdk-alpine ARG JAR_FILE=target/*.jar COPY ${JAR_FILE} app.jar EXPOSE 8080 ENTRYPOINT [\"java\",\"-jar\",\"/app.jar\"]' > Dockerfile Clean, install, build, login to Docker, tag, push, mvn clean install docker image build -t helloworld . docker run -d --name helloworld -p 8081:8080 helloworld docker login -u <username> docker tag helloworld remkohdev/helloworld:lab1v1.0 docker push remkohdev/helloworld:lab1v1.0","title":"Hello java app"},{"location":"archive/hello-java-app/#how-the-helloworld-app-was-created","text":"Do not The following step is only explaining how the HelloWorld application was created. The Kubernetes resources will actually pull an existing image from Docker Hub. Create the Spring Boot scaffolding, $ spring init --dependencies=web,data-rest,thymeleaf helloworld-api $ cd helloworld-api $ mvn clean install $ mvn test $ mvn spring-boot:run Create the APIController, $ echo 'package com.example.helloworldapi; import java.util.List; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.ResponseBody; import org.springframework.web.bind.annotation.RestController; import org.springframework.http.MediaType; import org.springframework.http.ResponseEntity; import org.springframework.http.HttpStatus; @RestController public class APIController { @Autowired private MessageRepository repository; @GetMapping(\"/api\") public String index() { return \"Welcome to Spring Boot App\"; } @RequestMapping(value = \"/api/hello\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE) @ResponseBody public String hello(@RequestParam String name) { String message = \"Hello \"+ name; String responseJson = \"{ \\\"message\\\" : \\\"\"+ message + \"\\\" }\"; repository.save(new Message(name, message)); for (Message msg : repository.findAll()) { System.out.println(msg); } return responseJson; } @RequestMapping(value = \"/api/messages\", method = RequestMethod.GET, produces = MediaType.APPLICATION_JSON_VALUE) @ResponseBody public ResponseEntity<List<Message>> getMessages() { List<Message> messages = (List<Message>) repository.findAll(); return new ResponseEntity<List<Message>>(messages, HttpStatus.OK); } } ' > src/main/java/com/example/helloworldapi/APIController.java Create the Message class, $ echo 'package com.example.helloworldapi; import org.springframework.data.annotation.Id; public class Message { @Id private String id; private String sender; private String message; public Message(String sender, String message) { this.sender = sender; this.message = message; } public String getId() { return id; } public String getSender() { return sender; } public String getMessage() { return message; } @Override public String toString() { return \"Message [id=\" + id + \", sender=\" + sender + \", message=\" + message + \"]\"; } }' > src/main/java/com/example/helloworldapi/Message.java Create the MessageRepository class, echo 'package com.example.helloworldapi; import java.util.List; import org.springframework.data.mongodb.repository.MongoRepository; public interface MessageRepository extends MongoRepository<Message, String> { public List<Message> findBySender(String sender); }' > src/main/java/com/example/helloworldapi/MessageRepository.java Add a new file \u2018~/src/main/resources/application.properties\u2019, whose configuration should match those when the MongoDB server was deployed, echo 'pring.data.mongodb.username=user1 spring.data.mongodb.password=passw0rd spring.data.mongodb.database=mydb spring.data.mongodb.port=27017 spring.data.mongodb.host=mongodb.default' > src/main/resources/application.properties Add the following dependency to the Maven build file in pom.xml , <dependencies> <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-data-mongodb</artifactId> </dependency> </dependencies>","title":"How the HelloWorld App was Created"},{"location":"archive/hello-java-app/#test-java-app-on-localhost","text":"Create local Mongodb, docker run --name mongo -d -p 27017:27017 -e MONGO_INITDB_ROOT_USERNAME=admin -e MONGO_INITDB_ROOT_PASSWORD=passw0rd mongo Get IP address on mac osx, $ ifconfig en0 | grep inet 192.168.86.45 Configure application.properties and change the host to your local IP Address to test the app on localhost. On the localhost we did not create a separate user, but you can use the admin user. spring.data.mongodb.username=user1 spring.data.mongodb.password=passw0rd spring.data.mongodb.database=mydb spring.data.mongodb.port=27017 spring.data.mongodb.host=192.168.86.45 Run the application $ mvn clean install $ mvn spring-boot:run test $ curl -X GET 'http://192.168.86.45:8080/api/hello?name=one' { \"message\" : \"Hello one\" }","title":"Test Java App on localhost:"},{"location":"archive/hello-java-app/#dockerize","text":"echo 'FROM openjdk:8-jdk-alpine ARG JAR_FILE=target/*.jar COPY ${JAR_FILE} app.jar EXPOSE 8080 ENTRYPOINT [\"java\",\"-jar\",\"/app.jar\"]' > Dockerfile Clean, install, build, login to Docker, tag, push, mvn clean install docker image build -t helloworld . docker run -d --name helloworld -p 8081:8080 helloworld docker login -u <username> docker tag helloworld remkohdev/helloworld:lab1v1.0 docker push remkohdev/helloworld:lab1v1.0","title":"Dockerize"},{"location":"archive/hello-proxy-setup/","text":"HelloWorld App Proxy Setup \u00b6 I want to deploy two versions of helloworld , a direct helloworld and a helloworld-proxy , which is a proxy to the helloworld application. To access the direct version, I add a path /helloworld . To access the proxy version, I add a path /helloworld/proxy . First deploy the proxy. $ kubectl create -f helloworld-proxy-deployment.yaml $ kubectl create -f helloworld-proxy-service-loadbalancer.yaml Get the proxy service details and test the proxy, $ kubectl get svc helloworld-proxy NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE helloworld-proxy LoadBalancer 172.21.78.158 169.48.67.164 8080:30940/TCP 61s To test use the External-IP and the NodePort, e.g. 169.48.67.164:30940, and send the proxy the Kubernetes service name and target port, $ $ curl -L -X POST \"http://169.48.67.164:30940/proxy/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld-proxy:8080\" }' {\"id\":\"3aa4f889-94a0-4be8-b8f2-8b59f8ad3de7\",\"sender\":\"remko\",\"message\":\"Hello remko (proxy)\",\"host\":\"helloworld-proxy:8080\"} The proxy will add a proxy message to the created message. You need the Ingress Subdomain and Ingress Secret of your cluster to configure your Ingress resource. Resources \u00b6 See: https://kubernetes.io/docs/concepts/services-networking/ingress/ https://medium.com/google-cloud/understanding-kubernetes-networking-ingress-1bc341c84078 https://github.com/ibm-cloud-docs/containers/blob/master/cs_ingress.md Kong, \"the world's most popular open source API Gateway\" , tutorial with Kong Ingress","title":"HelloWorld App Proxy Setup"},{"location":"archive/hello-proxy-setup/#helloworld-app-proxy-setup","text":"I want to deploy two versions of helloworld , a direct helloworld and a helloworld-proxy , which is a proxy to the helloworld application. To access the direct version, I add a path /helloworld . To access the proxy version, I add a path /helloworld/proxy . First deploy the proxy. $ kubectl create -f helloworld-proxy-deployment.yaml $ kubectl create -f helloworld-proxy-service-loadbalancer.yaml Get the proxy service details and test the proxy, $ kubectl get svc helloworld-proxy NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE helloworld-proxy LoadBalancer 172.21.78.158 169.48.67.164 8080:30940/TCP 61s To test use the External-IP and the NodePort, e.g. 169.48.67.164:30940, and send the proxy the Kubernetes service name and target port, $ $ curl -L -X POST \"http://169.48.67.164:30940/proxy/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld-proxy:8080\" }' {\"id\":\"3aa4f889-94a0-4be8-b8f2-8b59f8ad3de7\",\"sender\":\"remko\",\"message\":\"Hello remko (proxy)\",\"host\":\"helloworld-proxy:8080\"} The proxy will add a proxy message to the created message. You need the Ingress Subdomain and Ingress Secret of your cluster to configure your Ingress resource.","title":"HelloWorld App Proxy Setup"},{"location":"archive/hello-proxy-setup/#resources","text":"See: https://kubernetes.io/docs/concepts/services-networking/ingress/ https://medium.com/google-cloud/understanding-kubernetes-networking-ingress-1bc341c84078 https://github.com/ibm-cloud-docs/containers/blob/master/cs_ingress.md Kong, \"the world's most popular open source API Gateway\" , tutorial with Kong Ingress","title":"Resources"},{"location":"archive/ingress-byo/","text":"Bring your own Ingress controller \u00b6 run Ingress on a free 1 node IKS cluster In IBM Cloud Kubernetes Service, IBM-provided application load balancers (ALBs) are based on a custom implementation of the NGINX Ingress controller. However, depending on what your app requires, you might want to configure your own custom Ingress controller instead of using the IBM-provided ALBs. For example, you might want to use the Istio ingressgateway load balancer service to control traffic for your cluster. When you bring your own Ingress controller, you are responsible for supplying the controller image, maintaining the controller, updating the controller, and any security-related updates to keep your Ingress controller free from vulnerabilities. Expose your Ingress controller by creating an NLB and a hostname \u00b6 Create a free cluster Login to your cluster List existing NLBs CLUSTERNAME=remkohdev-iks116-1n-cluster-free % ibmcloud ks nlb-dns ls -c $CLUSTERNAME OK Subdomain Load Balancer Hostname SSL Cert Status SSL Cert Secret Name Secret Namespace kubectl config current-context ibmcloud ks worker ls --cluster $CLUSTERNAME OK ID Public IP Private IP Flavor State Status Zone Version kube-br2lrckd0icvsu163bt0-remkohdevik-default-000000f5 173.193.106.26 10.76.114.251 free normal Ready hou02 1.16.9_1531 % ibmcloud ks zones --provider classic % ibmcloud ks vlan ls --zone OK ID Name Number Type Router Supports Virtual Workers ibmcloud ks cluster get --cluster $CLUSTERNAME % ibmcloud ks worker-pool ls --cluster $CLUSTERNAME OK Name ID Flavor Workers default br1vf5df00av3tu5nbd0-45ebec5 free 1 Create a Network Load Balancer (NLB) to expose your custom Ingress controller deployment by creating a LoadBalancer service, echo 'apiVersion: v1 kind: Service metadata: name: my-ingress labels: app: my-ingress spec: ports: - port: 80 selector: app: my-ingress type: LoadBalancer' > my-ingress.yaml kubectl create -f my-ingress.yaml apiVersion: v1 kind: Service metadata: name: my-ingress annotations: service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type: service.kubernetes.io/ibm-load-balancer-cloud-provider-vlan: \" \" spec: type: LoadBalancer selector: : ports: - protocol: TCP port: 8080 loadBalancerIP: and then create a hostname for the NLB IP address. Get the configuration file for your Ingress controller ready. For example, you can use the cloud-generic NGINX community Ingress controller. If you use the community controller, edit the mandatory.yaml file by following these steps. Replace all instances of namespace: ingress-nginx with namespace: kube-system. Replace all instances of the app.kubernetes.io/name: ingress-nginx and app.kubernetes.io/part-of: ingress-nginx labels with one app: ingress-nginx label. Deploy your own Ingress controller. For example, to use the cloud-generic NGINX community Ingress controller, run the following command. kubectl apply -f mandatory.yaml -n kube-system Define a load balancer service to expose your custom Ingress deployment. apiVersion: v1 kind: Service metadata: name: ingress-nginx spec: type: LoadBalancer selector: app: ingress-nginx ports: - name: http protocol: TCP port: 80 - name: https protocol: TCP port: 443 externalTrafficPolicy: Cluster Create the service in your cluster. kubectl apply -f ingress-nginx.yaml -n kube-system Get the EXTERNAL-IP address for the load balancer. kubectl get svc ingress-nginx -n kube-system","title":"Bring your own Ingress controller"},{"location":"archive/ingress-byo/#bring-your-own-ingress-controller","text":"run Ingress on a free 1 node IKS cluster In IBM Cloud Kubernetes Service, IBM-provided application load balancers (ALBs) are based on a custom implementation of the NGINX Ingress controller. However, depending on what your app requires, you might want to configure your own custom Ingress controller instead of using the IBM-provided ALBs. For example, you might want to use the Istio ingressgateway load balancer service to control traffic for your cluster. When you bring your own Ingress controller, you are responsible for supplying the controller image, maintaining the controller, updating the controller, and any security-related updates to keep your Ingress controller free from vulnerabilities.","title":"Bring your own Ingress controller"},{"location":"archive/ingress-byo/#expose-your-ingress-controller-by-creating-an-nlb-and-a-hostname","text":"Create a free cluster Login to your cluster List existing NLBs CLUSTERNAME=remkohdev-iks116-1n-cluster-free % ibmcloud ks nlb-dns ls -c $CLUSTERNAME OK Subdomain Load Balancer Hostname SSL Cert Status SSL Cert Secret Name Secret Namespace kubectl config current-context ibmcloud ks worker ls --cluster $CLUSTERNAME OK ID Public IP Private IP Flavor State Status Zone Version kube-br2lrckd0icvsu163bt0-remkohdevik-default-000000f5 173.193.106.26 10.76.114.251 free normal Ready hou02 1.16.9_1531 % ibmcloud ks zones --provider classic % ibmcloud ks vlan ls --zone OK ID Name Number Type Router Supports Virtual Workers ibmcloud ks cluster get --cluster $CLUSTERNAME % ibmcloud ks worker-pool ls --cluster $CLUSTERNAME OK Name ID Flavor Workers default br1vf5df00av3tu5nbd0-45ebec5 free 1 Create a Network Load Balancer (NLB) to expose your custom Ingress controller deployment by creating a LoadBalancer service, echo 'apiVersion: v1 kind: Service metadata: name: my-ingress labels: app: my-ingress spec: ports: - port: 80 selector: app: my-ingress type: LoadBalancer' > my-ingress.yaml kubectl create -f my-ingress.yaml apiVersion: v1 kind: Service metadata: name: my-ingress annotations: service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type: service.kubernetes.io/ibm-load-balancer-cloud-provider-vlan: \" \" spec: type: LoadBalancer selector: : ports: - protocol: TCP port: 8080 loadBalancerIP: and then create a hostname for the NLB IP address. Get the configuration file for your Ingress controller ready. For example, you can use the cloud-generic NGINX community Ingress controller. If you use the community controller, edit the mandatory.yaml file by following these steps. Replace all instances of namespace: ingress-nginx with namespace: kube-system. Replace all instances of the app.kubernetes.io/name: ingress-nginx and app.kubernetes.io/part-of: ingress-nginx labels with one app: ingress-nginx label. Deploy your own Ingress controller. For example, to use the cloud-generic NGINX community Ingress controller, run the following command. kubectl apply -f mandatory.yaml -n kube-system Define a load balancer service to expose your custom Ingress deployment. apiVersion: v1 kind: Service metadata: name: ingress-nginx spec: type: LoadBalancer selector: app: ingress-nginx ports: - name: http protocol: TCP port: 80 - name: https protocol: TCP port: 443 externalTrafficPolicy: Cluster Create the service in your cluster. kubectl apply -f ingress-nginx.yaml -n kube-system Get the EXTERNAL-IP address for the load balancer. kubectl get svc ingress-nginx -n kube-system","title":"Expose your Ingress controller by creating an NLB and a hostname"},{"location":"archive/networkpolicy-notes/","text":"find your cloud shell name $ ls -al /tmp/ic .. cloudshell-a066b1e2-1975-41d3-a316-d9e92d917e90-1-7f5b46f8v96r5-1 $ SHELL=cloudshell-a066b1e2-1975-41d3-a316-d9e92d917e90-1-7f5b46f8v96r5-1 Find the cluster's admin folder $ ls -al /tmp/ic/$SHELL/.bluemix/plugins/container-service/clusters/ .. remkohdev-iks116-2n-cluster-br9v078d0qi43m0e31n0 remkohdev-iks116-2n-cluster-br9v078d0qi43m0e31n0-admin $ ADMINDIR=remkohdev-iks116-2n-cluster-br9v078d0qi43m0e31n0-admin /tmp/ic/$SHELL/.bluemix/plugins/container-service/clusters/$ADMINDIR/calicoctl.cfg $ mv /tmp/ic/$SHELL/.bluemix/plugins/container-service/clusters/$ADMINDIR/calicoctl.cfg /etc/calico $ calicoctl get nodes NAME kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 $ calicoctl version Client Version: v3.10.0 Git commit: 7968b525 Cluster Version: v3.9.5 Cluster Type: k8s,bgp View network policy $ calicoctl get hostendpoint -o yaml apiVersion: projectcalico.org/v3 items: - apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:23:51Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.101 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.149 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_private kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.149 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b-worker-private resourceVersion: \"2643\" uid: e0eaf0d6-a36b-11ea-8f29-06ce448b0c2d spec: expectedIPs: - 10.187.222.149 interfaceName: eth0 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b - apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:23:39Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.101 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.149 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_public kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.149 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b-worker-public resourceVersion: \"2642\" uid: 78ec7195-2ddd-4b5f-b7c4-8a54d25f4beb spec: expectedIPs: - 150.238.93.101 interfaceName: eth1 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-0000014b - apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:23:01Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.100 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.146 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_private kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.146 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0-worker-private resourceVersion: \"2295\" uid: c2fcd831-a36b-11ea-97f2-06edddc7e672 spec: expectedIPs: - 10.187.222.146 interfaceName: eth0 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 - apiVersion: projectcalico.org/v3 kind: HostEndpoint metadata: creationTimestamp: 2020-05-31T18:22:54Z labels: arch: amd64 beta.kubernetes.io/arch: amd64 beta.kubernetes.io/instance-type: b3c.4x16.encrypted beta.kubernetes.io/os: linux failure-domain.beta.kubernetes.io/region: us-south failure-domain.beta.kubernetes.io/zone: dal13 ibm-cloud.kubernetes.io/encrypted-docker-data: \"true\" ibm-cloud.kubernetes.io/external-ip: 150.238.93.100 ibm-cloud.kubernetes.io/ha-worker: \"true\" ibm-cloud.kubernetes.io/iaas-provider: softlayer ibm-cloud.kubernetes.io/internal-ip: 10.187.222.146 ibm-cloud.kubernetes.io/machine-type: b3c.4x16.encrypted ibm-cloud.kubernetes.io/os: UBUNTU_18_64 ibm-cloud.kubernetes.io/region: us-south ibm-cloud.kubernetes.io/sgx-enabled: \"false\" ibm-cloud.kubernetes.io/worker-id: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 ibm-cloud.kubernetes.io/worker-pool-id: br9v078d0qi43m0e31n0-3108b12 ibm-cloud.kubernetes.io/worker-pool-name: default ibm-cloud.kubernetes.io/worker-version: 1.16.10_1533 ibm-cloud.kubernetes.io/zone: dal13 ibm.role: worker_public kubernetes.io/arch: amd64 kubernetes.io/hostname: 10.187.222.146 kubernetes.io/os: linux privateVLAN: \"2847992\" publicVLAN: \"2847990\" name: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0-worker-public resourceVersion: \"2294\" uid: e8c1fb2b-1a88-4cb4-a10e-67c9bdd28e1c spec: expectedIPs: - 150.238.93.100 interfaceName: eth1 node: kube-br9v078d0qi43m0e31n0-remkohdevik-default-000002e0 kind: HostEndpointList metadata: resourceVersion: \"40605\" $ calicoctl get NetworkPolicy --all-namespaces -o wide NAMESPACE NAME ORDER SELECTOR kube-system knp.default.kubernetes-dashboard 1000 projectcalico.org/orchestrator == 'k8s' && k8s-app == 'kubernetes-dashboard' $ calicoctl get GlobalNetworkPolicy -o wide NAME ORDER SELECTOR allow-all-outbound 1900 ibm.role in { 'worker_public', 'master_public' } allow-all-private-default 100000 ibm.role == 'worker_private' allow-bigfix-port 1900 ibm.role in { 'worker_public', 'master_public' } allow-icmp 1500 ibm.role in { 'worker_public', 'master_public' } allow-node-port-dnat 1500 ibm.role == 'worker_public' allow-sys-mgmt 1950 ibm.role in { 'worker_public', 'master_public' } allow-vrrp 1500 ibm.role == 'worker_public' $ echo 'apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: access-nginx namespace: advanced-policy-demo spec: podSelector: matchLabels: app: helloworld ingress: - from: - podSelector: matchLabels: {}' > helloworld-policy-allow-to-helloworld.yaml","title":"Networkpolicy notes"},{"location":"calico/networkpolicy/","text":"Network Policy and Calico \u00b6 Prerequirements \u00b6 Finish the Services , ClusterIP , NodePort , LoadBalancer , Ingress , and Route labs. This should provide you with: Logged in to IBM Cloud account, Connected to Kubernetes cluster, Guestbook Deployment, Guestbook Service of type LoadBalancer, An Ingress and Route Ingress controller, Network Policies \u00b6 By default, pods are non-isolated and accept traffic from any source. When defining a pod- or namespace- based NetworkPolicy, labels are used to select pods. If a Pod is matched by selectors in one or more NetworkPolicy objects, then the Pod will accept only connections that are allowed by at least one of those NetworkPolicy's ingress/egress rules. A Pod that is not selected by any NetworkPolicy objects is fully accessible. Network policies do not conflict, they add up. Thus, order of evaluation does not affect the policy result. There are four kinds of selectors in an ingress from section or egress to section: podSelector, namespaceSelector, podSelector and namespaceSelector, ipBlock for IP CIDR ranges. The following example allows traffic from a frontend application to a backend application, apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : my-network-policy namespace : default spec : podSelector : matchLabels : role : db policyTypes : - Ingress - Egress ingress : - from : - podSelector : matchLabels : role : frontend ports : - protocol : TCP port : 6379 egress : - to : - podSelector : matchLabels : role : backend ports : - protocol : TCP port : 5978 The following example denies all ingress traffic, apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny-ingress spec : podSelector : {} policyTypes : - Ingress On IBM Cloud, every Kubernetes Service cluster is set up with a network plug-in called Calico , which includes default network policies to secure the public network interface of every worker node in the cluster. When a Kubernetes network policy is applied, it is automatically converted into a Calico network policy so that Calico can apply it as an Iptables rule. Iptables rules serve as a firewall for the worker node to define the characteristics that the network traffic must meet to be forwarded to the targeted resource. Create helloworld Proxy \u00b6 For this tutorial, we will use an additional app called helloworld-proxy , which proxies requests to the helloworld app. If you don't have the repository already, clone it to your local machine, git clone https://github.com/remkohdev/helloworld.git cd helloworld ls -al You should deploy the helloworld and helloworld-proxy application, oc new-project $MY_NS oc create -f helloworld-deployment.yaml -n $MY_NS oc create -f helloworld-service-loadbalancer.yaml -n $MY_NS oc expose service helloworld -n $MY_NS oc create -f helloworld-proxy-deployment.yaml -n $MY_NS oc create -f helloworld-proxy-service-loadbalancer.yaml -n $MY_NS oc expose service helloworld-proxy -n $MY_NS The deployment in your project namespace should now look as follows, oc get all -n $MY_NS $ oc get all -n $MY_NS NAME READY STATUS RESTARTS AGE pod/helloworld-6c76f57b9d-76lw9 1 /1 Running 0 5h47m pod/helloworld-6c76f57b9d-jr42j 1 /1 Running 0 5h47m pod/helloworld-6c76f57b9d-qllbz 1 /1 Running 0 5h47m pod/helloworld-proxy-9f89649db-77cc5 1 /1 Running 0 13s pod/helloworld-proxy-9f89649db-fz7wh 1 /1 Running 0 13s pod/helloworld-proxy-9f89649db-ts5zj 1 /1 Running 0 13s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/helloworld LoadBalancer 172 .21.72.153 169 .60.24.131 8080 :30549/TCP 5h10m service/helloworld-proxy LoadBalancer 172 .21.115.248 169 .60.24.132 8080 :31043/TCP 12s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/helloworld 3 /3 3 3 5h47m deployment.apps/helloworld-proxy 3 /3 3 3 13s NAME DESIRED CURRENT READY AGE replicaset.apps/helloworld-6c76f57b9d 3 3 3 5h47m replicaset.apps/helloworld-proxy-9f89649db 3 3 3 13s NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD route.route.openshift.io/example-5w8qv remkohdev-roks45-2n-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud / helloworld <all> None route.route.openshift.io/helloworld helloworld-my-apps.remkohdev-roks45-2n-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud helloworld http-server None Get the proxy service details and test the proxy, ROUTE = $( oc get route helloworld -n $MY_NS -o json | jq -r '.spec.host' ) echo $ROUTE NODE_PORT = $( oc get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $NODE_PORT PROXY_ROUTE = $( oc get route helloworld-proxy -n $MY_NS -o json | jq -r '.spec.host' ) echo $PROXY_ROUTE PROXY_PUBLIC_IP = $( oc get svc helloworld-proxy -n $MY_NS --output json | jq -r '.status.loadBalancer.ingress[0].ip' ) echo $PROXY_PUBLIC_IP PROXY_NODE_PORT = $( oc get svc helloworld-proxy -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $PROXY_NODE_PORT Test the helloworld-proxy app, add the host: helloworld:8080 property in the data object, which tells the helloworld-proxy app to proxy the message to the host app, and send the request to the /api/messages endpoint of our helloworld app on port 8080 using the internal DNS for service discovery. Because it is an internal request, the proxy uses the container port rather than the NodePort, which is used for external requests. $ curl -L -X POST \"http:// $ROUTE : $NODE_PORT /api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld:8080\" }' { \"id\" : \"ffb70e2f-34be-480c-9b05-53577119ff75\" , \"sender\" : \"remko\" , \"message\" : \"Hello remko (direct)\" , \"host\" : \"helloworld:8080\" } $ curl -L -X POST \"http:// $PROXY_ROUTE : $PROXY_NODE_PORT /proxy/api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld:8080\" }' { \"id\" : \"d1d22ad0-02c8-4e17-be07-48ae8b6ce964\" , \"sender\" : \"remko\" , \"message\" : \"Hello remko (proxy)\" , \"host\" : \"helloworld:8080\" } The source code for the helloworld application can be found here . Apply Network Policy - Deny All Traffic \u00b6 Adopting a zero trust network model is best practice for securing workloads and hosts in your cloud-native strategy. Define the Network Policy file to deny all traffic, echo 'apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: helloworld-deny-all spec: podSelector: {} policyTypes: - Ingress - Egress' > helloworld-policy-denyall.yaml Create the Network Policy, $ oc create -f helloworld-policy-denyall.yaml -n $MY_NS networkpolicy.networking.k8s.io/helloworld-deny-all created Test both the helloworld and the helloworld-proxy apps, $ curl -L -X POST \"http:// $ROUTE : $NODE_PORT /api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\" }' curl: ( 7 ) Failed connect to helloworld-my-apps.dte-ocp44-t6knp0-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud:30354 ; Connection timed out $ curl -L -X POST \"http:// $PROXY_ROUTE : $PROXY_NODE_PORT /proxy/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld:8080\" }' curl: ( 7 ) Failed connect to helloworld-proxy-my-apps.dte-ocp44-t6knp0-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud:32046 ; Connection timed out It takes quite a long time before connections time out. All traffic is denied, despite that we have a LoadBalancer services and routes added to each deployment, $ oc get svc -n $MY_NS NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE helloworld LoadBalancer 172 .21.72.153 169 .60.24.131 8080 :30549/TCP 5h34m helloworld-proxy LoadBalancer 172 .21.115.248 169 .60.24.132 8080 :31043/TCP 24m $ oc get routes -n $MY_NS NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD helloworld helloworld-my-apps.dte-ocp44-t6knp0-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud helloworld http-server None helloworld-ingress-lxz9w hello.dte-ocp44-t6knp0-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud helloworld <all> None helloworld-proxy helloworld-proxy-my-apps.dte-ocp44-t6knp0-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud helloworld-proxy http-server None Apply Network Policy - Allow Only Traffic to Pod \u00b6 Let's allow direct ingress traffic to the helloworld app on port 8080 , but not allow traffic to the helloworld-proxy app. Define the Network Policy file, echo 'apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-helloworld spec: policyTypes: - Ingress podSelector: matchLabels: app: helloworld ingress: - {}' > helloworld-allow.yaml Create the Network Policy, $ oc create -f helloworld-allow.yaml -n $MY_NS networkpolicy.networking.k8s.io/allow-helloworld created Review the existing NetworkPolices in the project namespace, $ oc get networkpolicies -n $MY_NS NAME POD-SELECTOR AGE allow-helloworld app = helloworld 21s helloworld-deny-all <none> 21m Test the helloworld and the `helloworld-proxy' apps again, $ curl -L -X POST \"http:// $ROUTE : $NODE_PORT /api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\" }' { \"id\" : \"12e59a9e-fa21-41f7-a6d5-823a0ca5d2ea\" , \"sender\" : \"remko\" , \"message\" : \"Hello remko (direct)\" , \"host\" :null } $ curl -L -X POST \"http:// $PROXY_ROUTE : $PROXY_NODE_PORT /proxy/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld:8080\" }' curl: ( 7 ) Failed connect to helloworld-proxy-my-apps.dte-ocp44-t6knp0-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud:32046 ; Connection timed out Cleanup \u00b6 Delete the NetworkPolicies in your namespace, oc delete networkpolicy allow-helloworld -n $MY_NS oc delete networkpolicy helloworld-deny-all -n $MY_NS Delete the previously created resources, oc delete deployment helloworld -n $MY_NS oc delete deployment helloworld-proxy -n $MY_NS oc delete svc helloworld -n $MY_NS oc delete svc helloworld-proxy -n $MY_NS oc delete namespace $MY_NS Verify all resources are removed, $ oc get all -n $MY_NS No resources found in my-apps namespace.","title":"Network Policy and Calico"},{"location":"calico/networkpolicy/#network-policy-and-calico","text":"","title":"Network Policy and Calico"},{"location":"calico/networkpolicy/#prerequirements","text":"Finish the Services , ClusterIP , NodePort , LoadBalancer , Ingress , and Route labs. This should provide you with: Logged in to IBM Cloud account, Connected to Kubernetes cluster, Guestbook Deployment, Guestbook Service of type LoadBalancer, An Ingress and Route Ingress controller,","title":"Prerequirements"},{"location":"calico/networkpolicy/#network-policies","text":"By default, pods are non-isolated and accept traffic from any source. When defining a pod- or namespace- based NetworkPolicy, labels are used to select pods. If a Pod is matched by selectors in one or more NetworkPolicy objects, then the Pod will accept only connections that are allowed by at least one of those NetworkPolicy's ingress/egress rules. A Pod that is not selected by any NetworkPolicy objects is fully accessible. Network policies do not conflict, they add up. Thus, order of evaluation does not affect the policy result. There are four kinds of selectors in an ingress from section or egress to section: podSelector, namespaceSelector, podSelector and namespaceSelector, ipBlock for IP CIDR ranges. The following example allows traffic from a frontend application to a backend application, apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : my-network-policy namespace : default spec : podSelector : matchLabels : role : db policyTypes : - Ingress - Egress ingress : - from : - podSelector : matchLabels : role : frontend ports : - protocol : TCP port : 6379 egress : - to : - podSelector : matchLabels : role : backend ports : - protocol : TCP port : 5978 The following example denies all ingress traffic, apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny-ingress spec : podSelector : {} policyTypes : - Ingress On IBM Cloud, every Kubernetes Service cluster is set up with a network plug-in called Calico , which includes default network policies to secure the public network interface of every worker node in the cluster. When a Kubernetes network policy is applied, it is automatically converted into a Calico network policy so that Calico can apply it as an Iptables rule. Iptables rules serve as a firewall for the worker node to define the characteristics that the network traffic must meet to be forwarded to the targeted resource.","title":"Network Policies"},{"location":"calico/networkpolicy/#create-helloworld-proxy","text":"For this tutorial, we will use an additional app called helloworld-proxy , which proxies requests to the helloworld app. If you don't have the repository already, clone it to your local machine, git clone https://github.com/remkohdev/helloworld.git cd helloworld ls -al You should deploy the helloworld and helloworld-proxy application, oc new-project $MY_NS oc create -f helloworld-deployment.yaml -n $MY_NS oc create -f helloworld-service-loadbalancer.yaml -n $MY_NS oc expose service helloworld -n $MY_NS oc create -f helloworld-proxy-deployment.yaml -n $MY_NS oc create -f helloworld-proxy-service-loadbalancer.yaml -n $MY_NS oc expose service helloworld-proxy -n $MY_NS The deployment in your project namespace should now look as follows, oc get all -n $MY_NS $ oc get all -n $MY_NS NAME READY STATUS RESTARTS AGE pod/helloworld-6c76f57b9d-76lw9 1 /1 Running 0 5h47m pod/helloworld-6c76f57b9d-jr42j 1 /1 Running 0 5h47m pod/helloworld-6c76f57b9d-qllbz 1 /1 Running 0 5h47m pod/helloworld-proxy-9f89649db-77cc5 1 /1 Running 0 13s pod/helloworld-proxy-9f89649db-fz7wh 1 /1 Running 0 13s pod/helloworld-proxy-9f89649db-ts5zj 1 /1 Running 0 13s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/helloworld LoadBalancer 172 .21.72.153 169 .60.24.131 8080 :30549/TCP 5h10m service/helloworld-proxy LoadBalancer 172 .21.115.248 169 .60.24.132 8080 :31043/TCP 12s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/helloworld 3 /3 3 3 5h47m deployment.apps/helloworld-proxy 3 /3 3 3 13s NAME DESIRED CURRENT READY AGE replicaset.apps/helloworld-6c76f57b9d 3 3 3 5h47m replicaset.apps/helloworld-proxy-9f89649db 3 3 3 13s NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD route.route.openshift.io/example-5w8qv remkohdev-roks45-2n-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud / helloworld <all> None route.route.openshift.io/helloworld helloworld-my-apps.remkohdev-roks45-2n-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud helloworld http-server None Get the proxy service details and test the proxy, ROUTE = $( oc get route helloworld -n $MY_NS -o json | jq -r '.spec.host' ) echo $ROUTE NODE_PORT = $( oc get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $NODE_PORT PROXY_ROUTE = $( oc get route helloworld-proxy -n $MY_NS -o json | jq -r '.spec.host' ) echo $PROXY_ROUTE PROXY_PUBLIC_IP = $( oc get svc helloworld-proxy -n $MY_NS --output json | jq -r '.status.loadBalancer.ingress[0].ip' ) echo $PROXY_PUBLIC_IP PROXY_NODE_PORT = $( oc get svc helloworld-proxy -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $PROXY_NODE_PORT Test the helloworld-proxy app, add the host: helloworld:8080 property in the data object, which tells the helloworld-proxy app to proxy the message to the host app, and send the request to the /api/messages endpoint of our helloworld app on port 8080 using the internal DNS for service discovery. Because it is an internal request, the proxy uses the container port rather than the NodePort, which is used for external requests. $ curl -L -X POST \"http:// $ROUTE : $NODE_PORT /api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld:8080\" }' { \"id\" : \"ffb70e2f-34be-480c-9b05-53577119ff75\" , \"sender\" : \"remko\" , \"message\" : \"Hello remko (direct)\" , \"host\" : \"helloworld:8080\" } $ curl -L -X POST \"http:// $PROXY_ROUTE : $PROXY_NODE_PORT /proxy/api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld:8080\" }' { \"id\" : \"d1d22ad0-02c8-4e17-be07-48ae8b6ce964\" , \"sender\" : \"remko\" , \"message\" : \"Hello remko (proxy)\" , \"host\" : \"helloworld:8080\" } The source code for the helloworld application can be found here .","title":"Create helloworld Proxy"},{"location":"calico/networkpolicy/#apply-network-policy-deny-all-traffic","text":"Adopting a zero trust network model is best practice for securing workloads and hosts in your cloud-native strategy. Define the Network Policy file to deny all traffic, echo 'apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: helloworld-deny-all spec: podSelector: {} policyTypes: - Ingress - Egress' > helloworld-policy-denyall.yaml Create the Network Policy, $ oc create -f helloworld-policy-denyall.yaml -n $MY_NS networkpolicy.networking.k8s.io/helloworld-deny-all created Test both the helloworld and the helloworld-proxy apps, $ curl -L -X POST \"http:// $ROUTE : $NODE_PORT /api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\" }' curl: ( 7 ) Failed connect to helloworld-my-apps.dte-ocp44-t6knp0-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud:30354 ; Connection timed out $ curl -L -X POST \"http:// $PROXY_ROUTE : $PROXY_NODE_PORT /proxy/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld:8080\" }' curl: ( 7 ) Failed connect to helloworld-proxy-my-apps.dte-ocp44-t6knp0-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud:32046 ; Connection timed out It takes quite a long time before connections time out. All traffic is denied, despite that we have a LoadBalancer services and routes added to each deployment, $ oc get svc -n $MY_NS NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE helloworld LoadBalancer 172 .21.72.153 169 .60.24.131 8080 :30549/TCP 5h34m helloworld-proxy LoadBalancer 172 .21.115.248 169 .60.24.132 8080 :31043/TCP 24m $ oc get routes -n $MY_NS NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD helloworld helloworld-my-apps.dte-ocp44-t6knp0-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud helloworld http-server None helloworld-ingress-lxz9w hello.dte-ocp44-t6knp0-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud helloworld <all> None helloworld-proxy helloworld-proxy-my-apps.dte-ocp44-t6knp0-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud helloworld-proxy http-server None","title":"Apply Network Policy - Deny All Traffic"},{"location":"calico/networkpolicy/#apply-network-policy-allow-only-traffic-to-pod","text":"Let's allow direct ingress traffic to the helloworld app on port 8080 , but not allow traffic to the helloworld-proxy app. Define the Network Policy file, echo 'apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-helloworld spec: policyTypes: - Ingress podSelector: matchLabels: app: helloworld ingress: - {}' > helloworld-allow.yaml Create the Network Policy, $ oc create -f helloworld-allow.yaml -n $MY_NS networkpolicy.networking.k8s.io/allow-helloworld created Review the existing NetworkPolices in the project namespace, $ oc get networkpolicies -n $MY_NS NAME POD-SELECTOR AGE allow-helloworld app = helloworld 21s helloworld-deny-all <none> 21m Test the helloworld and the `helloworld-proxy' apps again, $ curl -L -X POST \"http:// $ROUTE : $NODE_PORT /api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\" }' { \"id\" : \"12e59a9e-fa21-41f7-a6d5-823a0ca5d2ea\" , \"sender\" : \"remko\" , \"message\" : \"Hello remko (direct)\" , \"host\" :null } $ curl -L -X POST \"http:// $PROXY_ROUTE : $PROXY_NODE_PORT /proxy/api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: application/json' -d '{ \"sender\": \"remko\", \"host\": \"helloworld:8080\" }' curl: ( 7 ) Failed connect to helloworld-proxy-my-apps.dte-ocp44-t6knp0-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud:32046 ; Connection timed out","title":"Apply Network Policy - Allow Only Traffic to Pod"},{"location":"calico/networkpolicy/#cleanup","text":"Delete the NetworkPolicies in your namespace, oc delete networkpolicy allow-helloworld -n $MY_NS oc delete networkpolicy helloworld-deny-all -n $MY_NS Delete the previously created resources, oc delete deployment helloworld -n $MY_NS oc delete deployment helloworld-proxy -n $MY_NS oc delete svc helloworld -n $MY_NS oc delete svc helloworld-proxy -n $MY_NS oc delete namespace $MY_NS Verify all resources are removed, $ oc get all -n $MY_NS No resources found in my-apps namespace.","title":"Cleanup"},{"location":"ingress/ingress-alb/","text":"Ingress and Application Load Balancer (ALB) \u00b6 Pre-requisites \u00b6 Finish the Services , ClusterIP , NodePort and LoadBalancer labs. This should provide you with: Logged in to IBM Cloud account, Connected to Kubernetes cluster, Guestbook Deployment, Guestbook Service of type LoadBalancer. Network Administration \u00b6 When you create a standard cluster in IBM Cloud Kubernetes Service (IKS), a portable public subnet and a portable private subnet for the VLAN are automatically provisioned. To retrieve the cluster id, you need the cluster name. If you do not know the cluster name already, you can list all clusters in the active account, ibmcloud ks clusters Create an environment variable and set the cluster name, KS_CLUSTER_NAME = <cluster name> echo $KS_CLUSTER_NAME The portable public subnet for your cluster on IBM Cloud provides 5 usable IP addresses. 1 portable public IP address is used by the default public Ingress ALB. The remaining 4 portable public IP addresses can be used to expose single apps to the internet by creating public Network Load Balancer (NLB) services. To list all of the portable IP addresses in the Kubernetes cluster, both used and available, you can retrieve the following ConfigMap in the kube-system namespace listing the resources of the subnets, $ oc get cm ibm-cloud-provider-vlan-ip-config -n kube-system -o yaml apiVersion: v1 data: cluster_id: c09nq3aw0cv06eckrvsg reserved_private_ip: \"\" reserved_private_vlan_id: \"\" reserved_public_ip: \"\" reserved_public_vlan_id: \"\" vlanipmap.json: | - { \"vlans\" : [ { \"id\" : \"2936320\" , \"subnets\" : [ { \"id\" : \"2448002\" , \"ips\" : [ \"10.216.29.66\" , \"10.216.29.67\" , \"10.216.29.68\" , \"10.216.29.69\" , \"10.216.29.70\" ] , \"is_public\" : false, \"is_byoip\" : false, \"cidr\" : \"10.216.29.64/29\" } ] , \"zone\" : \"wdc04\" , \"region\" : \"us-east\" } , { \"id\" : \"2936318\" , \"subnets\" : [ { \"id\" : \"2390180\" , \"ips\" : [ \"169.47.155.242\" , \"169.47.155.243\" , \"169.47.155.244\" , \"169.47.155.245\" , \"169.47.155.246\" ] , \"is_public\" : true, \"is_byoip\" : false, \"cidr\" : \"169.47.155.240/29\" } ] , \"zone\" : \"wdc04\" , \"region\" : \"us-east\" } ] , \"vlan_errors\" : [] , \"reserved_ips\" : [] } kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"v1\" , \"data\" : { \"cluster_id\" : \"c09nq3aw0cv06eckrvsg\" , \"reserved_private_ip\" : \"\" , \"reserved_private_vlan_id\" : \"\" , \"reserved_public_ip\" : \"\" , \"reserved_public_vlan_id\" : \"\" , \"vlanipmap.json\" : \"{\\n \\\"vlans\\\": [\\n {\\n \\\"id\\\": \\\"2936320\\\",\\n \\\"subnets\\\": [\\n {\\n \\\"id\\\": \\\"2448002\\\",\\n \\\"ips\\\": [\\n \\\"10.216.29.66\\\",\\n \\\"10.216.29.67\\\",\\n \\\"10.216.29.68\\\",\\n \\\"10.216.29.69\\\",\\n \\\"10.216.29.70\\\"\\n ],\\n \\\"is_public\\\": false,\\n \\\"is_byoip\\\": false,\\n \\\"cidr\\\": \\\"10.216.29.64/29\\\"\\n }\\n ],\\n \\\"zone\\\": \\\"wdc04\\\",\\n \\\"region\\\": \\\"us-east\\\"\\n },\\n {\\n \\\"id\\\": \\\"2936318\\\",\\n \\\"subnets\\\": [\\n {\\n \\\"id\\\": \\\"2390180\\\",\\n \\\"ips\\\": [\\n \\\"169.47.155.242\\\",\\n \\\"169.47.155.243\\\",\\n \\\"169.47.155.244\\\",\\n \\\"169.47.155.245\\\",\\n \\\"169.47.155.246\\\"\\n ],\\n \\\"is_public\\\": true,\\n \\\"is_byoip\\\": false,\\n \\\"cidr\\\": \\\"169.47.155.240/29\\\"\\n }\\n ],\\n \\\"zone\\\": \\\"wdc04\\\",\\n \\\"region\\\": \\\"us-east\\\"\\n }\\n ],\\n \\\"vlan_errors\\\": [],\\n \\\"reserved_ips\\\": []\\n}\" } , \"kind\" : \"ConfigMap\" , \"metadata\" : { \"annotations\" : {} , \"labels\" : { \"addonmanager.kubernetes.io/mode\" : \"Reconcile\" , \"kubernetes.io/cluster-service\" : \"true\" } , \"name\" : \"ibm-cloud-provider-vlan-ip-config\" , \"namespace\" : \"kube-system\" }} creationTimestamp: \"2021-01-29T03:29:56Z\" labels: addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/cluster-service: \"true\" name: ibm-cloud-provider-vlan-ip-config namespace: kube-system resourceVersion: \"14367\" selfLink: /api/v1/namespaces/kube-system/configmaps/ibm-cloud-provider-vlan-ip-config uid: b2a8098e-61c5-4ced-a589-b2df1dcb6275 One of the public IP addresses on the public VLAN's subnet is assigned to the Network Load Balancer (NLB). List the registered NLB host name and IP address for your cluster (depending on the IBM Cloud CLI version, use --json or --output json ), $ ibmcloud ks nlb-dns ls --cluster $KS_CLUSTER_NAME --json [ { \"clusterID\" : \"c09nq3aw0cv06eckrvsg\" , \"nlbIP\" : \"[\\\"169.47.155.244\\\"]\" , \"nlbIPArray\" : [ \"169.47.155.244\" ] , \"nlbType\" : \"public\" , \"nlbHost\" : \"dte-ocp44-on2g3r-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud\" , \"secretNamespace\" : \"openshift-ingress\" , \"nlbMonitorState\" : \"None\" , \"nlbSslSecretName\" : \"dte-ocp44-on2g3r-915b3b336cabec458a7c7ec2aa7c625f-0000\" , \"nlbSslSecretStatus\" : \"created\" } ] And retrieve the NodePort via, NODE_PORT = $( oc get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $NODE_PORT You see that the portable IP address was assigned to the NLB of the LoadBalancer service. Ingress ALB \u00b6 Ingress is a reverse-proxy load balancer and Kubernetes API object that manages external access to services in a cluster. You can also use Ingress to expose multiple app services to a public or private network by using a single unique route. The Ingress API also supports TLS termination, virtual hosts, and path-based routing. Ingress consists of three components : Ingress resources, Application load balancers (ALBs), A load balancer to handle incoming requests across zones. To expose an app with Ingress, create a Kubernetes service of type LoadBalancer and register this Service with Ingress by defining an Ingress resource. One Ingress resource is required per namespace where you have apps that you want to expose. The Ingress resource is a Kubernetes resource that defines the rules for how to route incoming requests for apps. The Ingress resource also specifies the path to your app services. When you created a standard IKS cluster, an Ingress subdomain is already registered by default for your cluster. The paths to your app services are appended to the public route. In a standard cluster on IKS, the Ingress Application Load Balancer (ALB) is a layer 7 (L7) load balancer which implements the NGINX Ingress controller . In a Red Hat OpenShift Kubernetes Service (ROKS) cluster , the router is a layer 7 load balancer which implements an HAProxy Ingress controller . Create an Ingress Resource for the HelloWorld App \u00b6 Instead of using <external-ip>:<nodeport> to access the HelloWorld app, we want to access our HelloWorld aplication via the URL <Ingress-subdomain>:<nodeport>/<path> . To configure your Ingress resource, you need the Ingress Subdomain and Ingress Secret of your cluster. Both were already created by IKS when you created the cluster. INGRESS_SUBDOMAIN = $( ibmcloud ks nlb-dns ls --cluster $KS_CLUSTER_NAME --json | jq -r '.[0].nlbHost' ) echo $INGRESS_SUBDOMAIN INGRESS_SECRET = $( ibmcloud ks nlb-dns ls --cluster $KS_CLUSTER_NAME --json | jq -r '.[0].nlbSslSecretName' ) echo $INGRESS_SECRET Or, INGRESS_SUBDOMAIN = $( ibmcloud ks cluster get --show-resources -c $KS_CLUSTER_NAME --json | jq -r '.ingressHostname' ) echo $INGRESS_SUBDOMAIN INGRESS_SECRET = $( ibmcloud ks cluster get --show-resources -c $KS_CLUSTER_NAME --json | jq -r '.ingressSecretName' ) echo $INGRESS_SECRET Create the Ingress specification and change the hosts and host to the Ingress Subdomain of your cluster, and change the secretName to the value Ingress Secret of your cluster. Potentially, you can use annotations like rewrite path to customize an Ingress resource. See https://cloud.ibm.com/docs/containers?topic=containers-ingress_annotation . Find the Kubernetes version, $ oc get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10 .183.200.134 Ready master,worker 19h v1.17.1+40d7dbd 10 .183.200.134 169 .47.169.205 Red Hat 3 .10.0-1160.11.1.el7.x86_64 cri-o://1.17.5-11.rhaos4.4.git7f979af.el7 10 .183.200.181 Ready master,worker 19h v1.17.1+40d7dbd 10 .183.200.181 169 .47.169.203 Red Hat 3 .10.0-1160.11.1.el7.x86_64 cri-o://1.17.5-11.rhaos4.4.git7f979af.el7 In version 1.17 and 1.18 syntax, echo \"apiVersion: extensions/v1beta1 kind: Ingress metadata: name: helloworld-ingress annotations: kubernetes.io/ingress.class: \\\"public-iks-k8s-nginx\\\" spec: tls: - hosts: - $INGRESS_SUBDOMAIN secretName: $INGRESS_SECRET rules: - host: $INGRESS_SUBDOMAIN http: paths: - path: / backend: serviceName: helloworld servicePort: 8080\" > helloworld-ingress.yaml In version 1.19 syntax, echo \"apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: helloworld-ingress annotations: kubernetes.io/ingress.class: \" public-iks-k8s-nginx \" spec: tls: - hosts: - $INGRESS_SUBDOMAIN secretName: $INGRESS_SECRET rules: - host: $INGRESS_SUBDOMAIN http: paths: - path: / pathType: Prefix backend: service: name: helloworld port: number: 8080\" > helloworld-ingress.yaml The above resource will create an access path to the helloworld at http://$INGRESS_SUBDOMAIN:$PORT/ . You can further customize Ingres routing with annotations to customize the ALB settings, TLS settings, request and response annocations, service limits, user authentication, or error actions. Make sure, the values for the hosts , secretName and host are set correctly to match the values of the Ingress Subdomain and Secret of your cluster. Edit the helloworld-ingress.yaml file to make the necessary changes, Then create the Ingress for helloworld, $ oc create -f helloworld-ingress.yaml -n $MY_NS ingress.networking.k8s.io/helloworld-ingress created To find the service port again, NODE_PORT = $( oc get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $NODE_PORT Try to access the helloworld API and the proxy using the Ingress Subdomain with the path to the service, $ curl -L -X POST \"http:// $INGRESS_SUBDOMAIN : $NODE_PORT /api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"world3\" }' { \"id\" : \"c806432d-0f84-45bb-a654-0b6be0146044\" , \"sender\" : \"world3\" , \"message\" : \"Hello world3 (direct)\" , \"host\" :null } If you instead want to use subdomain paths instead of URI paths, echo \"apiVersion: extensions/v1beta1 kind: Ingress metadata: name: helloworld-ingress annotations: kubernetes.io/ingress.class: \\\"public-iks-k8s-nginx\\\" spec: tls: - hosts: - $INGRESS_SUBDOMAIN secretName: $INGRESS_SECRET rules: - host: >- $INGRESS_SUBDOMAIN http: paths: - backend: serviceName: helloworld servicePort: 8080 - host: >- hello. $INGRESS_SUBDOMAIN http: paths: - backend: serviceName: helloworld servicePort: 8080\" > helloworld-ingress-subdomain.yaml Delete the previous Ingress resource and create the Ingress resource using subdomain paths. oc get ingress -n $MY_NS oc delete ingress helloworld-ingress -n $MY_NS oc create -f helloworld-ingress-subdomain.yaml -n $MY_NS Try to access the application using the subdomain, $ curl -L -X POST \"http://hello. $INGRESS_SUBDOMAIN /api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"world4\" }' { \"id\" : \"ea9c00e9-190a-4d83-ab8a-cf6e81c1bb10\" , \"sender\" : \"world4\" , \"message\" : \"Hello world4 (direct)\" , \"host\" :null } Next \u00b6 Next, go to Route .","title":"Ingress and ALB"},{"location":"ingress/ingress-alb/#ingress-and-application-load-balancer-alb","text":"","title":"Ingress and Application Load Balancer (ALB)"},{"location":"ingress/ingress-alb/#pre-requisites","text":"Finish the Services , ClusterIP , NodePort and LoadBalancer labs. This should provide you with: Logged in to IBM Cloud account, Connected to Kubernetes cluster, Guestbook Deployment, Guestbook Service of type LoadBalancer.","title":"Pre-requisites"},{"location":"ingress/ingress-alb/#network-administration","text":"When you create a standard cluster in IBM Cloud Kubernetes Service (IKS), a portable public subnet and a portable private subnet for the VLAN are automatically provisioned. To retrieve the cluster id, you need the cluster name. If you do not know the cluster name already, you can list all clusters in the active account, ibmcloud ks clusters Create an environment variable and set the cluster name, KS_CLUSTER_NAME = <cluster name> echo $KS_CLUSTER_NAME The portable public subnet for your cluster on IBM Cloud provides 5 usable IP addresses. 1 portable public IP address is used by the default public Ingress ALB. The remaining 4 portable public IP addresses can be used to expose single apps to the internet by creating public Network Load Balancer (NLB) services. To list all of the portable IP addresses in the Kubernetes cluster, both used and available, you can retrieve the following ConfigMap in the kube-system namespace listing the resources of the subnets, $ oc get cm ibm-cloud-provider-vlan-ip-config -n kube-system -o yaml apiVersion: v1 data: cluster_id: c09nq3aw0cv06eckrvsg reserved_private_ip: \"\" reserved_private_vlan_id: \"\" reserved_public_ip: \"\" reserved_public_vlan_id: \"\" vlanipmap.json: | - { \"vlans\" : [ { \"id\" : \"2936320\" , \"subnets\" : [ { \"id\" : \"2448002\" , \"ips\" : [ \"10.216.29.66\" , \"10.216.29.67\" , \"10.216.29.68\" , \"10.216.29.69\" , \"10.216.29.70\" ] , \"is_public\" : false, \"is_byoip\" : false, \"cidr\" : \"10.216.29.64/29\" } ] , \"zone\" : \"wdc04\" , \"region\" : \"us-east\" } , { \"id\" : \"2936318\" , \"subnets\" : [ { \"id\" : \"2390180\" , \"ips\" : [ \"169.47.155.242\" , \"169.47.155.243\" , \"169.47.155.244\" , \"169.47.155.245\" , \"169.47.155.246\" ] , \"is_public\" : true, \"is_byoip\" : false, \"cidr\" : \"169.47.155.240/29\" } ] , \"zone\" : \"wdc04\" , \"region\" : \"us-east\" } ] , \"vlan_errors\" : [] , \"reserved_ips\" : [] } kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | { \"apiVersion\" : \"v1\" , \"data\" : { \"cluster_id\" : \"c09nq3aw0cv06eckrvsg\" , \"reserved_private_ip\" : \"\" , \"reserved_private_vlan_id\" : \"\" , \"reserved_public_ip\" : \"\" , \"reserved_public_vlan_id\" : \"\" , \"vlanipmap.json\" : \"{\\n \\\"vlans\\\": [\\n {\\n \\\"id\\\": \\\"2936320\\\",\\n \\\"subnets\\\": [\\n {\\n \\\"id\\\": \\\"2448002\\\",\\n \\\"ips\\\": [\\n \\\"10.216.29.66\\\",\\n \\\"10.216.29.67\\\",\\n \\\"10.216.29.68\\\",\\n \\\"10.216.29.69\\\",\\n \\\"10.216.29.70\\\"\\n ],\\n \\\"is_public\\\": false,\\n \\\"is_byoip\\\": false,\\n \\\"cidr\\\": \\\"10.216.29.64/29\\\"\\n }\\n ],\\n \\\"zone\\\": \\\"wdc04\\\",\\n \\\"region\\\": \\\"us-east\\\"\\n },\\n {\\n \\\"id\\\": \\\"2936318\\\",\\n \\\"subnets\\\": [\\n {\\n \\\"id\\\": \\\"2390180\\\",\\n \\\"ips\\\": [\\n \\\"169.47.155.242\\\",\\n \\\"169.47.155.243\\\",\\n \\\"169.47.155.244\\\",\\n \\\"169.47.155.245\\\",\\n \\\"169.47.155.246\\\"\\n ],\\n \\\"is_public\\\": true,\\n \\\"is_byoip\\\": false,\\n \\\"cidr\\\": \\\"169.47.155.240/29\\\"\\n }\\n ],\\n \\\"zone\\\": \\\"wdc04\\\",\\n \\\"region\\\": \\\"us-east\\\"\\n }\\n ],\\n \\\"vlan_errors\\\": [],\\n \\\"reserved_ips\\\": []\\n}\" } , \"kind\" : \"ConfigMap\" , \"metadata\" : { \"annotations\" : {} , \"labels\" : { \"addonmanager.kubernetes.io/mode\" : \"Reconcile\" , \"kubernetes.io/cluster-service\" : \"true\" } , \"name\" : \"ibm-cloud-provider-vlan-ip-config\" , \"namespace\" : \"kube-system\" }} creationTimestamp: \"2021-01-29T03:29:56Z\" labels: addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/cluster-service: \"true\" name: ibm-cloud-provider-vlan-ip-config namespace: kube-system resourceVersion: \"14367\" selfLink: /api/v1/namespaces/kube-system/configmaps/ibm-cloud-provider-vlan-ip-config uid: b2a8098e-61c5-4ced-a589-b2df1dcb6275 One of the public IP addresses on the public VLAN's subnet is assigned to the Network Load Balancer (NLB). List the registered NLB host name and IP address for your cluster (depending on the IBM Cloud CLI version, use --json or --output json ), $ ibmcloud ks nlb-dns ls --cluster $KS_CLUSTER_NAME --json [ { \"clusterID\" : \"c09nq3aw0cv06eckrvsg\" , \"nlbIP\" : \"[\\\"169.47.155.244\\\"]\" , \"nlbIPArray\" : [ \"169.47.155.244\" ] , \"nlbType\" : \"public\" , \"nlbHost\" : \"dte-ocp44-on2g3r-915b3b336cabec458a7c7ec2aa7c625f-0000.us-east.containers.appdomain.cloud\" , \"secretNamespace\" : \"openshift-ingress\" , \"nlbMonitorState\" : \"None\" , \"nlbSslSecretName\" : \"dte-ocp44-on2g3r-915b3b336cabec458a7c7ec2aa7c625f-0000\" , \"nlbSslSecretStatus\" : \"created\" } ] And retrieve the NodePort via, NODE_PORT = $( oc get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $NODE_PORT You see that the portable IP address was assigned to the NLB of the LoadBalancer service.","title":"Network Administration"},{"location":"ingress/ingress-alb/#ingress-alb","text":"Ingress is a reverse-proxy load balancer and Kubernetes API object that manages external access to services in a cluster. You can also use Ingress to expose multiple app services to a public or private network by using a single unique route. The Ingress API also supports TLS termination, virtual hosts, and path-based routing. Ingress consists of three components : Ingress resources, Application load balancers (ALBs), A load balancer to handle incoming requests across zones. To expose an app with Ingress, create a Kubernetes service of type LoadBalancer and register this Service with Ingress by defining an Ingress resource. One Ingress resource is required per namespace where you have apps that you want to expose. The Ingress resource is a Kubernetes resource that defines the rules for how to route incoming requests for apps. The Ingress resource also specifies the path to your app services. When you created a standard IKS cluster, an Ingress subdomain is already registered by default for your cluster. The paths to your app services are appended to the public route. In a standard cluster on IKS, the Ingress Application Load Balancer (ALB) is a layer 7 (L7) load balancer which implements the NGINX Ingress controller . In a Red Hat OpenShift Kubernetes Service (ROKS) cluster , the router is a layer 7 load balancer which implements an HAProxy Ingress controller .","title":"Ingress ALB"},{"location":"ingress/ingress-alb/#create-an-ingress-resource-for-the-helloworld-app","text":"Instead of using <external-ip>:<nodeport> to access the HelloWorld app, we want to access our HelloWorld aplication via the URL <Ingress-subdomain>:<nodeport>/<path> . To configure your Ingress resource, you need the Ingress Subdomain and Ingress Secret of your cluster. Both were already created by IKS when you created the cluster. INGRESS_SUBDOMAIN = $( ibmcloud ks nlb-dns ls --cluster $KS_CLUSTER_NAME --json | jq -r '.[0].nlbHost' ) echo $INGRESS_SUBDOMAIN INGRESS_SECRET = $( ibmcloud ks nlb-dns ls --cluster $KS_CLUSTER_NAME --json | jq -r '.[0].nlbSslSecretName' ) echo $INGRESS_SECRET Or, INGRESS_SUBDOMAIN = $( ibmcloud ks cluster get --show-resources -c $KS_CLUSTER_NAME --json | jq -r '.ingressHostname' ) echo $INGRESS_SUBDOMAIN INGRESS_SECRET = $( ibmcloud ks cluster get --show-resources -c $KS_CLUSTER_NAME --json | jq -r '.ingressSecretName' ) echo $INGRESS_SECRET Create the Ingress specification and change the hosts and host to the Ingress Subdomain of your cluster, and change the secretName to the value Ingress Secret of your cluster. Potentially, you can use annotations like rewrite path to customize an Ingress resource. See https://cloud.ibm.com/docs/containers?topic=containers-ingress_annotation . Find the Kubernetes version, $ oc get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10 .183.200.134 Ready master,worker 19h v1.17.1+40d7dbd 10 .183.200.134 169 .47.169.205 Red Hat 3 .10.0-1160.11.1.el7.x86_64 cri-o://1.17.5-11.rhaos4.4.git7f979af.el7 10 .183.200.181 Ready master,worker 19h v1.17.1+40d7dbd 10 .183.200.181 169 .47.169.203 Red Hat 3 .10.0-1160.11.1.el7.x86_64 cri-o://1.17.5-11.rhaos4.4.git7f979af.el7 In version 1.17 and 1.18 syntax, echo \"apiVersion: extensions/v1beta1 kind: Ingress metadata: name: helloworld-ingress annotations: kubernetes.io/ingress.class: \\\"public-iks-k8s-nginx\\\" spec: tls: - hosts: - $INGRESS_SUBDOMAIN secretName: $INGRESS_SECRET rules: - host: $INGRESS_SUBDOMAIN http: paths: - path: / backend: serviceName: helloworld servicePort: 8080\" > helloworld-ingress.yaml In version 1.19 syntax, echo \"apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: helloworld-ingress annotations: kubernetes.io/ingress.class: \" public-iks-k8s-nginx \" spec: tls: - hosts: - $INGRESS_SUBDOMAIN secretName: $INGRESS_SECRET rules: - host: $INGRESS_SUBDOMAIN http: paths: - path: / pathType: Prefix backend: service: name: helloworld port: number: 8080\" > helloworld-ingress.yaml The above resource will create an access path to the helloworld at http://$INGRESS_SUBDOMAIN:$PORT/ . You can further customize Ingres routing with annotations to customize the ALB settings, TLS settings, request and response annocations, service limits, user authentication, or error actions. Make sure, the values for the hosts , secretName and host are set correctly to match the values of the Ingress Subdomain and Secret of your cluster. Edit the helloworld-ingress.yaml file to make the necessary changes, Then create the Ingress for helloworld, $ oc create -f helloworld-ingress.yaml -n $MY_NS ingress.networking.k8s.io/helloworld-ingress created To find the service port again, NODE_PORT = $( oc get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $NODE_PORT Try to access the helloworld API and the proxy using the Ingress Subdomain with the path to the service, $ curl -L -X POST \"http:// $INGRESS_SUBDOMAIN : $NODE_PORT /api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"world3\" }' { \"id\" : \"c806432d-0f84-45bb-a654-0b6be0146044\" , \"sender\" : \"world3\" , \"message\" : \"Hello world3 (direct)\" , \"host\" :null } If you instead want to use subdomain paths instead of URI paths, echo \"apiVersion: extensions/v1beta1 kind: Ingress metadata: name: helloworld-ingress annotations: kubernetes.io/ingress.class: \\\"public-iks-k8s-nginx\\\" spec: tls: - hosts: - $INGRESS_SUBDOMAIN secretName: $INGRESS_SECRET rules: - host: >- $INGRESS_SUBDOMAIN http: paths: - backend: serviceName: helloworld servicePort: 8080 - host: >- hello. $INGRESS_SUBDOMAIN http: paths: - backend: serviceName: helloworld servicePort: 8080\" > helloworld-ingress-subdomain.yaml Delete the previous Ingress resource and create the Ingress resource using subdomain paths. oc get ingress -n $MY_NS oc delete ingress helloworld-ingress -n $MY_NS oc create -f helloworld-ingress-subdomain.yaml -n $MY_NS Try to access the application using the subdomain, $ curl -L -X POST \"http://hello. $INGRESS_SUBDOMAIN /api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"world4\" }' { \"id\" : \"ea9c00e9-190a-4d83-ab8a-cf6e81c1bb10\" , \"sender\" : \"world4\" , \"message\" : \"Hello world4 (direct)\" , \"host\" :null }","title":"Create an Ingress Resource for the HelloWorld App"},{"location":"ingress/ingress-alb/#next","text":"Next, go to Route .","title":"Next"},{"location":"ingress/ingress-extra/","text":"Ingress Extra \u00b6 Allocation of an external IP address requires the system to create an external IP address, a forwarding rule, a target proxy, a backend service, and possibly an instance group. Once the IP address has been allocated you can connect to your service through it, assign it a domain name and distribute it to clients. Ingress Subdomain \u00b6 By default, IKS created an Ingress subdomain already when you created the cluster. You can also create a new Ingress subdomain or NLB hostname in addition, % kubectl config current-context % ibmcloud ks nlb-dns create classic -cluster remkohdev-iks116-3x-cluster --ip 169 .48.75.82 OK NLB hostname was created as remkohdev-iks116-3x-clu-2bef1f4b4097001da9502000c44fc2b2-0001.us-south.containers.appdomain.cloud There is also the option to set up Direct Server Return (DSR) load balancing with an NLB 2.0 see here . When you create a Kubernetes LoadBalancer service for an app in your IKS cluster, a layer 7 Virtual Private Cloud (VPC) load balancer is automatically created in your VPC outside of your cluster. The VPC load balancer is multizonal and routes requests for your app through the private NodePorts that are automatically opened on your worker nodes. In a free cluster on IKS, the cluster's worker nodes are connected to an IBM-owned public VLAN and private VLAN by default. Because IBM controls the VLANs, subnets, and IP addresses, you cannot create multizone clusters or add subnets to your cluster, and can use only NodePort services to expose your app. If your cluster is a free cluster or only has 1 worker node, the EXTERNAL-IP will say pending because in vain the cluster is waiting on the asynchronous response. The portable public subnet provides 5 usable IP addresses. 1 portable public IP address is used by the default public Ingress ALB. The remaining 4 portable public IP addresses can be used to expose single apps to the internet by creating public network load balancer services, or NLBs. The portable private subnet provides 5 usable IP addresses. 1 portable private IP address is used by the default private Ingress ALB. The remaining 4 portable private IP addresses can be used to expose single apps to a private network by creating private load balancer services, or NLBs. Resources \u00b6 Components and architecture of an NLB 1.0 . Choosing an app exposure service, choosing a deployment pattern for classic clusters . Quick start for load balancers Classic: Setting up DSR load balancing with an NLB 2.0 (beta)","title":"Ingress Extra"},{"location":"ingress/ingress-extra/#ingress-extra","text":"Allocation of an external IP address requires the system to create an external IP address, a forwarding rule, a target proxy, a backend service, and possibly an instance group. Once the IP address has been allocated you can connect to your service through it, assign it a domain name and distribute it to clients.","title":"Ingress Extra"},{"location":"ingress/ingress-extra/#ingress-subdomain","text":"By default, IKS created an Ingress subdomain already when you created the cluster. You can also create a new Ingress subdomain or NLB hostname in addition, % kubectl config current-context % ibmcloud ks nlb-dns create classic -cluster remkohdev-iks116-3x-cluster --ip 169 .48.75.82 OK NLB hostname was created as remkohdev-iks116-3x-clu-2bef1f4b4097001da9502000c44fc2b2-0001.us-south.containers.appdomain.cloud There is also the option to set up Direct Server Return (DSR) load balancing with an NLB 2.0 see here . When you create a Kubernetes LoadBalancer service for an app in your IKS cluster, a layer 7 Virtual Private Cloud (VPC) load balancer is automatically created in your VPC outside of your cluster. The VPC load balancer is multizonal and routes requests for your app through the private NodePorts that are automatically opened on your worker nodes. In a free cluster on IKS, the cluster's worker nodes are connected to an IBM-owned public VLAN and private VLAN by default. Because IBM controls the VLANs, subnets, and IP addresses, you cannot create multizone clusters or add subnets to your cluster, and can use only NodePort services to expose your app. If your cluster is a free cluster or only has 1 worker node, the EXTERNAL-IP will say pending because in vain the cluster is waiting on the asynchronous response. The portable public subnet provides 5 usable IP addresses. 1 portable public IP address is used by the default public Ingress ALB. The remaining 4 portable public IP addresses can be used to expose single apps to the internet by creating public network load balancer services, or NLBs. The portable private subnet provides 5 usable IP addresses. 1 portable private IP address is used by the default private Ingress ALB. The remaining 4 portable private IP addresses can be used to expose single apps to a private network by creating private load balancer services, or NLBs.","title":"Ingress Subdomain"},{"location":"ingress/ingress-extra/#resources","text":"Components and architecture of an NLB 1.0 . Choosing an app exposure service, choosing a deployment pattern for classic clusters . Quick start for load balancers Classic: Setting up DSR load balancing with an NLB 2.0 (beta)","title":"Resources"},{"location":"opa/opa/","text":"Open Policy Agent (OPA) \u00b6 TBD","title":"Open Policy Agent (OPA)"},{"location":"opa/opa/#open-policy-agent-opa","text":"TBD","title":"Open Policy Agent (OPA)"},{"location":"references/contributors/","text":"Contributors \u00b6 Remko de Knikker, remkohdev , Masa Abushamleh, nerdingitout Tim Robinson, timroster ,","title":"Contributors"},{"location":"references/contributors/#contributors","text":"Remko de Knikker, remkohdev , Masa Abushamleh, nerdingitout Tim Robinson, timroster ,","title":"Contributors"},{"location":"route/route/","text":"Route \u00b6 Services of type LoadBalancer cannot do TLS termination, virtual hosts or path-based routing. These limitations led to the addition in Kubernetes v1.2 of a separate kubernetes resource called Ingress and Route (on OpenShift). OpenShift's Route was created for the same purpose as the Kubernetes Ingress resource, with a few additional capabilities such as splitting traffic between multiple backends, and sticky sessions. In a Red Hat OpenShift Kubernetes Service (ROKS) cluster , the router is a layer 7 load balancer which implements an HAProxy Ingress controller . When a Route is created, the built-in HAProxy load balancer picks it up in order to expose the requested service. The RouteSpec for a Route describes the hostname or path the route exposes, security information, and one to four services the route points to. Requests are distributed among the backends depending on the weights assigned to each backend. Weights are between 0 and 256 with default 100. The tls field is optional and allows specific certificates or behavior for the route. A router uses the service selector to find the service and the endpoints backing the service. Routes can be sharded among the set of routers. Each router in the group serves only a subset of traffic. OpenShift defines four types of Routes based on the type of TLS termination that your app requires: Simple for non-encrypted HTTP traffic, require no key or certificates. Passthrough when no TLS termination for encrypted HTTPS traffic is needed, TLS termination is done by the app, Edge, TLS connection between the client and the router service is terminated, and the connection between the router service and your app pod is unencrypted. Re-encrypt, TLS connection between the client and the router service is terminated, and a new TLS connection between the router service and your app pod is created. Create a Route \u00b6 Expose the helloworld service as a Route by using the oc expose command. $ oc expose service helloworld -n $MY_NS route.route.openshift.io/helloworld exposed Get all routes, $ oc get routes -n $MY_NS NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD helloworld helloworld-my-apps.remkohdev-roks45-2n-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud helloworld http-server None Retrieve the created host for the Route and the NodePort of the helloworld service, ROUTE = $( oc get routes -n $MY_NS -o json | jq -r '.items[0].spec.host' ) echo $ROUTE NODE_PORT = $( oc get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $NODE_PORT Send a request to the Route host for your Service, $ curl -L -X POST \"http:// $ROUTE : $NODE_PORT /api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"world5\" }' { \"id\" : \"ba8b9101-6594-420b-8cab-251dc7a2de65\" , \"sender\" : \"world5\" , \"message\" : \"Hello world5 (direct)\" , \"host\" :null } Next \u00b6 Next, go to Network Policy .","title":"Route"},{"location":"route/route/#route","text":"Services of type LoadBalancer cannot do TLS termination, virtual hosts or path-based routing. These limitations led to the addition in Kubernetes v1.2 of a separate kubernetes resource called Ingress and Route (on OpenShift). OpenShift's Route was created for the same purpose as the Kubernetes Ingress resource, with a few additional capabilities such as splitting traffic between multiple backends, and sticky sessions. In a Red Hat OpenShift Kubernetes Service (ROKS) cluster , the router is a layer 7 load balancer which implements an HAProxy Ingress controller . When a Route is created, the built-in HAProxy load balancer picks it up in order to expose the requested service. The RouteSpec for a Route describes the hostname or path the route exposes, security information, and one to four services the route points to. Requests are distributed among the backends depending on the weights assigned to each backend. Weights are between 0 and 256 with default 100. The tls field is optional and allows specific certificates or behavior for the route. A router uses the service selector to find the service and the endpoints backing the service. Routes can be sharded among the set of routers. Each router in the group serves only a subset of traffic. OpenShift defines four types of Routes based on the type of TLS termination that your app requires: Simple for non-encrypted HTTP traffic, require no key or certificates. Passthrough when no TLS termination for encrypted HTTPS traffic is needed, TLS termination is done by the app, Edge, TLS connection between the client and the router service is terminated, and the connection between the router service and your app pod is unencrypted. Re-encrypt, TLS connection between the client and the router service is terminated, and a new TLS connection between the router service and your app pod is created.","title":"Route"},{"location":"route/route/#create-a-route","text":"Expose the helloworld service as a Route by using the oc expose command. $ oc expose service helloworld -n $MY_NS route.route.openshift.io/helloworld exposed Get all routes, $ oc get routes -n $MY_NS NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD helloworld helloworld-my-apps.remkohdev-roks45-2n-clu-2bef1f4b4097001da9502000c44fc2b2-0000.us-south.containers.appdomain.cloud helloworld http-server None Retrieve the created host for the Route and the NodePort of the helloworld service, ROUTE = $( oc get routes -n $MY_NS -o json | jq -r '.items[0].spec.host' ) echo $ROUTE NODE_PORT = $( oc get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $NODE_PORT Send a request to the Route host for your Service, $ curl -L -X POST \"http:// $ROUTE : $NODE_PORT /api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"world5\" }' { \"id\" : \"ba8b9101-6594-420b-8cab-251dc7a2de65\" , \"sender\" : \"world5\" , \"message\" : \"Hello world5 (direct)\" , \"host\" :null }","title":"Create a Route"},{"location":"route/route/#next","text":"Next, go to Network Policy .","title":"Next"},{"location":"route/secured-routes/","text":"Secured Routes \u00b6 Introduction \u00b6 In OpenShift, there are different types of routes in which you can expose your applications: clear, edge, re-encrypt, and pass-through. The clear route is insecure and doesn't require any certifications, as for the rest of the routes, they are encrypted on different levels and require certificates. In this tutorial, you will learn how to create 3 types of routes for your applications: clear, edge and passthroug and you will learn the difference in creating each type of route. Prerequisites \u00b6 For this tutorial you will need: Sign up for your IBM Cloud account Red Hat OpenShift Cluster 4 on IBM Cloud. oc CLI (can be downloaded from this link or you can use it at http://shell.cloud.ibm.com/ . Estimated Time \u00b6 It will take you around 30 minutes to complete this tutorial. Steps: Login from the CLI & Create Project Setting up Create Application Expose the Route Extract the SSL Cert Secret Create Edge Route Create Golang Application Create Passthrough Route Login from the CLI & Create Project \u00b6 Go to the web console and click on your username at the top right then 'Copy Login Command', then display the token and copy the oc login command in your terminal. Create my-route-project project. oc new-project my-route-project Setting up \u00b6 In this section, you will view details about your OpenShift Cluster on IBM Cloud. Details you would be interested in are hostname, SSL Cert Secret Name, and the namespace that holds the secret. First, this is the Overview page that shows some details about your cluster like Cluster ID and resource group they can be useful when using ibmcloud CLI to grab some information about your cluster. on the top right, click on your profile and select \"Log in to CLI and API\" Copy the IBM Cloud CLI login command and paste it in your CLI Select the resource group where your cluster resides (in my case it is default) ibmcloud target -g default View information about your cluster ibmcloud oc nlb-dns ls --cluster <cluster_name_or_id> Create Application \u00b6 Create a new deployment resource using the ibmcom/guestbook:v2 docker image in the project we just created. oc new-app myguestbook --image = ibmcom/guestbook:v1 This deployment creates the corresponding Pod that's in running state. Use the following command to see the list of pods in your namespace. oc get pods reate a Kubernetes ClusterIP service for your app deployment. The service provides an internal IP address for the app that the router can send traffic to. oc expose deployment myguestbook --type = \"NodePort\" --port = 3000 Expose the route \u00b6 To view the service that we need to expose. Use the following command. oc get svc Notice that the application isn't accessible externally, we have only exposed the deployment internally, to make it externally accessible use the following command oc expose svc myguestbook Now to get the route using the following command oc get routes copy and paste the link in your browser, you will be redirected to a web page like the following screenshot. Notice that the webpage is not secure because we haven't used any type of encryption yet. Since you will be using the same application to create an edge route, make sure to delete the route before moving to the next step oc delete route myguestbook Extract the SSL Cert Secret \u00b6 Now let's take a look at the secrets in openshift-ingress project. You will need a TLS secret that's generated for your cluster which is of type kubernetes.io/tls. oc get secrets -n openshift-ingress View the secret values in your command line, notice that the key and certificate pair are saved in PEM encoded files. oc extract secret/<YOUR-TLS-SECRET-NAME> --to * -n openshift-ingress Save the secret in a temporary directory oc extract secret/<YOUR-TLS-SECRET-NAME> --to = /tmp -n openshift-ingress Create Edge Route \u00b6 Create the edge route using the following command oc create route edge --service myguestbook --key /tmp/tls.key --cert /tmp/tls.crt Get the route of your application and open it from your browser oc get routes The application has been deployed successfully You can check information about the secured website and certificate from the lock icon at the top left of the browser Create Golang Application \u00b6 In this section, you will be deploying a new application that you will be using for both passthrough and re-encrypt routes, then you will create a secret and mount it to the volume so you can create the routes. Create the deployment config and service using oc create command. oc create -f https://raw.githubusercontent.com/nerdingitout/oc-route/main/golang-https.yml Create TLS secret using the same secret you extracted earlier. oc create secret tls mycert --cert /tmp/tls.crt --key /tmp/tls.key Mount the secret to your volume. oc set volume dc/golang-https --add -t secret -m /go/src/app/certs --name cert --secret-name mycert Create Passthrough Route \u00b6 Create the passthrough route oc create route passthrough golang-https --service golang-https Get the URL oc get routes Access the application and view the certificate Resources \u00b6 You can learn more using the following resources: OpenShift Routes on IBM Cloud OpenSSL Self-Serviced End-to-end Encryption Approaches for Applications Deployed in OpenShift Secured Routes End to End Encryption with OpenShift: Part 1 End to End Encryption with OpenShift: Part 2","title":"Route"},{"location":"route/secured-routes/#secured-routes","text":"","title":"Secured Routes"},{"location":"route/secured-routes/#introduction","text":"In OpenShift, there are different types of routes in which you can expose your applications: clear, edge, re-encrypt, and pass-through. The clear route is insecure and doesn't require any certifications, as for the rest of the routes, they are encrypted on different levels and require certificates. In this tutorial, you will learn how to create 3 types of routes for your applications: clear, edge and passthroug and you will learn the difference in creating each type of route.","title":"Introduction"},{"location":"route/secured-routes/#prerequisites","text":"For this tutorial you will need: Sign up for your IBM Cloud account Red Hat OpenShift Cluster 4 on IBM Cloud. oc CLI (can be downloaded from this link or you can use it at http://shell.cloud.ibm.com/ .","title":"Prerequisites"},{"location":"route/secured-routes/#estimated-time","text":"It will take you around 30 minutes to complete this tutorial. Steps: Login from the CLI & Create Project Setting up Create Application Expose the Route Extract the SSL Cert Secret Create Edge Route Create Golang Application Create Passthrough Route","title":"Estimated Time"},{"location":"route/secured-routes/#login-from-the-cli-create-project","text":"Go to the web console and click on your username at the top right then 'Copy Login Command', then display the token and copy the oc login command in your terminal. Create my-route-project project. oc new-project my-route-project","title":"Login from the CLI &amp; Create Project"},{"location":"route/secured-routes/#setting-up","text":"In this section, you will view details about your OpenShift Cluster on IBM Cloud. Details you would be interested in are hostname, SSL Cert Secret Name, and the namespace that holds the secret. First, this is the Overview page that shows some details about your cluster like Cluster ID and resource group they can be useful when using ibmcloud CLI to grab some information about your cluster. on the top right, click on your profile and select \"Log in to CLI and API\" Copy the IBM Cloud CLI login command and paste it in your CLI Select the resource group where your cluster resides (in my case it is default) ibmcloud target -g default View information about your cluster ibmcloud oc nlb-dns ls --cluster <cluster_name_or_id>","title":"Setting up"},{"location":"route/secured-routes/#create-application","text":"Create a new deployment resource using the ibmcom/guestbook:v2 docker image in the project we just created. oc new-app myguestbook --image = ibmcom/guestbook:v1 This deployment creates the corresponding Pod that's in running state. Use the following command to see the list of pods in your namespace. oc get pods reate a Kubernetes ClusterIP service for your app deployment. The service provides an internal IP address for the app that the router can send traffic to. oc expose deployment myguestbook --type = \"NodePort\" --port = 3000","title":"Create Application"},{"location":"route/secured-routes/#expose-the-route","text":"To view the service that we need to expose. Use the following command. oc get svc Notice that the application isn't accessible externally, we have only exposed the deployment internally, to make it externally accessible use the following command oc expose svc myguestbook Now to get the route using the following command oc get routes copy and paste the link in your browser, you will be redirected to a web page like the following screenshot. Notice that the webpage is not secure because we haven't used any type of encryption yet. Since you will be using the same application to create an edge route, make sure to delete the route before moving to the next step oc delete route myguestbook","title":"Expose the route"},{"location":"route/secured-routes/#extract-the-ssl-cert-secret","text":"Now let's take a look at the secrets in openshift-ingress project. You will need a TLS secret that's generated for your cluster which is of type kubernetes.io/tls. oc get secrets -n openshift-ingress View the secret values in your command line, notice that the key and certificate pair are saved in PEM encoded files. oc extract secret/<YOUR-TLS-SECRET-NAME> --to * -n openshift-ingress Save the secret in a temporary directory oc extract secret/<YOUR-TLS-SECRET-NAME> --to = /tmp -n openshift-ingress","title":"Extract the SSL Cert Secret"},{"location":"route/secured-routes/#create-edge-route","text":"Create the edge route using the following command oc create route edge --service myguestbook --key /tmp/tls.key --cert /tmp/tls.crt Get the route of your application and open it from your browser oc get routes The application has been deployed successfully You can check information about the secured website and certificate from the lock icon at the top left of the browser","title":"Create Edge Route"},{"location":"route/secured-routes/#create-golang-application","text":"In this section, you will be deploying a new application that you will be using for both passthrough and re-encrypt routes, then you will create a secret and mount it to the volume so you can create the routes. Create the deployment config and service using oc create command. oc create -f https://raw.githubusercontent.com/nerdingitout/oc-route/main/golang-https.yml Create TLS secret using the same secret you extracted earlier. oc create secret tls mycert --cert /tmp/tls.crt --key /tmp/tls.key Mount the secret to your volume. oc set volume dc/golang-https --add -t secret -m /go/src/app/certs --name cert --secret-name mycert","title":"Create Golang Application"},{"location":"route/secured-routes/#create-passthrough-route","text":"Create the passthrough route oc create route passthrough golang-https --service golang-https Get the URL oc get routes Access the application and view the certificate","title":"Create Passthrough Route"},{"location":"route/secured-routes/#resources","text":"You can learn more using the following resources: OpenShift Routes on IBM Cloud OpenSSL Self-Serviced End-to-end Encryption Approaches for Applications Deployed in OpenShift Secured Routes End to End Encryption with OpenShift: Part 1 End to End Encryption with OpenShift: Part 2","title":"Resources"},{"location":"services/clusterip/","text":"ClusterIP \u00b6 Add a Service to helloworld \u00b6 Now we have a basic understanding of service discovery and the different ServiceTypes on Kubernetes, it is time to expose the Deployment of helloworld using a new Service specification. Let's look at the definition for the helloworld-service.yaml , cat helloworld-service.yaml apiVersion : v1 kind : Service metadata : name : helloworld labels : app : helloworld spec : ports : - port : 8080 targetPort : http-server selector : app : helloworld The spec defines a few important attributes for service discovery: labels , selector and port . The set of Pods that a Service targets, is determined by the selector and labels . When a Service has no selector, the corresponding Endpoints object is not created automatically. The Service maps the incoming port to the container's targetPort . By default the targetPort is set to the same value as the incoming port field. Create the Service object with the default type, $ oc create -f helloworld-service.yaml -n $MY_NS service/helloworld created Describe the Service, $ oc describe svc helloworld -n $MY_NS Name: helloworld Namespace: my-apps Labels: app = helloworld Annotations: <none> Selector: app = helloworld Type: ClusterIP IP: 172 .21.86.16 Port: <unset> 8080 /TCP TargetPort: http-server/TCP Endpoints: 172 .30.172.228:8080,172.30.234.176:8080,172.30.234.177:8080 Session Affinity: None Events: <none> You see that Kubernetes by default creates a Service of type ClusterIP . The service is now available and discoverable, but only within the cluster, using the Endpoints and port mapping found via the selector and labels . Next \u00b6 Go to NodePort to learn more about ServiceType NodePort.","title":"ClusterIP"},{"location":"services/clusterip/#clusterip","text":"","title":"ClusterIP"},{"location":"services/clusterip/#add-a-service-to-helloworld","text":"Now we have a basic understanding of service discovery and the different ServiceTypes on Kubernetes, it is time to expose the Deployment of helloworld using a new Service specification. Let's look at the definition for the helloworld-service.yaml , cat helloworld-service.yaml apiVersion : v1 kind : Service metadata : name : helloworld labels : app : helloworld spec : ports : - port : 8080 targetPort : http-server selector : app : helloworld The spec defines a few important attributes for service discovery: labels , selector and port . The set of Pods that a Service targets, is determined by the selector and labels . When a Service has no selector, the corresponding Endpoints object is not created automatically. The Service maps the incoming port to the container's targetPort . By default the targetPort is set to the same value as the incoming port field. Create the Service object with the default type, $ oc create -f helloworld-service.yaml -n $MY_NS service/helloworld created Describe the Service, $ oc describe svc helloworld -n $MY_NS Name: helloworld Namespace: my-apps Labels: app = helloworld Annotations: <none> Selector: app = helloworld Type: ClusterIP IP: 172 .21.86.16 Port: <unset> 8080 /TCP TargetPort: http-server/TCP Endpoints: 172 .30.172.228:8080,172.30.234.176:8080,172.30.234.177:8080 Session Affinity: None Events: <none> You see that Kubernetes by default creates a Service of type ClusterIP . The service is now available and discoverable, but only within the cluster, using the Endpoints and port mapping found via the selector and labels .","title":"Add a Service to helloworld"},{"location":"services/clusterip/#next","text":"Go to NodePort to learn more about ServiceType NodePort.","title":"Next"},{"location":"services/externalname/","text":"External Name \u00b6 Pre-requisites \u00b6 Finish the Services , ClusterIP , NodePort , and LoadBalancer labs. About External Name \u00b6 An ExternalName Service is a special case of Service that does not have selectors and uses DNS names instead, e.g. apiVersion : v1 kind : Service metadata : name : my-database-svc namespace : prod spec : type : ExternalName externalName : my.database.example.com When looking up the service my-database-svc.prod.svc.cluster.local , the cluster DNS Service returns a CNAME record for my.database.example.com . Next \u00b6 Next, go to Ingress .","title":"ExternalName"},{"location":"services/externalname/#external-name","text":"","title":"External Name"},{"location":"services/externalname/#pre-requisites","text":"Finish the Services , ClusterIP , NodePort , and LoadBalancer labs.","title":"Pre-requisites"},{"location":"services/externalname/#about-external-name","text":"An ExternalName Service is a special case of Service that does not have selectors and uses DNS names instead, e.g. apiVersion : v1 kind : Service metadata : name : my-database-svc namespace : prod spec : type : ExternalName externalName : my.database.example.com When looking up the service my-database-svc.prod.svc.cluster.local , the cluster DNS Service returns a CNAME record for my.database.example.com .","title":"About External Name"},{"location":"services/externalname/#next","text":"Next, go to Ingress .","title":"Next"},{"location":"services/loadbalancer/","text":"Loadbalancer and Network Load Balancer (NLB) 1.0 \u00b6 Pre-requisites \u00b6 Finish the Services , ClusterIP , and NodePort labs. LoadBalancer \u00b6 In the previous labs, you created a service for the helloworld application with a default clusterIP and then added a NodePort to the Service, which proxies requests to the Service resource. But in a production environment, you still need a type of load balancer, whether client requests are internal or external coming in over the public network. The LoadBalancer service in Kubernetes configures an Open Systems Interconnection (OSI) model Layer 4 (L4) load balancer, which forwards and balances traffic from the internet to your backend application. To use a load balancer for distributing client traffic to the nodes in a cluster, you need a public IP address that the clients can connect to, and you need IP addresses on the nodes themselves to which the load balancer can forward the requests. When you create a standard cluster, IKS and ROKS automatically provision a portable public subnet and a portable private subnet. The portable public subnet provides 5 usable IP addresses. 1 portable public IP address is used by the default public Ingress ALB . The remaining 4 portable public IP addresses can be used to expose single apps using layer 4 (L4) TCP/UDP Network Load Balancer (NLB). The portable public and private IP addresses are static floating IPs pointing to worker nodes. A Keepalived daemon constantly monitors the IP, and automatically moves the IP to another worker node if the worker node is removed. See: Classic: About network load balancers (NLBs) . Services of type LoadBalancer have some limitations. They cannot do TLS termination, do virtual hosts or path-based routing, so you can\u2019t use a single load balancer to proxy to multiple services. These limitations led to the addition in Kubernetes v1.2 of a separate kubernetes resource called Ingress and Route (on OpenShift). Because, the creation of the load balancer happens asynchronously with the creation of the Service, you might have to wait until both the Service and the load balancer have been created. Load Balancer on IBM Cloud \u00b6 The LoadBalancer service type is implemented differently depending on your cluster's infrastructure provider. On IBM Kubernetes Service (IKS) and Red Hat OpenShift Kubernetes Service (ROKS) on IBM Cloud, classic clusters implement by default a Network Load Balancer (NLB) 1.0. Version 1.0 NLBs use Network Address Translation (NAT) to rewrite the request packet's source IP address to the IP of worker node where a load balancer pod exists. Version 2.0 NLBs doesn't use NAT but IP over IP (IPIP) to encapsulate the original request packet into another packet, which preserves the client IP as its source IP address. The worker node then uses Direct Server Return (DSR) to send the app response packet back to the client IP. Load Balancing Methods on IBM Cloud \u00b6 Before we create the LoadBalancer service for the helloworld application, review some of the Load Balancing options on Kubernetes in IBM Cloud: NodePort exposes the app via a port and public IP address on a worker node using kube-proxy . LoadBalancer NLB v1.0 uses basic load balancing that exposes the app with an IP address or a subdomain. LoadBalancer NLB v2.0 , uses Direct Server Return (DSR) load balancing to expose the app with an IP address or a subdomain, and supports SSL/TLS termination. Istio Ingress Gateway + NLB uses basic load balancing that exposes the app with a subdomain and uses Istio routing rules. Ingress with public ALB uses HTTPS load balancing that exposes the app with a subdomain and uses custom routing rules and SSL/TLS termination for multiple apps. You can customize the ALB routing rules with annotations . Custom Ingress + NLB uses HTTPS load balancing with a custom Ingress that exposes the app with the IBM-provided ALB subdomain and uses custom routing rules. Create a LoadBalancer \u00b6 In the previous lab, you already created a NodePort Service. $ oc get svc -n $MY_NS NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE helloworld NodePort 172 .21.86.16 <none> 8080 :32387/TCP 12m Patch the service for helloworld and change the type to LoadBalancer . oc patch svc helloworld -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' -n $MY_NS The TYPE should be set to LoadBalancer now, and an EXTERNAL-IP should be assigned. $ oc get svc helloworld -n $MY_NS NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE helloworld LoadBalancer 172 .21.86.16 169 .47.155.242 8080 :32387/TCP 12m To access the Service of the helloworld from the public internet, you can use the public IP address of the NLB and the assigned NodePort of the service in the format <IP_address>:<NodePort> . PUBLIC_IP = $( oc get svc helloworld -n $MY_NS --output json | jq -r '.status.loadBalancer.ingress[0].ip' ) echo $PUBLIC_IP NODE_PORT = $( oc get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $NODE_PORT Access the helloworld app in a browser or with Curl, $ curl -L -X POST \"http:// $PUBLIC_IP : $NODE_PORT /api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"world2\" }' { \"id\" : \"0ebdc166-32cd-4d0d-93b6-f278e4405c6f\" , \"sender\" : \"world2\" , \"message\" : \"Hello world2 (direct)\" , \"host\" :null } Next \u00b6 Next, go to ExternalName .","title":"Loadbalancer and NLB"},{"location":"services/loadbalancer/#loadbalancer-and-network-load-balancer-nlb-10","text":"","title":"Loadbalancer and Network Load Balancer (NLB) 1.0"},{"location":"services/loadbalancer/#pre-requisites","text":"Finish the Services , ClusterIP , and NodePort labs.","title":"Pre-requisites"},{"location":"services/loadbalancer/#loadbalancer","text":"In the previous labs, you created a service for the helloworld application with a default clusterIP and then added a NodePort to the Service, which proxies requests to the Service resource. But in a production environment, you still need a type of load balancer, whether client requests are internal or external coming in over the public network. The LoadBalancer service in Kubernetes configures an Open Systems Interconnection (OSI) model Layer 4 (L4) load balancer, which forwards and balances traffic from the internet to your backend application. To use a load balancer for distributing client traffic to the nodes in a cluster, you need a public IP address that the clients can connect to, and you need IP addresses on the nodes themselves to which the load balancer can forward the requests. When you create a standard cluster, IKS and ROKS automatically provision a portable public subnet and a portable private subnet. The portable public subnet provides 5 usable IP addresses. 1 portable public IP address is used by the default public Ingress ALB . The remaining 4 portable public IP addresses can be used to expose single apps using layer 4 (L4) TCP/UDP Network Load Balancer (NLB). The portable public and private IP addresses are static floating IPs pointing to worker nodes. A Keepalived daemon constantly monitors the IP, and automatically moves the IP to another worker node if the worker node is removed. See: Classic: About network load balancers (NLBs) . Services of type LoadBalancer have some limitations. They cannot do TLS termination, do virtual hosts or path-based routing, so you can\u2019t use a single load balancer to proxy to multiple services. These limitations led to the addition in Kubernetes v1.2 of a separate kubernetes resource called Ingress and Route (on OpenShift). Because, the creation of the load balancer happens asynchronously with the creation of the Service, you might have to wait until both the Service and the load balancer have been created.","title":"LoadBalancer"},{"location":"services/loadbalancer/#load-balancer-on-ibm-cloud","text":"The LoadBalancer service type is implemented differently depending on your cluster's infrastructure provider. On IBM Kubernetes Service (IKS) and Red Hat OpenShift Kubernetes Service (ROKS) on IBM Cloud, classic clusters implement by default a Network Load Balancer (NLB) 1.0. Version 1.0 NLBs use Network Address Translation (NAT) to rewrite the request packet's source IP address to the IP of worker node where a load balancer pod exists. Version 2.0 NLBs doesn't use NAT but IP over IP (IPIP) to encapsulate the original request packet into another packet, which preserves the client IP as its source IP address. The worker node then uses Direct Server Return (DSR) to send the app response packet back to the client IP.","title":"Load Balancer on IBM Cloud"},{"location":"services/loadbalancer/#load-balancing-methods-on-ibm-cloud","text":"Before we create the LoadBalancer service for the helloworld application, review some of the Load Balancing options on Kubernetes in IBM Cloud: NodePort exposes the app via a port and public IP address on a worker node using kube-proxy . LoadBalancer NLB v1.0 uses basic load balancing that exposes the app with an IP address or a subdomain. LoadBalancer NLB v2.0 , uses Direct Server Return (DSR) load balancing to expose the app with an IP address or a subdomain, and supports SSL/TLS termination. Istio Ingress Gateway + NLB uses basic load balancing that exposes the app with a subdomain and uses Istio routing rules. Ingress with public ALB uses HTTPS load balancing that exposes the app with a subdomain and uses custom routing rules and SSL/TLS termination for multiple apps. You can customize the ALB routing rules with annotations . Custom Ingress + NLB uses HTTPS load balancing with a custom Ingress that exposes the app with the IBM-provided ALB subdomain and uses custom routing rules.","title":"Load Balancing Methods on IBM Cloud"},{"location":"services/loadbalancer/#create-a-loadbalancer","text":"In the previous lab, you already created a NodePort Service. $ oc get svc -n $MY_NS NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE helloworld NodePort 172 .21.86.16 <none> 8080 :32387/TCP 12m Patch the service for helloworld and change the type to LoadBalancer . oc patch svc helloworld -p '{\"spec\": {\"type\": \"LoadBalancer\"}}' -n $MY_NS The TYPE should be set to LoadBalancer now, and an EXTERNAL-IP should be assigned. $ oc get svc helloworld -n $MY_NS NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE helloworld LoadBalancer 172 .21.86.16 169 .47.155.242 8080 :32387/TCP 12m To access the Service of the helloworld from the public internet, you can use the public IP address of the NLB and the assigned NodePort of the service in the format <IP_address>:<NodePort> . PUBLIC_IP = $( oc get svc helloworld -n $MY_NS --output json | jq -r '.status.loadBalancer.ingress[0].ip' ) echo $PUBLIC_IP NODE_PORT = $( oc get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $NODE_PORT Access the helloworld app in a browser or with Curl, $ curl -L -X POST \"http:// $PUBLIC_IP : $NODE_PORT /api/messages\" -H 'Content-Type: application/json' -d '{ \"sender\": \"world2\" }' { \"id\" : \"0ebdc166-32cd-4d0d-93b6-f278e4405c6f\" , \"sender\" : \"world2\" , \"message\" : \"Hello world2 (direct)\" , \"host\" :null }","title":"Create a LoadBalancer"},{"location":"services/loadbalancer/#next","text":"Next, go to ExternalName .","title":"Next"},{"location":"services/nodeport/","text":"NodePort \u00b6 Pre-requisites \u00b6 Finish the Services and ClusterIP labs. About NodePort \u00b6 To expose a Service to an external IP address, you have to create a ServiceType other than ClusterIP. When you send a request to the name of the service, kube-proxy looks up the name in the cluster DNS server and routes the request to the in-cluster IP address of the service. To allow external traffic into a kubernetes cluster, you need a NodePort ServiceType. When kubernetes creates a NodePort service, kube-proxy allocates a port in the range 30000-32767 and opens this port on the eth0 interface of every node (the NodePort ). Connections to this port are then forwarded to the service\u2019s cluster IP. A gateway router typically sits in front of the cluster and forwards packets to the node. Patch the existing Service for helloworld to type: NodePort using the kubectl patch command, $ oc patch svc helloworld -p '{\"spec\": {\"type\": \"NodePort\"}}' -n $MY_NS service/helloworld patched You can also add a property type: NodePort in the specification in the file helloworld-service-nodeport.yaml , apiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld spec: ports: - port: 8080 targetPort: http-server selector: app: helloworld type: NodePort To apply changes to the configuration from file, $ oc apply -f helloworld-service-nodeport.yaml -n $MY_NS Warning: oc apply should be used on resource created by either oc create --save-config or oc apply service/helloworld configured Describe the Service, $ oc describe svc helloworld -n $MY_NS Name: helloworld Namespace: my-apps Labels: app=helloworld Annotations: Selector: app=helloworld Type: NodePort IP: 172.21.86.16 Port: <unset> 8080/TCP TargetPort: http-server/TCP NodePort: <unset> 32387/TCP Endpoints: 172.30.172.228:8080,172.30.234.176:8080,172.30.234.177:8080 Session Affinity: None External Traffic Policy: Cluster Events: <none> Kubernetes added a NodePort, in the example with port value 32387 . You can now connect to the service from outside the cluster via the public IP address of any worker node in the cluster and traffic will be forwarded to the service. Service discovery with the selector and labels is used to deliver the request to one of the pod's IP addresses. With this piece in place we now have a complete pipeline for load balancing external client requests to all the nodes in the cluster. To connect to the service, we need the Public IP address of one of the worker nodes and the NodePort of the Service. You can use a bash processor called jq to parse JSON from command line. PUBLIC_IP=$(oc get nodes -o wide -o json | jq -r '.items[0].status.addresses | .[] | select( .type==\"ExternalIP\" ) | .address ') echo $PUBLIC_IP NODE_PORT=$(oc get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $NODE_PORT Test the deployment, $ curl -L -X POST \"http:// $PUBLIC_IP : $NODE_PORT /api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: text/plain' -d '{ \"sender\": \"world1\" }' {\"id\":\"f142f74f-c679-4738-96e3-6518e607efa2\",\"sender\":\"world1\",\"message\":\"Hello world1 (direct)\",\"host\":null} The client connects to your application via a public IP address of a worker node and the NodePort. Each node proxies the port, kube-proxy receives the request, and forwards it to the service at the cluster IP. At this point the request matches the netfilter or iptables rules and gets redirected to the server pod. However, we still require some level of load balancing. a LoadBalancer service is the standard way to expose a service. Next \u00b6 Go to LoadBalancer to learn more about ServiceType LoadBalancer.","title":"NodePort"},{"location":"services/nodeport/#nodeport","text":"","title":"NodePort"},{"location":"services/nodeport/#pre-requisites","text":"Finish the Services and ClusterIP labs.","title":"Pre-requisites"},{"location":"services/nodeport/#about-nodeport","text":"To expose a Service to an external IP address, you have to create a ServiceType other than ClusterIP. When you send a request to the name of the service, kube-proxy looks up the name in the cluster DNS server and routes the request to the in-cluster IP address of the service. To allow external traffic into a kubernetes cluster, you need a NodePort ServiceType. When kubernetes creates a NodePort service, kube-proxy allocates a port in the range 30000-32767 and opens this port on the eth0 interface of every node (the NodePort ). Connections to this port are then forwarded to the service\u2019s cluster IP. A gateway router typically sits in front of the cluster and forwards packets to the node. Patch the existing Service for helloworld to type: NodePort using the kubectl patch command, $ oc patch svc helloworld -p '{\"spec\": {\"type\": \"NodePort\"}}' -n $MY_NS service/helloworld patched You can also add a property type: NodePort in the specification in the file helloworld-service-nodeport.yaml , apiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld spec: ports: - port: 8080 targetPort: http-server selector: app: helloworld type: NodePort To apply changes to the configuration from file, $ oc apply -f helloworld-service-nodeport.yaml -n $MY_NS Warning: oc apply should be used on resource created by either oc create --save-config or oc apply service/helloworld configured Describe the Service, $ oc describe svc helloworld -n $MY_NS Name: helloworld Namespace: my-apps Labels: app=helloworld Annotations: Selector: app=helloworld Type: NodePort IP: 172.21.86.16 Port: <unset> 8080/TCP TargetPort: http-server/TCP NodePort: <unset> 32387/TCP Endpoints: 172.30.172.228:8080,172.30.234.176:8080,172.30.234.177:8080 Session Affinity: None External Traffic Policy: Cluster Events: <none> Kubernetes added a NodePort, in the example with port value 32387 . You can now connect to the service from outside the cluster via the public IP address of any worker node in the cluster and traffic will be forwarded to the service. Service discovery with the selector and labels is used to deliver the request to one of the pod's IP addresses. With this piece in place we now have a complete pipeline for load balancing external client requests to all the nodes in the cluster. To connect to the service, we need the Public IP address of one of the worker nodes and the NodePort of the Service. You can use a bash processor called jq to parse JSON from command line. PUBLIC_IP=$(oc get nodes -o wide -o json | jq -r '.items[0].status.addresses | .[] | select( .type==\"ExternalIP\" ) | .address ') echo $PUBLIC_IP NODE_PORT=$(oc get svc helloworld -n $MY_NS --output json | jq -r '.spec.ports[0].nodePort' ) echo $NODE_PORT Test the deployment, $ curl -L -X POST \"http:// $PUBLIC_IP : $NODE_PORT /api/messages\" -H 'Content-Type: application/json' -H 'Content-Type: text/plain' -d '{ \"sender\": \"world1\" }' {\"id\":\"f142f74f-c679-4738-96e3-6518e607efa2\",\"sender\":\"world1\",\"message\":\"Hello world1 (direct)\",\"host\":null} The client connects to your application via a public IP address of a worker node and the NodePort. Each node proxies the port, kube-proxy receives the request, and forwards it to the service at the cluster IP. At this point the request matches the netfilter or iptables rules and gets redirected to the server pod. However, we still require some level of load balancing. a LoadBalancer service is the standard way to expose a service.","title":"About NodePort"},{"location":"services/nodeport/#next","text":"Go to LoadBalancer to learn more about ServiceType LoadBalancer.","title":"Next"},{"location":"services/services/","text":"Services \u00b6 This tutorial will demonstrate different ways to control traffic on a Kubernetes cluster using Service types, Ingress, Route, Network Policy and Calico. You will learn basic networking and security concepts on Kubernetes. Setup \u00b6 For setup and pre-requisities, go here . Make sure you are connected to the Kubernetes cluster. $ oc config current-context my-apps/c115-e-us-south-containers-cloud-ibm-com:32370/IAM#remkohdev@us.ibm.com If you are not connected to your cluster, finish the setup . Deploy the Helloworld App \u00b6 To deploy the application to a Kubernetes cluster in an isolated namespace, we create a separate namespace for the application. Throughout the tutorial I will use environment variables in the commands. First, create an environment variable for a namespace to deploy our application, MY_NS = my-apps echo $MY_NS Create the namespace, $ oc new-project $MY_NS Now using project \"my-apps\" on server \"https://c109-e.us-east.containers.cloud.ibm.com:31345\" . You can add applications to this project with the 'new-app' command. For example, try: oc new-app ruby~https://github.com/sclorg/ruby-ex.git to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image = gcr.io/hello-minikube-zero-install/hello-node Or if you already created the namespace, switch to the namespace where you want to deploy the application to, $ oc project $MY_NS Now using project \"my-apps\" on server \"https://c109-e.us-east.containers.cloud.ibm.com:31345\" . In this workshop, we use a helloworld application. The source code and Kubernetes resource specifications are included in the repository. Make sure, you cloned the helloworld repository to your client, git clone https://github.com/remkohdev/helloworld.git cd helloworld ls -al Now you can deploy the helloworld application in an isolated namespace, this is the first step to create some level of isolation and limit a possible attack surface, $ oc create -f helloworld-deployment.yaml -n $MY_NS deployment.apps/helloworld created On OpenShift you do not have to specify the namespace to deploy to the current namespace, but for clarity I added the flag -n $MY_NS . If you don't have the source code for the Helloworld app with the Kubernetes specification files, go to the setup to Get Helloworld Source Code . The helloworld application is now deployed and a Kubernetes Deployment object was created. $ oc get all -n $MY_NS NAME READY STATUS RESTARTS AGE pod/helloworld-5f8b6b587b-qqqjs 1 /1 Running 0 73s pod/helloworld-5f8b6b587b-rwkf9 1 /1 Running 0 73s pod/helloworld-5f8b6b587b-tbpts 1 /1 Running 0 73s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/helloworld 3 /3 3 3 73s NAME DESIRED CURRENT READY AGE replicaset.apps/helloworld-5f8b6b587b 3 3 3 73s The deployment consists of a Deployment object, a ReplicaSet with 3 replicas of Pods . Because we did not create a Service for the helloworld containers running in pods, they cannot yet be readily accessed. When a Pod is deployed to a worker node, it is assigned a private IP address in the 172.30.0.0/16 range. Worker nodes and pods can securely communicate on the private network by using private IP addresses. However, Kubernetes creates and destroys Pods dynamically, which means that the location of the Pods changes dynamically. When a Pod is destroyed or a worker node needs to be re-created, a new private IP address is assigned. With a Service object, you can use built-in Kubernetes service discovery to expose Pods. A Service defines a set of Pods and a policy to access those Pods. Kubernetes assigns a single DNS name for a set of Pods and can load balance across Pods. When you create a Service, a set of pods and EndPoints are created to manage access to the pods. The Endpoints object in Kubernetes contains a list of IP and port addresses and are created automatically when a Service is created and configured with the pods matching the selector of the Service. ServiceTypes \u00b6 Before we create the Service for the helloworld application, briefly review the types of services. The default type is ClusterIP . To expose a Service onto an external IP address, you have to create a ServiceType other than ClusterIP. Available Service types: ClusterIP : Exposes the Service on a cluster-internal IP. This is the default ServiceType. NodePort : Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll be able to contact the NodePort Service, from outside the cluster, by requesting NodeIP : NodePort . LoadBalancer : Exposes the Service externally using a cloud provider\u2019s load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. ExternalName : Maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record. You can also use Ingress in place of Service to expose HTTP/HTTPS Services. Ingress however is technically not a ServiceType, but it acts as the entry point for your cluster and lets you consolidate routing rules into a single resource. A Route is similar to Ingress and was created on OpenShift before the introduction of Ingress in Kubernetes. Route has additional capabilities such as splitting traffic between multiple backends and sticky sessions. Route design principles heavily influenced the Ingress design. Next \u00b6 Next, go to ClusterIP to learn more about ServiceType ClusterIP.","title":"Services"},{"location":"services/services/#services","text":"This tutorial will demonstrate different ways to control traffic on a Kubernetes cluster using Service types, Ingress, Route, Network Policy and Calico. You will learn basic networking and security concepts on Kubernetes.","title":"Services"},{"location":"services/services/#setup","text":"For setup and pre-requisities, go here . Make sure you are connected to the Kubernetes cluster. $ oc config current-context my-apps/c115-e-us-south-containers-cloud-ibm-com:32370/IAM#remkohdev@us.ibm.com If you are not connected to your cluster, finish the setup .","title":"Setup"},{"location":"services/services/#deploy-the-helloworld-app","text":"To deploy the application to a Kubernetes cluster in an isolated namespace, we create a separate namespace for the application. Throughout the tutorial I will use environment variables in the commands. First, create an environment variable for a namespace to deploy our application, MY_NS = my-apps echo $MY_NS Create the namespace, $ oc new-project $MY_NS Now using project \"my-apps\" on server \"https://c109-e.us-east.containers.cloud.ibm.com:31345\" . You can add applications to this project with the 'new-app' command. For example, try: oc new-app ruby~https://github.com/sclorg/ruby-ex.git to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image = gcr.io/hello-minikube-zero-install/hello-node Or if you already created the namespace, switch to the namespace where you want to deploy the application to, $ oc project $MY_NS Now using project \"my-apps\" on server \"https://c109-e.us-east.containers.cloud.ibm.com:31345\" . In this workshop, we use a helloworld application. The source code and Kubernetes resource specifications are included in the repository. Make sure, you cloned the helloworld repository to your client, git clone https://github.com/remkohdev/helloworld.git cd helloworld ls -al Now you can deploy the helloworld application in an isolated namespace, this is the first step to create some level of isolation and limit a possible attack surface, $ oc create -f helloworld-deployment.yaml -n $MY_NS deployment.apps/helloworld created On OpenShift you do not have to specify the namespace to deploy to the current namespace, but for clarity I added the flag -n $MY_NS . If you don't have the source code for the Helloworld app with the Kubernetes specification files, go to the setup to Get Helloworld Source Code . The helloworld application is now deployed and a Kubernetes Deployment object was created. $ oc get all -n $MY_NS NAME READY STATUS RESTARTS AGE pod/helloworld-5f8b6b587b-qqqjs 1 /1 Running 0 73s pod/helloworld-5f8b6b587b-rwkf9 1 /1 Running 0 73s pod/helloworld-5f8b6b587b-tbpts 1 /1 Running 0 73s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/helloworld 3 /3 3 3 73s NAME DESIRED CURRENT READY AGE replicaset.apps/helloworld-5f8b6b587b 3 3 3 73s The deployment consists of a Deployment object, a ReplicaSet with 3 replicas of Pods . Because we did not create a Service for the helloworld containers running in pods, they cannot yet be readily accessed. When a Pod is deployed to a worker node, it is assigned a private IP address in the 172.30.0.0/16 range. Worker nodes and pods can securely communicate on the private network by using private IP addresses. However, Kubernetes creates and destroys Pods dynamically, which means that the location of the Pods changes dynamically. When a Pod is destroyed or a worker node needs to be re-created, a new private IP address is assigned. With a Service object, you can use built-in Kubernetes service discovery to expose Pods. A Service defines a set of Pods and a policy to access those Pods. Kubernetes assigns a single DNS name for a set of Pods and can load balance across Pods. When you create a Service, a set of pods and EndPoints are created to manage access to the pods. The Endpoints object in Kubernetes contains a list of IP and port addresses and are created automatically when a Service is created and configured with the pods matching the selector of the Service.","title":"Deploy the Helloworld App"},{"location":"services/services/#servicetypes","text":"Before we create the Service for the helloworld application, briefly review the types of services. The default type is ClusterIP . To expose a Service onto an external IP address, you have to create a ServiceType other than ClusterIP. Available Service types: ClusterIP : Exposes the Service on a cluster-internal IP. This is the default ServiceType. NodePort : Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll be able to contact the NodePort Service, from outside the cluster, by requesting NodeIP : NodePort . LoadBalancer : Exposes the Service externally using a cloud provider\u2019s load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. ExternalName : Maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record. You can also use Ingress in place of Service to expose HTTP/HTTPS Services. Ingress however is technically not a ServiceType, but it acts as the entry point for your cluster and lets you consolidate routing rules into a single resource. A Route is similar to Ingress and was created on OpenShift before the introduction of Ingress in Kubernetes. Route has additional capabilities such as splitting traffic between multiple backends and sticky sessions. Route design principles heavily influenced the Ingress design.","title":"ServiceTypes"},{"location":"services/services/#next","text":"Next, go to ClusterIP to learn more about ServiceType ClusterIP.","title":"Next"},{"location":"setup/setup1/","text":"Setup \u00b6 Pre-requisities \u00b6 Free IBM Cloud account, to create a new IBM Cloud account go here . Free Pay-As-You-Go account. To upgrade a free IBM Cloud account, go here . Client terminal with: IBM Cloud CLI, IBM CLoud CLI Kubernetes Service plugin, IBM CLoud CLI Infrastructure Service plugin, kubectl CLI, OpenShift commandline oc CLI, Recommended client terminals: CognitiveLabs.ai, to access a client terminal at CognitiveLabs.ai, go here , IBM OpenLabs IBM Cloud Shell Kubernetes cluster: IBM Cloud Kubernetes Service (IKS) v1.18 with a standard plan, a classic provider, OR an Red Hat OpenShift Kubernetes Service v4.5 with a classic provider, at least 2 worker nodes (with a subnet and public IPs, external LoadBalancer (for details about VLAN, subnets and IPs, see here ), Setup Kubernetes Cluster \u00b6 To use an IBM provided Kubernetes cluster with your IBM Cloud account and IBMId, grant access permissions to the cluster, as instructed here . To connect to a managed Red Hat OpenShift Kubernetes Service (ROKS) , go here . To access a client terminal and OpenShift cluster at OpenLabs follow the instructions here . Setup Client Terminal \u00b6 This workshop was tested using: the labs environment at CognitiveLabs. To access a client terminal at CognitiveLabs.ai, follow the instructions here . the labs environment at OpenLabs. To access a client terminal and OpenShift cluster at OpenLabs follow the instructions here . Login to IBM Cloud \u00b6 Log in to your cluster, e.g. if created in the us-south region, IBMID=<your IBMID email> ibmcloud login -u $IBMID If you are using federated SSO login, use the -sso flag instead. Select the account in which the cluster was created. Connect to IKS Cluster \u00b6 Set the KS_CLUSTER_NAME environment variable to the correct cluster name, KS_CLUSTER_NAME=<your cluster name> Optionally, if you don't know your cluster name, list all clusters, ibmcloud ks clusters Or search for all clusters that contain a known substring, KS_NAME_SUB=<substring> ibmcloud ks clusters --output json | jq -r 'map(select(.name | contains('\\\"$KS_NAME_SUB\\\"')))' Create an environment variable $MY_NS for the desired target namespace, MY_NS=my-apps For IKS, download the cluster configuration to the client, ibmcloud ks cluster config --cluster $KS_CLUSTER_NAME oc config current-context The config should be set to a clustername/clusterid pair, Connect to OpenShift Cluster \u00b6 For OpenShift, find the oc login command with the login token. To connect to a managed Red Hat OpenShift Kubernetes Service (ROKS) , follow the instructions here . For example, $ oc login --token = e6PfXsQKjxiUf7qkb4jJxEd851pa0ZuUSEimVuGt4aQ --server = https://c114-e.us-south.containers.cloud.ibm.com:32115 Logged into \"https://c109-e.us-east.containers.cloud.ibm.com:31345\" as \"IAM#b.newell2@remkoh.dev\" using the token provided. You have access to x projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". Get Helloworld Source Code \u00b6 In this workshop, we will use a helloworld application. The source code, Maven build file, container images, and Kubernetes resource specifications are included in the repository. To get started, clone the helloworld repository to your client, git clone https://github.com/remkohdev/helloworld.git cd helloworld ls -al Next \u00b6 Next, go to Services .","title":"Setup"},{"location":"setup/setup1/#setup","text":"","title":"Setup"},{"location":"setup/setup1/#pre-requisities","text":"Free IBM Cloud account, to create a new IBM Cloud account go here . Free Pay-As-You-Go account. To upgrade a free IBM Cloud account, go here . Client terminal with: IBM Cloud CLI, IBM CLoud CLI Kubernetes Service plugin, IBM CLoud CLI Infrastructure Service plugin, kubectl CLI, OpenShift commandline oc CLI, Recommended client terminals: CognitiveLabs.ai, to access a client terminal at CognitiveLabs.ai, go here , IBM OpenLabs IBM Cloud Shell Kubernetes cluster: IBM Cloud Kubernetes Service (IKS) v1.18 with a standard plan, a classic provider, OR an Red Hat OpenShift Kubernetes Service v4.5 with a classic provider, at least 2 worker nodes (with a subnet and public IPs, external LoadBalancer (for details about VLAN, subnets and IPs, see here ),","title":"Pre-requisities"},{"location":"setup/setup1/#setup-kubernetes-cluster","text":"To use an IBM provided Kubernetes cluster with your IBM Cloud account and IBMId, grant access permissions to the cluster, as instructed here . To connect to a managed Red Hat OpenShift Kubernetes Service (ROKS) , go here . To access a client terminal and OpenShift cluster at OpenLabs follow the instructions here .","title":"Setup Kubernetes Cluster"},{"location":"setup/setup1/#setup-client-terminal","text":"This workshop was tested using: the labs environment at CognitiveLabs. To access a client terminal at CognitiveLabs.ai, follow the instructions here . the labs environment at OpenLabs. To access a client terminal and OpenShift cluster at OpenLabs follow the instructions here .","title":"Setup Client Terminal"},{"location":"setup/setup1/#login-to-ibm-cloud","text":"Log in to your cluster, e.g. if created in the us-south region, IBMID=<your IBMID email> ibmcloud login -u $IBMID If you are using federated SSO login, use the -sso flag instead. Select the account in which the cluster was created.","title":"Login to IBM Cloud"},{"location":"setup/setup1/#connect-to-iks-cluster","text":"Set the KS_CLUSTER_NAME environment variable to the correct cluster name, KS_CLUSTER_NAME=<your cluster name> Optionally, if you don't know your cluster name, list all clusters, ibmcloud ks clusters Or search for all clusters that contain a known substring, KS_NAME_SUB=<substring> ibmcloud ks clusters --output json | jq -r 'map(select(.name | contains('\\\"$KS_NAME_SUB\\\"')))' Create an environment variable $MY_NS for the desired target namespace, MY_NS=my-apps For IKS, download the cluster configuration to the client, ibmcloud ks cluster config --cluster $KS_CLUSTER_NAME oc config current-context The config should be set to a clustername/clusterid pair,","title":"Connect to IKS Cluster"},{"location":"setup/setup1/#connect-to-openshift-cluster","text":"For OpenShift, find the oc login command with the login token. To connect to a managed Red Hat OpenShift Kubernetes Service (ROKS) , follow the instructions here . For example, $ oc login --token = e6PfXsQKjxiUf7qkb4jJxEd851pa0ZuUSEimVuGt4aQ --server = https://c114-e.us-south.containers.cloud.ibm.com:32115 Logged into \"https://c109-e.us-east.containers.cloud.ibm.com:31345\" as \"IAM#b.newell2@remkoh.dev\" using the token provided. You have access to x projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\".","title":"Connect to OpenShift Cluster"},{"location":"setup/setup1/#get-helloworld-source-code","text":"In this workshop, we will use a helloworld application. The source code, Maven build file, container images, and Kubernetes resource specifications are included in the repository. To get started, clone the helloworld repository to your client, git clone https://github.com/remkohdev/helloworld.git cd helloworld ls -al","title":"Get Helloworld Source Code"},{"location":"setup/setup1/#next","text":"Next, go to Services .","title":"Next"},{"location":"setup/setup2/","text":"Setup \u00b6 Pre-requisities \u00b6 Free IBM Cloud account, to create a new IBM Cloud account go here . Free Pay-As-You-Go account. To upgrade a free IBM Cloud account, go here . CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, go here . IBM Cloud Kubernetes Service (IKS) v1.18 with a standard plan, a classic provider and at least 2 worker nodes (with a subnet and public IPs, external LoadBalancer (for details about VLAN, subnets and IPs, see here ); OR an Red Hat OpenShift Kubernetes Service v4.5 with a classic provider and at least 2 worker nodes. Setup Kubernetes Cluster \u00b6 To use an IBM provided Kubernetes cluster with your IBM Cloud account and IBMId, grant access permissions to the cluster, as instructed here . To connect to a managed Red Hat OpenShift Kubernetes Service (ROKS) , go here . Setup Client Terminal \u00b6 This workshop was tested using the Labs environment at CognitiveLabs. To access a client terminal at CognitiveLabs.ai, follow the instructions here . Login to IBM Cloud \u00b6 Log in to your cluster, e.g. if created in the us-south region, IBMID=<your IBMID email> ibmcloud login -u $IBMID If you are using federated SSO login, use the -sso flag instead. Select the account in which the cluster was created. Connect to Cluster \u00b6 Optionally, if you don't know your cluster name, list all clusters, KS_NAME_SUB=<substring> ibmcloud ks clusters --output json | jq -r 'map(select(.name | contains('\\\"$KS_NAME_SUB\\\"')))' Set the KS_CLUSTER_NAME environment variable to the correct cluster name, and the namespace environment variable $MY_NS to the desired namespace, KS_CLUSTER_NAME=<your cluster name> MY_NS=my-apps For IKS, download the cluster configuration to the client, ibmcloud ks cluster config --cluster $KS_CLUSTER_NAME kubectl config current-context The config should be set to a clustername/clusterid pair, For OpenShift, find the oc login command with the login token. To connect to a managed Red Hat OpenShift Kubernetes Service (ROKS) , go here . For example, oc login --token=e6PfXsQKjxiUf7qkb4jJxEd851pa0ZuUSEimVuGt4aQ --server=https://c114-e.us-south.containers.cloud.ibm.com:32115 Install Calicoctl \u00b6 Calicoctl is not needed for the current lab. Install calicoctl, curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl chmod +x calicoctl echo \"export PATH=$(pwd):$PATH\" > $HOME/.bash_profile source $HOME/.bash_profile ibmcloud ks cluster config --cluster $KS_CLUSTER_NAME --admin --network Test the version, $ calicoctl version Client Version: v3.17.1 Git commit: 8871aca3 Cluster Version: v3.16.5 Cluster Type: typha,kdd,k8s,operator,openshift,bgp As kubectl plugin, curl -o kubectl-calico -L https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl chmod +x kubectl-calico kubectl calico -h","title":"Setup"},{"location":"setup/setup2/#setup","text":"","title":"Setup"},{"location":"setup/setup2/#pre-requisities","text":"Free IBM Cloud account, to create a new IBM Cloud account go here . Free Pay-As-You-Go account. To upgrade a free IBM Cloud account, go here . CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, go here . IBM Cloud Kubernetes Service (IKS) v1.18 with a standard plan, a classic provider and at least 2 worker nodes (with a subnet and public IPs, external LoadBalancer (for details about VLAN, subnets and IPs, see here ); OR an Red Hat OpenShift Kubernetes Service v4.5 with a classic provider and at least 2 worker nodes.","title":"Pre-requisities"},{"location":"setup/setup2/#setup-kubernetes-cluster","text":"To use an IBM provided Kubernetes cluster with your IBM Cloud account and IBMId, grant access permissions to the cluster, as instructed here . To connect to a managed Red Hat OpenShift Kubernetes Service (ROKS) , go here .","title":"Setup Kubernetes Cluster"},{"location":"setup/setup2/#setup-client-terminal","text":"This workshop was tested using the Labs environment at CognitiveLabs. To access a client terminal at CognitiveLabs.ai, follow the instructions here .","title":"Setup Client Terminal"},{"location":"setup/setup2/#login-to-ibm-cloud","text":"Log in to your cluster, e.g. if created in the us-south region, IBMID=<your IBMID email> ibmcloud login -u $IBMID If you are using federated SSO login, use the -sso flag instead. Select the account in which the cluster was created.","title":"Login to IBM Cloud"},{"location":"setup/setup2/#connect-to-cluster","text":"Optionally, if you don't know your cluster name, list all clusters, KS_NAME_SUB=<substring> ibmcloud ks clusters --output json | jq -r 'map(select(.name | contains('\\\"$KS_NAME_SUB\\\"')))' Set the KS_CLUSTER_NAME environment variable to the correct cluster name, and the namespace environment variable $MY_NS to the desired namespace, KS_CLUSTER_NAME=<your cluster name> MY_NS=my-apps For IKS, download the cluster configuration to the client, ibmcloud ks cluster config --cluster $KS_CLUSTER_NAME kubectl config current-context The config should be set to a clustername/clusterid pair, For OpenShift, find the oc login command with the login token. To connect to a managed Red Hat OpenShift Kubernetes Service (ROKS) , go here . For example, oc login --token=e6PfXsQKjxiUf7qkb4jJxEd851pa0ZuUSEimVuGt4aQ --server=https://c114-e.us-south.containers.cloud.ibm.com:32115","title":"Connect to Cluster"},{"location":"setup/setup2/#install-calicoctl","text":"Calicoctl is not needed for the current lab. Install calicoctl, curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl chmod +x calicoctl echo \"export PATH=$(pwd):$PATH\" > $HOME/.bash_profile source $HOME/.bash_profile ibmcloud ks cluster config --cluster $KS_CLUSTER_NAME --admin --network Test the version, $ calicoctl version Client Version: v3.17.1 Git commit: 8871aca3 Cluster Version: v3.16.5 Cluster Type: typha,kdd,k8s,operator,openshift,bgp As kubectl plugin, curl -o kubectl-calico -L https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl chmod +x kubectl-calico kubectl calico -h","title":"Install Calicoctl"},{"location":"vpc/airgap/","text":"What is Airgap \u00b6 An air gap , air wall , air gapping or disconnected network is a network security measure to ensure that a secure computer network is physically isolated from unsecured networks, such as the public Internet or an unsecured local area network. It means a computer or network has no network interfaces connected to other networks. For most enterprise clients in regulated industries and public sector agencies, the term air gap has a more specific meaning. Storage \u00b6 \"To move data between the outside world and the air-gapped system, it is necessary to write data to a physical medium such as a thumbdrive, and physically move it between computers.\" source Container Registry \u00b6 In an air-gapped Kubernetes cluster, you would not be able to deploy a Docker Hub image like ibmcom/guestbook:v1 . Instead, an air-gapped Kubernetes cluster would rely on a private registry. In theory, a container runtime can be configured with an image mirror to a private registry (thereby telling the container runtime to retrieve docker.io/ibmcom/guestbook:v1 from somewhere else, but it doesn't seem like this article is explaining that. Public Service Endpoint \u00b6 In a properly air-gapped Kubernetes cluster, you are not able to reach the control plane endpoint from outside the airgapped network, e.g. over the Internet. When you create an IBM Cloud Kubernetes Service with VPC instance, as in: ibmcloud ks cluster create vpc-gen2 --name $MY_CLUSTER_NAME --zone $MY_ZONE --version $KS_VERSION --flavor bx2.2x8 --workers 1 --vpc-id $MY_VPC_ID --subnet-id $MY_VPC_SUBNET_ID the created cluster will have public service endpoints. To disable the creation of these, use the --disable-public-service-endpoint flag. Best Practices \u00b6 Disable the public gateway for the VPC subnet with the worker nodes. This means that you cannot access the cluster nodes from outside the secure network, which includes for instance a build server and a container registry. Disable public endpoints for the control plane. When using private only endpoints, orgs can use systems within the VPC to manage the control plane, or set up Direct Link / VPN connections to their VPC. If this is not possible, proxying connections through a haproxy instance in the VPC is an alternative. How can a container engine on a worker in such a cluster pull container images to create containers? One option is to use a private registry inside the network. IBM Container Registry, the built-in OpenShift private registry, or a custom registry deployment inside the network are great options. Organizations can have their Continuous Integration (CI) systems tag and push images to IBM CR and then k8s/openshift clusters that don't have Internet access can pull images from IBM CR. Traffic to RESTful API endpoints on your cluster can be proxied through a secure gateway like API Connect with DataPower gateway.","title":"Airgap"},{"location":"vpc/airgap/#what-is-airgap","text":"An air gap , air wall , air gapping or disconnected network is a network security measure to ensure that a secure computer network is physically isolated from unsecured networks, such as the public Internet or an unsecured local area network. It means a computer or network has no network interfaces connected to other networks. For most enterprise clients in regulated industries and public sector agencies, the term air gap has a more specific meaning.","title":"What is Airgap"},{"location":"vpc/airgap/#storage","text":"\"To move data between the outside world and the air-gapped system, it is necessary to write data to a physical medium such as a thumbdrive, and physically move it between computers.\" source","title":"Storage"},{"location":"vpc/airgap/#container-registry","text":"In an air-gapped Kubernetes cluster, you would not be able to deploy a Docker Hub image like ibmcom/guestbook:v1 . Instead, an air-gapped Kubernetes cluster would rely on a private registry. In theory, a container runtime can be configured with an image mirror to a private registry (thereby telling the container runtime to retrieve docker.io/ibmcom/guestbook:v1 from somewhere else, but it doesn't seem like this article is explaining that.","title":"Container Registry"},{"location":"vpc/airgap/#public-service-endpoint","text":"In a properly air-gapped Kubernetes cluster, you are not able to reach the control plane endpoint from outside the airgapped network, e.g. over the Internet. When you create an IBM Cloud Kubernetes Service with VPC instance, as in: ibmcloud ks cluster create vpc-gen2 --name $MY_CLUSTER_NAME --zone $MY_ZONE --version $KS_VERSION --flavor bx2.2x8 --workers 1 --vpc-id $MY_VPC_ID --subnet-id $MY_VPC_SUBNET_ID the created cluster will have public service endpoints. To disable the creation of these, use the --disable-public-service-endpoint flag.","title":"Public Service Endpoint"},{"location":"vpc/airgap/#best-practices","text":"Disable the public gateway for the VPC subnet with the worker nodes. This means that you cannot access the cluster nodes from outside the secure network, which includes for instance a build server and a container registry. Disable public endpoints for the control plane. When using private only endpoints, orgs can use systems within the VPC to manage the control plane, or set up Direct Link / VPN connections to their VPC. If this is not possible, proxying connections through a haproxy instance in the VPC is an alternative. How can a container engine on a worker in such a cluster pull container images to create containers? One option is to use a private registry inside the network. IBM Container Registry, the built-in OpenShift private registry, or a custom registry deployment inside the network are great options. Organizations can have their Continuous Integration (CI) systems tag and push images to IBM CR and then k8s/openshift clusters that don't have Internet access can pull images from IBM CR. Traffic to RESTful API endpoints on your cluster can be proxied through a secure gateway like API Connect with DataPower gateway.","title":"Best Practices"},{"location":"vpc/vpcgen2-loadbalancer/","text":"Understanding the Load Balancer for VPC \u00b6 To access the guestbook application via the Load Balancer for VPC, we are using the port of the Service object instead of the NodePort , which is the port we allowed inbound traffic on and which is the port value we used to access our deployment without a VPC. To see why, let's take a step back and go to the list of Load balancers for VPC . Click the load balancer with the hostname that corresponds to the External IP of the LoadBalacer service we created earlier. You see a few tabs under the long name for Back-end pools and Front-end listeners . You see the Hostname that corresponds to the external IP of the LoadBalancer service for the guestbook application. And you see a Health status panel with a Pool name value of tcp-3000-32219 with a Listener protocol and port of value TCP - 3000 . Click on the Back-end pool tab and expand the tcp-3000-32219 pool. Note the Health port for the back-end pool is the NodePort of the guestbook service, and the distribution method used is round robin . You also see that a virtual server is attached on IP address 10.240.0.4 , the private IP address of the worker node of our cluster, and on port $SVC_NODEPORT , the NodePort value of the guestbook service. Click on the Front-end listener tab and note that Port is set to the port of the guestbook service. The front-end listener is mapped to the back-end pool tcp-3000-32219 or tcp-$SVC_PORT-$SVC_NODEPORT . That tells us that it is the load balancer for VPC that maps external traffic to our guestbook application via the hostname:port of the front end listener to the back-end pool with the private IP of the cluster in the IP range of our VPC 10.240.0.0/24 , and the NodePort of the guestbook service. You can also use the CLI to analyze how the traffic flows. Let's inspect the guestbook Service of type LoadBalancer resource in more detail. kubectl get svc -n $MY_NAMESPACE --output json { \"apiVersion\" : \"v1\" , \"items\" : [ { \"apiVersion\" : \"v1\" , \"kind\" : \"Service\" , \"metadata\" : { \"creationTimestamp\" : \"2020-12-23T03:08:27Z\" , \"finalizers\" : [ \"service.kubernetes.io/load-balancer-cleanup\" ] , \"labels\" : { \"app\" : \"guestbook\" } , \"managedFields\" : [ { \"apiVersion\" : \"v1\" , \"fieldsType\" : \"FieldsV1\" , \"fieldsV1\" : { \"f:metadata\" : { \"f:labels\" : { \".\" : {} , \"f:app\" : {} } } , \"f:spec\" : { \"f:externalTrafficPolicy\" : {} , \"f:ports\" : { \".\" : {} , \"k:{\\\"port\\\":3000,\\\"protocol\\\":\\\"TCP\\\"}\" : { \".\" : {} , \"f:port\" : {} , \"f:protocol\" : {} , \"f:targetPort\" : {} } } , \"f:selector\" : { \".\" : {} , \"f:app\" : {} } , \"f:sessionAffinity\" : {} , \"f:type\" : {} } } , \"manager\" : \"kubectl-expose\" , \"operation\" : \"Update\" , \"time\" : \"2020-12-23T03:08:26Z\" } , { \"apiVersion\" : \"v1\" , \"fieldsType\" : \"FieldsV1\" , \"fieldsV1\" : { \"f:metadata\" : { \"f:finalizers\" : { \".\" : {} , \"v:\\\"service.kubernetes.io/load-balancer-cleanup\\\"\" : {} } } , \"f:status\" : { \"f:loadBalancer\" : { \"f:ingress\" : {} } } } , \"manager\" : \"ibm-cloud-controller-manager\" , \"operation\" : \"Update\" , \"time\" : \"2020-12-23T03:29:13Z\" } ] , \"name\" : \"guestbook\" , \"namespace\" : \"my-guestbook\" , \"resourceVersion\" : \"6502\" , \"selfLink\" : \"/api/v1/namespaces/my-guestbook/services/guestbook\" , \"uid\" : \"22bf711e-08e0-4827-8299-53a008b52481\" } , \"spec\" : { \"clusterIP\" : \"172.21.48.26\" , \"externalTrafficPolicy\" : \"Cluster\" , \"ports\" : [ { \"nodePort\" : 32219 , \"port\" : 3000 , \"protocol\" : \"TCP\" , \"targetPort\" : 3000 } ] , \"selector\" : { \"app\" : \"guestbook\" } , \"sessionAffinity\" : \"None\" , \"type\" : \"LoadBalancer\" } , \"status\" : { \"loadBalancer\" : { \"ingress\" : [ { \"hostname\" : \"7a6a66a7-us-south.lb.appdomain.cloud\" } ] } } } ] , \"kind\" : \"List\" , \"metadata\" : { \"resourceVersion\" : \"\" , \"selfLink\" : \"\" } } Note: Service resource lists a property status.loadBalancer.ingress with hostname . We use the hostname to access the Guestbook Service. The spec.ports.nodePort is the port on the worker nodes through which external traffic comes to the cluster. The spec.ports.port is the incoming port that the service listens for requests that are forwarded by Kubernetes from the NodePort. Requests are finally sent to the spec.ports.targetPort on any pod. Traffic flow: nodePort -> port -> targetPort The externalTrafficPolicy property is set to Cluster, which always routes traffic to all pods running a service with equal distribution, with a Service type of LoadBalancer for distributing traffic. Our deployment however is using an Ingress load balancer with an Application Load Balancer (ALB), that includes an external load balancer, which in the architecture of a VPC network is a VPC load balancer with a VPC hostname. When you create an inbound rule for the VPC load balancer, IBM Cloud defines the front-end listener mapped to the back-end pool, and uses the port and the nodeport respectively to load balance traffic. Ingress Application Load Balancer (ALB) \u00b6 Ingress is a Kubernetes service that balances network traffic workloads in your cluster by forwarding public or private requests to your apps by using a unique public or private route. The Ingress application load balancer (ALB) is a layer 7 (L7) load balancer, which implements the NGINX Ingress controller. A layer 4 (L4) LoadBalancer service exposes the ALB so that the ALB can receive external requests that come into your cluster. An Ingress deployment consists of three components: Ingress resources, an internal L7 Application Load Balancer (ALB), an external L4 load balancer to handle incoming requests across zones. For classic clusters, this component is the Multi-Zone Load Balancer (MZLB) that IBM Cloud Kubernetes Service creates for you. For VPC clusters, this component is the VPC load balancer created in your VPC. To expose an app by using Ingress, you must create a Kubernetes service for your app and register this service with Ingress by defining an Ingress resource. (To learn how, go to Ingress and ALB ). The following diagram shows how Ingress directs communication from the internet to an app in a VPC multizone cluster. source A VPC load balancer listens for external traffic, and based on the resolved IP address, the VPC load balancer sends the request to an available Application Load Balancer (ALB). The Application Load Balancer (ALB) listens for incoming HTTP, HTTPS, or TCP service requests, checks if routing rules exist for the application, and then forwards requests to the appropriate app pod according to the rules defined in the Ingress resource. In the IKS instance, there is a private and a public Ingress ALB installed, the private Ingress ALB is disabled. ibmcloud ks ingress alb ls -c $MY_CLUSTER_NAME OK ALB ID Enabled State Type Load Balancer Hostname Zone Build Status private-crbvhau5gd0do97g1vvo50-alb1 false disabled private - us-south-1 ingress:/ingress-auth: - public-crbvhau5gd0do97g1vvo50-alb1 true enabled public 1e7d00e2-us-south.lb.appdomain.cloud us-south-1 ingress:0.35.0_826_iks/ingress-auth: - List the VPC Load Balancers that were created, ibmcloud is load-balancers Listing load balancers for generation 2 compute in all resource groups and region us-south under account Remko de Knikker as user b.newell2@remkoh.dev... ID Name Family Subnets Is public Provision status Operating status Resource group r006-1e7d00e2-cc50-4f5b-a12f-b5bcfa95c439 kube-bvhau5gd0do97g1vvo50-c974a95bc72740fba2839a191ba23e17 Application remkohdev-vpcsubnet1 true active online Default r006-7a6a66a7-9871-473e-b105-20640caa0f77 kube-bvhau5gd0do97g1vvo50-22bf711e08e04827829953a008b52481 Application remkohdev-vpcsubnet1 true active online Default The Family property is listed as value Application (Application Load Balancer (ALB)) and the Name is listed as value dynamic . This means that IBM Cloud Application Load Balancer for VPC integrates with instance groups, which can auto scale your back-end members. Pool members are dynamically added and deleted based on your usage and requirements. Round-robin is the default load-balancing method, but you can also use weighted round-robin or least connections. Inspect details for each VPC ALB and inspect Listeners, Pools and Pool Members, MY_LOAD_BALANCER_ID = r006-7a6a66a7-9871-473e-b105-20640caa0f77 ibmcloud is load-balancer $MY_LOAD_BALANCER_ID --output json { \"created_at\" : \"2020-12-23T03:29:11.831Z\" , \"crn\" : \"crn:v1:bluemix:public:is:us-south:a/31296e3a285f42fdadd51ce14beba65e::load-balancer:r006-7a6a66a7-9871-473e-b105-20640caa0f77\" , \"hostname\" : \"7a6a66a7-us-south.lb.appdomain.cloud\" , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77\" , \"id\" : \"r006-7a6a66a7-9871-473e-b105-20640caa0f77\" , \"is_public\" : true, \"listeners\" : [ { \"deleted\" : { \"more_info\" : null } , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/listeners/r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\" , \"id\" : \"r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\" } ] , \"logging\" : { \"datapath\" : { \"active\" : false } } , \"name\" : \"kube-bvhau5gd0do97g1vvo50-22bf711e08e04827829953a008b52481\" , \"operating_status\" : \"online\" , \"pools\" : [ { \"deleted\" : { \"more_info\" : null } , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\" , \"id\" : \"r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\" , \"name\" : \"tcp-3000-32219\" } ] , \"private_ips\" : [ { \"address\" : \"10.240.0.7\" } , { \"address\" : \"10.240.0.8\" } ] , \"profile\" : { \"family\" : \"Application\" , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancer/profiles/dynamic\" , \"name\" : \"dynamic\" } , \"provisioning_status\" : \"active\" , \"public_ips\" : [ { \"address\" : \"52.116.134.164\" } , { \"address\" : \"52.116.142.235\" } ] , \"resource_group\" : { \"href\" : \"https://resource-controller.cloud.ibm.com/v1/resource_groups/68af6383f717459686457a6434c4d19f\" , \"id\" : \"68af6383f717459686457a6434c4d19f\" , \"name\" : \"Default\" } , \"security_groups\" : null, \"subnets\" : [ { \"deleted\" : { \"more_info\" : null } , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/subnets/0717-78f8f62d-7865-4547-8afd-6e6e3eb37f11\" , \"id\" : \"0717-78f8f62d-7865-4547-8afd-6e6e3eb37f11\" , \"name\" : \"remkohdev-vpcsubnet1\" } ] } MYLOAD_BALANCER_LISTENER_ID = r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c ibmcloud is load-balancer-listener $MY_LOAD_BALANCER_ID $MYLOAD_BALANCER_LISTENER_ID --output json { \"created_at\" : \"2020-12-23T03:29:12.078Z\" , \"default_pool\" : { \"deleted\" : { \"more_info\" : null } , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\" , \"id\" : \"r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\" , \"name\" : \"tcp-3000-32219\" } , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/listeners/r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\" , \"id\" : \"r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\" , \"policies\" : null, \"port\" : 3000 , \"protocol\" : \"tcp\" , \"provisioning_status\" : \"active\" } ibmcloud is load-balancer-pools $MY_LOAD_BALANCER_ID --output json [ { \"algorithm\" : \"round_robin\" , \"created_at\" : \"2020-12-23T03:29:11.957Z\" , \"health_monitor\" : { \"delay\" : 5 , \"max_retries\" : 2 , \"port\" : 32219 , \"timeout\" : 2 , \"type\" : \"tcp\" } , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\" , \"id\" : \"r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\" , \"members\" : [ { \"deleted\" : { \"more_info\" : null } , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492/members/r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\" , \"id\" : \"r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\" , \"port\" : null, \"target\" : null } ] , \"name\" : \"tcp-3000-32219\" , \"protocol\" : \"tcp\" , \"provisioning_status\" : \"active\" , \"proxy_protocol\" : \"disabled\" } ] MY_LOAD_BALANCER_POOL_ID = r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492 ibmcloud is load-balancer-pool-members $MY_LOAD_BALANCER_ID $MY_LOAD_BALANCER_POOL_ID --output json [ { \"created_at\" : \"2020-12-23T03:29:11.981Z\" , \"health\" : \"ok\" , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492/members/r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\" , \"id\" : \"r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\" , \"port\" : 32219 , \"provisioning_status\" : \"active\" , \"target\" : { \"address\" : \"10.240.0.4\" } , \"weight\" : 50 } ] MY_LOAD_BALANCER_POOL_MEMBER_ID = r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51 ibmcloud is load-balancer-pool-member $MY_LOAD_BALANCER_ID $MY_LOAD_BALANCER_POOL_ID $MY_LOAD_BALANCER_POOL_MEMBER_ID --output json { \"created_at\" : \"2020-12-23T03:29:11.981Z\" , \"health\" : \"ok\" , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492/members/r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\" , \"id\" : \"r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\" , \"port\" : 32219 , \"provisioning_status\" : \"active\" , \"target\" : { \"address\" : \"10.240.0.4\" } , \"weight\" : 50 } Note: above, I manually copy-pasted the IDs of the VPC resources into the next command, instead of using the full jq syntax each time to retrieve the values and set corresponding environment variables. You see that the second load balancer has a single front-end listener on port 3000 forwarding to a load balancer pool member on port 32219 on IP 10.240.0.4, which is the Private IP of the single worker node of my cluster. kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10 .240.0.4 Ready <none> 14h v1.18.13+IKS 10 .240.0.4 10 .240.0.4 Ubuntu 18 .04.5 LTS 4 .15.0-128-generic containerd://1.3.9 From the Kubernetes docs about External Load Balancer Providers you see that: \"When the Service type is set to LoadBalancer, Kubernetes provides functionality equivalent to type equals ClusterIP to pods within the cluster and extends it by programming the (external to Kubernetes) load balancer with entries for the Kubernetes pods. The Kubernetes service controller automates the creation of the external load balancer, health checks (if needed), firewall rules (if needed) and retrieves the external IP allocated by the cloud provider and populates it in the service object.\"","title":"Load\u00a0Balancer for VPC"},{"location":"vpc/vpcgen2-loadbalancer/#understanding-the-load-balancer-for-vpc","text":"To access the guestbook application via the Load Balancer for VPC, we are using the port of the Service object instead of the NodePort , which is the port we allowed inbound traffic on and which is the port value we used to access our deployment without a VPC. To see why, let's take a step back and go to the list of Load balancers for VPC . Click the load balancer with the hostname that corresponds to the External IP of the LoadBalacer service we created earlier. You see a few tabs under the long name for Back-end pools and Front-end listeners . You see the Hostname that corresponds to the external IP of the LoadBalancer service for the guestbook application. And you see a Health status panel with a Pool name value of tcp-3000-32219 with a Listener protocol and port of value TCP - 3000 . Click on the Back-end pool tab and expand the tcp-3000-32219 pool. Note the Health port for the back-end pool is the NodePort of the guestbook service, and the distribution method used is round robin . You also see that a virtual server is attached on IP address 10.240.0.4 , the private IP address of the worker node of our cluster, and on port $SVC_NODEPORT , the NodePort value of the guestbook service. Click on the Front-end listener tab and note that Port is set to the port of the guestbook service. The front-end listener is mapped to the back-end pool tcp-3000-32219 or tcp-$SVC_PORT-$SVC_NODEPORT . That tells us that it is the load balancer for VPC that maps external traffic to our guestbook application via the hostname:port of the front end listener to the back-end pool with the private IP of the cluster in the IP range of our VPC 10.240.0.0/24 , and the NodePort of the guestbook service. You can also use the CLI to analyze how the traffic flows. Let's inspect the guestbook Service of type LoadBalancer resource in more detail. kubectl get svc -n $MY_NAMESPACE --output json { \"apiVersion\" : \"v1\" , \"items\" : [ { \"apiVersion\" : \"v1\" , \"kind\" : \"Service\" , \"metadata\" : { \"creationTimestamp\" : \"2020-12-23T03:08:27Z\" , \"finalizers\" : [ \"service.kubernetes.io/load-balancer-cleanup\" ] , \"labels\" : { \"app\" : \"guestbook\" } , \"managedFields\" : [ { \"apiVersion\" : \"v1\" , \"fieldsType\" : \"FieldsV1\" , \"fieldsV1\" : { \"f:metadata\" : { \"f:labels\" : { \".\" : {} , \"f:app\" : {} } } , \"f:spec\" : { \"f:externalTrafficPolicy\" : {} , \"f:ports\" : { \".\" : {} , \"k:{\\\"port\\\":3000,\\\"protocol\\\":\\\"TCP\\\"}\" : { \".\" : {} , \"f:port\" : {} , \"f:protocol\" : {} , \"f:targetPort\" : {} } } , \"f:selector\" : { \".\" : {} , \"f:app\" : {} } , \"f:sessionAffinity\" : {} , \"f:type\" : {} } } , \"manager\" : \"kubectl-expose\" , \"operation\" : \"Update\" , \"time\" : \"2020-12-23T03:08:26Z\" } , { \"apiVersion\" : \"v1\" , \"fieldsType\" : \"FieldsV1\" , \"fieldsV1\" : { \"f:metadata\" : { \"f:finalizers\" : { \".\" : {} , \"v:\\\"service.kubernetes.io/load-balancer-cleanup\\\"\" : {} } } , \"f:status\" : { \"f:loadBalancer\" : { \"f:ingress\" : {} } } } , \"manager\" : \"ibm-cloud-controller-manager\" , \"operation\" : \"Update\" , \"time\" : \"2020-12-23T03:29:13Z\" } ] , \"name\" : \"guestbook\" , \"namespace\" : \"my-guestbook\" , \"resourceVersion\" : \"6502\" , \"selfLink\" : \"/api/v1/namespaces/my-guestbook/services/guestbook\" , \"uid\" : \"22bf711e-08e0-4827-8299-53a008b52481\" } , \"spec\" : { \"clusterIP\" : \"172.21.48.26\" , \"externalTrafficPolicy\" : \"Cluster\" , \"ports\" : [ { \"nodePort\" : 32219 , \"port\" : 3000 , \"protocol\" : \"TCP\" , \"targetPort\" : 3000 } ] , \"selector\" : { \"app\" : \"guestbook\" } , \"sessionAffinity\" : \"None\" , \"type\" : \"LoadBalancer\" } , \"status\" : { \"loadBalancer\" : { \"ingress\" : [ { \"hostname\" : \"7a6a66a7-us-south.lb.appdomain.cloud\" } ] } } } ] , \"kind\" : \"List\" , \"metadata\" : { \"resourceVersion\" : \"\" , \"selfLink\" : \"\" } } Note: Service resource lists a property status.loadBalancer.ingress with hostname . We use the hostname to access the Guestbook Service. The spec.ports.nodePort is the port on the worker nodes through which external traffic comes to the cluster. The spec.ports.port is the incoming port that the service listens for requests that are forwarded by Kubernetes from the NodePort. Requests are finally sent to the spec.ports.targetPort on any pod. Traffic flow: nodePort -> port -> targetPort The externalTrafficPolicy property is set to Cluster, which always routes traffic to all pods running a service with equal distribution, with a Service type of LoadBalancer for distributing traffic. Our deployment however is using an Ingress load balancer with an Application Load Balancer (ALB), that includes an external load balancer, which in the architecture of a VPC network is a VPC load balancer with a VPC hostname. When you create an inbound rule for the VPC load balancer, IBM Cloud defines the front-end listener mapped to the back-end pool, and uses the port and the nodeport respectively to load balance traffic.","title":"Understanding the Load\u00a0Balancer for VPC"},{"location":"vpc/vpcgen2-loadbalancer/#ingress-application-load-balancer-alb","text":"Ingress is a Kubernetes service that balances network traffic workloads in your cluster by forwarding public or private requests to your apps by using a unique public or private route. The Ingress application load balancer (ALB) is a layer 7 (L7) load balancer, which implements the NGINX Ingress controller. A layer 4 (L4) LoadBalancer service exposes the ALB so that the ALB can receive external requests that come into your cluster. An Ingress deployment consists of three components: Ingress resources, an internal L7 Application Load Balancer (ALB), an external L4 load balancer to handle incoming requests across zones. For classic clusters, this component is the Multi-Zone Load Balancer (MZLB) that IBM Cloud Kubernetes Service creates for you. For VPC clusters, this component is the VPC load balancer created in your VPC. To expose an app by using Ingress, you must create a Kubernetes service for your app and register this service with Ingress by defining an Ingress resource. (To learn how, go to Ingress and ALB ). The following diagram shows how Ingress directs communication from the internet to an app in a VPC multizone cluster. source A VPC load balancer listens for external traffic, and based on the resolved IP address, the VPC load balancer sends the request to an available Application Load Balancer (ALB). The Application Load Balancer (ALB) listens for incoming HTTP, HTTPS, or TCP service requests, checks if routing rules exist for the application, and then forwards requests to the appropriate app pod according to the rules defined in the Ingress resource. In the IKS instance, there is a private and a public Ingress ALB installed, the private Ingress ALB is disabled. ibmcloud ks ingress alb ls -c $MY_CLUSTER_NAME OK ALB ID Enabled State Type Load Balancer Hostname Zone Build Status private-crbvhau5gd0do97g1vvo50-alb1 false disabled private - us-south-1 ingress:/ingress-auth: - public-crbvhau5gd0do97g1vvo50-alb1 true enabled public 1e7d00e2-us-south.lb.appdomain.cloud us-south-1 ingress:0.35.0_826_iks/ingress-auth: - List the VPC Load Balancers that were created, ibmcloud is load-balancers Listing load balancers for generation 2 compute in all resource groups and region us-south under account Remko de Knikker as user b.newell2@remkoh.dev... ID Name Family Subnets Is public Provision status Operating status Resource group r006-1e7d00e2-cc50-4f5b-a12f-b5bcfa95c439 kube-bvhau5gd0do97g1vvo50-c974a95bc72740fba2839a191ba23e17 Application remkohdev-vpcsubnet1 true active online Default r006-7a6a66a7-9871-473e-b105-20640caa0f77 kube-bvhau5gd0do97g1vvo50-22bf711e08e04827829953a008b52481 Application remkohdev-vpcsubnet1 true active online Default The Family property is listed as value Application (Application Load Balancer (ALB)) and the Name is listed as value dynamic . This means that IBM Cloud Application Load Balancer for VPC integrates with instance groups, which can auto scale your back-end members. Pool members are dynamically added and deleted based on your usage and requirements. Round-robin is the default load-balancing method, but you can also use weighted round-robin or least connections. Inspect details for each VPC ALB and inspect Listeners, Pools and Pool Members, MY_LOAD_BALANCER_ID = r006-7a6a66a7-9871-473e-b105-20640caa0f77 ibmcloud is load-balancer $MY_LOAD_BALANCER_ID --output json { \"created_at\" : \"2020-12-23T03:29:11.831Z\" , \"crn\" : \"crn:v1:bluemix:public:is:us-south:a/31296e3a285f42fdadd51ce14beba65e::load-balancer:r006-7a6a66a7-9871-473e-b105-20640caa0f77\" , \"hostname\" : \"7a6a66a7-us-south.lb.appdomain.cloud\" , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77\" , \"id\" : \"r006-7a6a66a7-9871-473e-b105-20640caa0f77\" , \"is_public\" : true, \"listeners\" : [ { \"deleted\" : { \"more_info\" : null } , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/listeners/r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\" , \"id\" : \"r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\" } ] , \"logging\" : { \"datapath\" : { \"active\" : false } } , \"name\" : \"kube-bvhau5gd0do97g1vvo50-22bf711e08e04827829953a008b52481\" , \"operating_status\" : \"online\" , \"pools\" : [ { \"deleted\" : { \"more_info\" : null } , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\" , \"id\" : \"r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\" , \"name\" : \"tcp-3000-32219\" } ] , \"private_ips\" : [ { \"address\" : \"10.240.0.7\" } , { \"address\" : \"10.240.0.8\" } ] , \"profile\" : { \"family\" : \"Application\" , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancer/profiles/dynamic\" , \"name\" : \"dynamic\" } , \"provisioning_status\" : \"active\" , \"public_ips\" : [ { \"address\" : \"52.116.134.164\" } , { \"address\" : \"52.116.142.235\" } ] , \"resource_group\" : { \"href\" : \"https://resource-controller.cloud.ibm.com/v1/resource_groups/68af6383f717459686457a6434c4d19f\" , \"id\" : \"68af6383f717459686457a6434c4d19f\" , \"name\" : \"Default\" } , \"security_groups\" : null, \"subnets\" : [ { \"deleted\" : { \"more_info\" : null } , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/subnets/0717-78f8f62d-7865-4547-8afd-6e6e3eb37f11\" , \"id\" : \"0717-78f8f62d-7865-4547-8afd-6e6e3eb37f11\" , \"name\" : \"remkohdev-vpcsubnet1\" } ] } MYLOAD_BALANCER_LISTENER_ID = r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c ibmcloud is load-balancer-listener $MY_LOAD_BALANCER_ID $MYLOAD_BALANCER_LISTENER_ID --output json { \"created_at\" : \"2020-12-23T03:29:12.078Z\" , \"default_pool\" : { \"deleted\" : { \"more_info\" : null } , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\" , \"id\" : \"r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\" , \"name\" : \"tcp-3000-32219\" } , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/listeners/r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\" , \"id\" : \"r006-a7ca3e8f-e8d5-4ac4-8dcb-ce076dc2d01c\" , \"policies\" : null, \"port\" : 3000 , \"protocol\" : \"tcp\" , \"provisioning_status\" : \"active\" } ibmcloud is load-balancer-pools $MY_LOAD_BALANCER_ID --output json [ { \"algorithm\" : \"round_robin\" , \"created_at\" : \"2020-12-23T03:29:11.957Z\" , \"health_monitor\" : { \"delay\" : 5 , \"max_retries\" : 2 , \"port\" : 32219 , \"timeout\" : 2 , \"type\" : \"tcp\" } , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\" , \"id\" : \"r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492\" , \"members\" : [ { \"deleted\" : { \"more_info\" : null } , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492/members/r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\" , \"id\" : \"r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\" , \"port\" : null, \"target\" : null } ] , \"name\" : \"tcp-3000-32219\" , \"protocol\" : \"tcp\" , \"provisioning_status\" : \"active\" , \"proxy_protocol\" : \"disabled\" } ] MY_LOAD_BALANCER_POOL_ID = r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492 ibmcloud is load-balancer-pool-members $MY_LOAD_BALANCER_ID $MY_LOAD_BALANCER_POOL_ID --output json [ { \"created_at\" : \"2020-12-23T03:29:11.981Z\" , \"health\" : \"ok\" , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492/members/r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\" , \"id\" : \"r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\" , \"port\" : 32219 , \"provisioning_status\" : \"active\" , \"target\" : { \"address\" : \"10.240.0.4\" } , \"weight\" : 50 } ] MY_LOAD_BALANCER_POOL_MEMBER_ID = r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51 ibmcloud is load-balancer-pool-member $MY_LOAD_BALANCER_ID $MY_LOAD_BALANCER_POOL_ID $MY_LOAD_BALANCER_POOL_MEMBER_ID --output json { \"created_at\" : \"2020-12-23T03:29:11.981Z\" , \"health\" : \"ok\" , \"href\" : \"https://us-south.iaas.cloud.ibm.com/v1/load_balancers/r006-7a6a66a7-9871-473e-b105-20640caa0f77/pools/r006-338f0670-7a16-45e0-b42c-ca5f9a3bd492/members/r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\" , \"id\" : \"r006-c422b1cb-79ee-44fc-8360-d4ebcaf1dd51\" , \"port\" : 32219 , \"provisioning_status\" : \"active\" , \"target\" : { \"address\" : \"10.240.0.4\" } , \"weight\" : 50 } Note: above, I manually copy-pasted the IDs of the VPC resources into the next command, instead of using the full jq syntax each time to retrieve the values and set corresponding environment variables. You see that the second load balancer has a single front-end listener on port 3000 forwarding to a load balancer pool member on port 32219 on IP 10.240.0.4, which is the Private IP of the single worker node of my cluster. kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10 .240.0.4 Ready <none> 14h v1.18.13+IKS 10 .240.0.4 10 .240.0.4 Ubuntu 18 .04.5 LTS 4 .15.0-128-generic containerd://1.3.9 From the Kubernetes docs about External Load Balancer Providers you see that: \"When the Service type is set to LoadBalancer, Kubernetes provides functionality equivalent to type equals ClusterIP to pods within the cluster and extends it by programming the (external to Kubernetes) load balancer with entries for the Kubernetes pods. The Kubernetes service controller automates the creation of the external load balancer, health checks (if needed), firewall rules (if needed) and retrieves the external IP allocated by the cloud provider and populates it in the service object.\"","title":"Ingress Application Load Balancer\u00a0(ALB)"},{"location":"vpc/vpcgen2-using-ui/","text":"Create a Cluster with VPC using the UI \u00b6 This section uses the browser and IBM Cloud UI to create a Red Hat OpenShift cluster and VPC. Using the UI, create an IBM Cloud Red Hat OpenShift Kubernetes Service (ROKS), for infrastructure provider choose Generation 2 Compute instead of Classic. For a comparison of infrastructure providers, read Supported Infrastructure Providers, or read an Overview of VPC Networking in IBM Cloud Kubernetes Service (IKS). This will setup the following infrastructure for you: Virtual Private Cloud, watch this good primer on VPC by Ryan Sumner , Subnet, Security Group, Access Control List (ACL), A Public Gateway with a Floating IP for Public Access, An encrypted VPN Gateway or a Direct Link Private Circuits for Private Access, An elastic Load Balancer for VPC, using an Application Load Balancer (ALB), which includes Sysdig monitoring, High Availability (HA) with a Domain Name Server (DNS), Multi-Zone Region (MZR) support, L4 network layer and L7 application layer support, both public and private load balancing. Steps: Setup, Create a VPC Generation 2 Compute using CLI Create a Kubernetes Cluster Deploy the Guestbook Application, Update the Security Group Understanding the Load Balancer Ingress Application Load Balancer (ALB) Cleanup [Optional] Using UI to Create a Cluster with VPC Generation 2 Compute Create a VPC \u00b6 Go to https://cloud.ibm.com/vpc-ext/network/vpcs , Click the Create button, Edit the configuration of the new VPC, Enter a Name for the VPC: $USERNAME-vpc-ussouth2-1 Select a Resource group : $USERNAME-rg Add Tags : $USERNAME, vpc1, tutorial Default ACL Rules are set to Allow all In the Default security group , check Allow SSH and Allow ping Check Create a default prefix for each zone , Enter a Name for the New subnet for VPC : $USERNAME-subnet-ussouth2-1 Select a Resource group : $USERNAME-rg For Location , select Dallas and select Dallas 2 In the IP range selection , accept all defaults, enable the Public gateway switch to Attached In the right panel, click Create virtual private cloud , The Virtual Private Clouds list will load, Locate the VPC you just created and click the linked name of your VPC, The VPC Overview page loads, Create an Additional Subnet \u00b6 TODO create subnet $USERNAME-subnet-ussouth3-2 Create Object Storage \u00b6 TODO create $USERNAME-cos-1 Create an OpenShift Cluster \u00b6 Select the Containers Category in the IBM Cloud Catalog , Select the tile for Red Hat OpenShift on IBM Cloud, note that this service is tagged as Satellite Enabled , Financial Services Validated , IAM-Enabled , and Service Endpoint Supported , Select the openShift version, e.g. 4.6.x Select your OCP entitlement, For Infrastructure select VPC , which will load your existing VPC infrastructure, In Virtual private cloud , select the VPC you created above or another existing VPC to attach your cluster to, e.g. $USERNAME-vpc-ussouth2-1 , In Cloud Object Storage , select the Cloud Object Storage instance you created above or another existing Cloud Object Storage instance, e.g. $USERNAME-cos-1 , Under Location , select the same Resource group as for the VPC resource group, $USERNAME-rg, For Worker zones , available subnets will be loaded. Select $USERNAME-subnet-ussouth2-1 in Dallas 2 , uncheck zones without subnet and subnets you do not want to use, Select your flavor or node configuration, e.g. bx2.4x16 with 4 vCPU, 16GB Memory, using RHEL nodes, Select number of Worker nodes per zone , e.g. 3 Name your worker pool, e.g. $USERNAME-worker-pool-ussouth-1 Select the master service endpoints, choose Private endpoint only to access your cluster via the private network or VPN tunnel, Under Resource details , enter a Cluster name , e.g. $USERNAME-roks46-3n-tutorial-cl1 Add Tags : $USERNAME, vpc1, tutorial In the right column, review your configuration, the estimated cost and to accept, click the Create button, TODO: check popup notification \"Your cluster order is complete and your cluster is creating. However, some related components need your attention: If your cluster runs Kubernetes version 1.18 or earlier, the default security group 'stifled-legendary-hurdle-excavator' (ID 'r006-1bb65293-c287-4bb9-9783-14ee2f83488f') for VPC 'remkohdev-vpc-ussouth2-1' (ID 'r006-f5e669a8-e4f1-4724-97a6-eb4d7f854e2d') must be edited to allow incoming network traffic from the VPC load balancers. For more info, see http://ibm.biz/vpc-sgs\" . The cluster order is created and the new cluster Access page will load, Click the Overview link, wait until cluster creation has completed, you should be seeing the Ingress Subdomain appear, From the IBM Cloud drop down menu in the top left of your page, browse to VPC Infrastructure , VPCs , Go to subnet, A Virtual private endpoint gateway for VPC was created","title":"Create a Cluster with VPC using the UI"},{"location":"vpc/vpcgen2-using-ui/#create-a-cluster-with-vpc-using-the-ui","text":"This section uses the browser and IBM Cloud UI to create a Red Hat OpenShift cluster and VPC. Using the UI, create an IBM Cloud Red Hat OpenShift Kubernetes Service (ROKS), for infrastructure provider choose Generation 2 Compute instead of Classic. For a comparison of infrastructure providers, read Supported Infrastructure Providers, or read an Overview of VPC Networking in IBM Cloud Kubernetes Service (IKS). This will setup the following infrastructure for you: Virtual Private Cloud, watch this good primer on VPC by Ryan Sumner , Subnet, Security Group, Access Control List (ACL), A Public Gateway with a Floating IP for Public Access, An encrypted VPN Gateway or a Direct Link Private Circuits for Private Access, An elastic Load Balancer for VPC, using an Application Load Balancer (ALB), which includes Sysdig monitoring, High Availability (HA) with a Domain Name Server (DNS), Multi-Zone Region (MZR) support, L4 network layer and L7 application layer support, both public and private load balancing. Steps: Setup, Create a VPC Generation 2 Compute using CLI Create a Kubernetes Cluster Deploy the Guestbook Application, Update the Security Group Understanding the Load Balancer Ingress Application Load Balancer (ALB) Cleanup [Optional] Using UI to Create a Cluster with VPC Generation 2 Compute","title":"Create a Cluster with VPC using the UI"},{"location":"vpc/vpcgen2-using-ui/#create-a-vpc","text":"Go to https://cloud.ibm.com/vpc-ext/network/vpcs , Click the Create button, Edit the configuration of the new VPC, Enter a Name for the VPC: $USERNAME-vpc-ussouth2-1 Select a Resource group : $USERNAME-rg Add Tags : $USERNAME, vpc1, tutorial Default ACL Rules are set to Allow all In the Default security group , check Allow SSH and Allow ping Check Create a default prefix for each zone , Enter a Name for the New subnet for VPC : $USERNAME-subnet-ussouth2-1 Select a Resource group : $USERNAME-rg For Location , select Dallas and select Dallas 2 In the IP range selection , accept all defaults, enable the Public gateway switch to Attached In the right panel, click Create virtual private cloud , The Virtual Private Clouds list will load, Locate the VPC you just created and click the linked name of your VPC, The VPC Overview page loads,","title":"Create a VPC"},{"location":"vpc/vpcgen2-using-ui/#create-an-additional-subnet","text":"TODO create subnet $USERNAME-subnet-ussouth3-2","title":"Create an Additional Subnet"},{"location":"vpc/vpcgen2-using-ui/#create-object-storage","text":"TODO create $USERNAME-cos-1","title":"Create Object Storage"},{"location":"vpc/vpcgen2-using-ui/#create-an-openshift-cluster","text":"Select the Containers Category in the IBM Cloud Catalog , Select the tile for Red Hat OpenShift on IBM Cloud, note that this service is tagged as Satellite Enabled , Financial Services Validated , IAM-Enabled , and Service Endpoint Supported , Select the openShift version, e.g. 4.6.x Select your OCP entitlement, For Infrastructure select VPC , which will load your existing VPC infrastructure, In Virtual private cloud , select the VPC you created above or another existing VPC to attach your cluster to, e.g. $USERNAME-vpc-ussouth2-1 , In Cloud Object Storage , select the Cloud Object Storage instance you created above or another existing Cloud Object Storage instance, e.g. $USERNAME-cos-1 , Under Location , select the same Resource group as for the VPC resource group, $USERNAME-rg, For Worker zones , available subnets will be loaded. Select $USERNAME-subnet-ussouth2-1 in Dallas 2 , uncheck zones without subnet and subnets you do not want to use, Select your flavor or node configuration, e.g. bx2.4x16 with 4 vCPU, 16GB Memory, using RHEL nodes, Select number of Worker nodes per zone , e.g. 3 Name your worker pool, e.g. $USERNAME-worker-pool-ussouth-1 Select the master service endpoints, choose Private endpoint only to access your cluster via the private network or VPN tunnel, Under Resource details , enter a Cluster name , e.g. $USERNAME-roks46-3n-tutorial-cl1 Add Tags : $USERNAME, vpc1, tutorial In the right column, review your configuration, the estimated cost and to accept, click the Create button, TODO: check popup notification \"Your cluster order is complete and your cluster is creating. However, some related components need your attention: If your cluster runs Kubernetes version 1.18 or earlier, the default security group 'stifled-legendary-hurdle-excavator' (ID 'r006-1bb65293-c287-4bb9-9783-14ee2f83488f') for VPC 'remkohdev-vpc-ussouth2-1' (ID 'r006-f5e669a8-e4f1-4724-97a6-eb4d7f854e2d') must be edited to allow incoming network traffic from the VPC load balancers. For more info, see http://ibm.biz/vpc-sgs\" . The cluster order is created and the new cluster Access page will load, Click the Overview link, wait until cluster creation has completed, you should be seeing the Ingress Subdomain appear, From the IBM Cloud drop down menu in the top left of your page, browse to VPC Infrastructure , VPCs , Go to subnet, A Virtual private endpoint gateway for VPC was created","title":"Create an OpenShift Cluster"},{"location":"vpc/vpcgen2/","text":"Secure a Kubernetes Cluster with VPC \u00b6 This tutorial explains how to start to air-gap and secure an OpenShift or Kubernetes cluster and physically isolate your cluster using a Virtual Private Cloud (VPC). Using an air gapped cluster is one of the first things you will do to secure your container deployments. For more information about air-gap , go here . The first step to secure your cluster is to create a Virtual Private Cloud (VPC) and add rules to a Security Group of an Application Load Balancer (ALB) to allow certain inbound traffic. You can add a gateway like API Connect to the VPC and expose the gateway to manage traffic, for instance by using OAuth, rate limiting and API key access control. Setup \u00b6 For setup and pre-requisities, go here . About \u00b6 With Red Hat OpenShift Kubernetes (ROKS) or IBM Cloud Kubernetes Service (IKS) on VPC Generation 2 Computeon IBM Cloud, you can create an OpenShift or Kubernetes cluster in Virtual Private Cloud (VPC) infrastructure in a matter of minutes. OpenShift is a more enterprise-ready secure Container Orchestration (CO) platform, but in this tutorial I use a plain managed Kubernetes service because it is more accessible and affordable. If you want however, you can choose to use OpenShift instead. This tutorial uses a free IBM Cloud account, a free Pay-As-You-Go account, a free IBM Cloud Kubernetes Service (IKS) service with Kubernetes v1.18 with 1 worker node, and a free Virtual Private Cloud (VPC) service. This tutorial is based on the official documentation for creating a cluster in your Virtual Private Cloud (VPC) on generation 2 compute . Steps: Setup, Create a VPC Generation 2 Compute using CLI Create a Kubernetes Cluster Deploy the Guestbook Application, Update the Security Group Understanding the Load Balancer Ingress Application Load Balancer (ALB) Cleanup [Optional] Using UI to Create a Cluster with VPC Generation 2 Compute Setup CLI \u00b6 Add the VPC infrastructure service plug-in to the IBM Cloud CLI in the client terminal, ibmcloud plugin install infrastructure-service verify the installed plugins, $ ibmcloud plugin list Listing installed plug-ins... Plugin Name Version Status cloud-functions/wsk/functions/fn 1.0.46 Update Available cloud-object-storage 1.2.1 Update Available container-registry 0.1.494 Update Available container-service/kubernetes-service 1.0.171 Update Available vpc-infrastructure/infrastructure-service 0.7.6 Set the following environment variables to be used in the remainder of the tutorial. Set the USERNAME to a unique value shorter than 10 characters and set the IBMID to the email you used to create the IBM Cloud account. The other variables don't need to be changed but you can choose to rename them if you prefer. USERNAME=<bnewell> IBMID=<b.newell2@remkoh.dev> MY_REGION=us-south MY_ZONE=$MY_REGION-1 MY_VPC_NAME=$USERNAME-vpcgen2-vpc1 MY_VPC_SUBNET_NAME=$USERNAME-vpcsubnet1 MY_PUBLIC_GATEWAY=$USERNAME-public-gateway1 MY_CLUSTER_NAME=$USERNAME-iks118-vpc-cluster1 KS_VERSION=1.18 MY_NAMESPACE=my-guestbook Log in to the IBM Cloud account, if you use a Single Sign-On (SSO) provider, use the --sso flag instead of the username flag. ibmcloud login -u $IBMID [--sso] Target the region where you want to create your VPC environment. The resource group flag is optional, if you happen to know it, you can target it explicitly, but this tutorial does not use it. ibmcloud target -r $MY_REGION [-g <resource_group>] The VPC must be set up in the same multi-zone metro location where you want to create your cluster. Target generation 2 infrastructure. ibmcloud is target --gen 2 Create a VPC Generation 2 Compute using CLI \u00b6 This section uses the IBM Cloud CLI to create a cluster in your VPC on generation 2 compute. Create a VPC, ibmcloud is vpc-create $MY_VPC_NAME ibmcloud is vpcs ID Name Status Classic access Default network ACL Default security group Resource group r006\u20133883b334\u20131f4e-4d6a-b3b7\u2013343a029d81bc remkohdev-vpcgen2-vpc1 available false prowling-prevail-universe-equivocal hatracks-fraying-unloader-prevail default Set an environment variable MY_VPC_ID , MY_VPC_ID=$(ibmcloud is vpcs --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_NAME\\\"') | .id ') echo $MY_VPC_ID Create a subnet ibmcloud is subnet-create $MY_VPC_SUBNET_NAME $MY_VPC_ID --zone $MY_ZONE --ipv4-address-count 256 Set an environment variable MY_VPC_SUBNET_ID , MY_VPC_SUBNET_ID=$(ibmcloud is subnets --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_SUBNET_NAME\\\"') | .id ') echo $MY_VPC_SUBNET_ID Inspect the VPC's default security group, set an environment variable MY_DEFAULT_SG_NAME and MY_DEFAULT_SG_ID , MY_DEFAULT_SG_NAME=$(ibmcloud is vpcs --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_NAME\\\"') | .default_security_group.name ') echo $MY_DEFAULT_SG_NAME MY_DEFAULT_SG_ID=$(ibmcloud is security-groups --output json | jq -r '.[] | select( .name=='\\\"$MY_DEFAULT_SG_NAME\\\"') | .id ') echo $MY_DEFAULT_SG_ID ibmcloud is security-group $MY_DEFAULT_SG_ID --output json { \"created_at\": \"2020-12-18T22:54:04.000Z\", \"crn\": \"crn:v1:bluemix:public:is:us-south:a/e65910fa61ce9072d64902d03f3d4774::security-group:r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/security_groups/r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"id\": \"r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"name\": \"hatracks-fraying-unloader-prevail\", \"network_interfaces\": [ { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/instances/0717_2ba757dd-0d40-48c6-b72e-6fb2ea12584a/network_interfaces/0717-57f73998-6af6-43e1-981f-698f021f4842\", \"id\": \"0717-57f73998-6af6-43e1-981f-698f021f4842\", \"name\": \"unpleased-urethane-recycled-subduing\", \"primary_ipv4_address\": \"10.240.0.4\", \"resource_type\": \"network_interface\" }, { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/instances/0717_3affef2d-c4b7-49e0-a785-c65ad2f64a35/network_interfaces/0717-0dc82474-6b74-48e7-89e7-536994a45524\", \"id\": \"0717-0dc82474-6b74-48e7-89e7-536994a45524\", \"name\": \"sweep-timpani-unable-stricken\", \"primary_ipv4_address\": \"10.240.0.5\", \"resource_type\": \"network_interface\" } ], \"resource_group\": { \"href\": \"https://resource-controller.cloud.ibm.com/v2/resource_groups/fdd290732f7d47909181a189494e2990\", \"id\": \"fdd290732f7d47909181a189494e2990\", \"name\": \"default\" }, \"rules\": [ { \"direction\": \"outbound\", \"id\": \"r006-c0b88082-748c-47a8-bc75-c9aabb3da4e3\", \"ip_version\": \"ipv4\", \"protocol\": \"all\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } }, { \"direction\": \"inbound\", \"id\": \"r006-4693fcb0-2998-4d73-b62f-9dd80fe1ac56\", \"ip_version\": \"ipv4\", \"protocol\": \"all\", \"remote\": { \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/security_groups/r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"id\": \"r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"name\": \"hatracks-fraying-unloader-prevail\" } }, { \"direction\": \"inbound\", \"id\": \"r006-b0fb17d4-b858-4a1f-bb8c-24231f527155\", \"ip_version\": \"ipv4\", \"port_max\": 22, \"port_min\": 22, \"protocol\": \"tcp\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } }, { \"direction\": \"inbound\", \"id\": \"r006-87a1f11b-6d30-4d63-84df-1bda1c5e25f2\", \"ip_version\": \"ipv4\", \"protocol\": \"icmp\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" }, \"type\": 8 }, { \"direction\": \"inbound\", \"id\": \"r006-ade5f825-49e0-47bf-b6cf-b020b5fa23b8\", \"ip_version\": \"ipv4\", \"port_max\": 80, \"port_min\": 80, \"protocol\": \"tcp\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } } ], \"targets\": null, \"vpc\": { \"crn\": \"crn:v1:bluemix:public:is:us-south:a/e65910fa61ce9072d64902d03f3d4774::vpc:r006-3883b334-1f4e-4d6a-b3b7-343a029d81bc\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/vpcs/r006-3883b334-1f4e-4d6a-b3b7-343a029d81bc\", \"id\": \"r006-3883b334-1f4e-4d6a-b3b7-343a029d81bc\", \"name\": \"remkohdev-vpcgen2-vpc1\" } } Create a public gateway, and update the subnet with the public gateway, this should create a floating IP for your public gateway attached to your subnet, ibmcloud is public-gateway-create $MY_PUBLIC_GATEWAY $MY_VPC_ID $MY_ZONE MY_PUBLIC_GATEWAY_ID=$(ibmcloud is public-gateways --output json | jq -r '.[] | select( .name=='\\\"$MY_PUBLIC_GATEWAY\\\"') | .id') echo $MY_PUBLIC_GATEWAY_ID ibmcloud is subnet-update $MY_VPC_SUBNET_ID --public-gateway-id $MY_PUBLIC_GATEWAY_ID MY_FLOATING_IP=$(ibmcloud is subnet-public-gateway $MY_VPC_SUBNET_ID --output json | jq -r '.floating_ip.address') echo $MY_FLOATING_IP Check that the public gateway is now attached to the subnet by retrieving the public gateway id from the subnet configuration, MY_PUBLIC_GATEWAY_ID2=$(ibmcloud is subnets --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_SUBNET_NAME\\\"') | .public_gateway.id ') echo $MY_PUBLIC_GATEWAY_ID2 To check the new infrastructure in the IBM Cloud dashboard, go to your subnets for VPC , Click the linked subnet name to view the new subnet details, You should see a public gateway attached with a floating IP. Create a Kubernetes Cluster \u00b6 Create a cluster in your VPC in the same zone as the subnet. By default, your cluster is created with a public and a private service endpoint. You can use the public service endpoint to access the Kubernetes master, We already defined environment variables for the region, zone and Kubernetes version, but start to check these are valid and available, ibmcloud ks zone ls --provider vpc-gen2 ibmcloud ks versions Create the cluster in the VPC, $ ibmcloud ks cluster create vpc-gen2 --name $MY_CLUSTER_NAME --zone $MY_ZONE --version $KS_VERSION --flavor bx2.2x8 --workers 1 --vpc-id $MY_VPC_ID --subnet-id $MY_VPC_SUBNET_ID Creating cluster... OK Cluster created with ID bvglln7d0e5j0u9lfa80 Review your IBM Cloud account resources, Click the linked cluster name of the cluster you just created. If you do not see the cluster listed yet, wait and refresh the page. Check the status of the new cluster, ibmcloud ks clusters MY_CLUSTER_ID = $( ibmcloud ks clusters --output json --provider vpc-gen2 | jq -r '.[] | select( .name==' \\\" $MY_CLUSTER_NAME \\\" ') | .id ' ) echo $MY_CLUSTER_ID To continue with the next step, the cluster status and the Ingress status must indicate to be available. The cluster might take about 15 minutes to complete. ibmcloud ks cluster get --cluster $MY_CLUSTER_ID Once the cluster is fully provisioned including the Ingress Application Load Balancer (ALB), you should see the Worker nodes status to be 100% Normal , the Ingress subdomain should be populated, among other, Or via the CLI, $ ibmcloud ks cluster get --cluster $MY_CLUSTER_ID Retrieving cluster c031jqqd0rll2ksfg97g... OK Name: bnewell2-iks118-vpc-cluster1 ID: c031jqqd0rll2ksfg97g State: normal Status: All Workers Normal Created: 2021 -01-18 23 :29:47 +0000 ( 16 hours ago ) Resource Group ID: 68af6383f717459686457a6434c4d19f Resource Group Name: Default Pod Subnet: 172 .17.0.0/18 Service Subnet: 172 .21.0.0/16 Workers: 1 Worker Zones: us-south-1 Ingress Subdomain: bnewell2-iks118-vpc-clu-47d7983a425e05fef831e694b7945b16-0000.us-south.containers.appdomain.cloud Ingress Secret: bnewell2-iks118-vpc-clu-47d7983a425e05fef831e694b7945b16-0000 Ingress Status: warning Ingress Message: One or more ALBs are unhealthy. See http://ibm.biz/ingress-alb-check Creator: - Public Service Endpoint URL: https://c111.us-south.containers.cloud.ibm.com:31097 Private Service Endpoint URL: https://c111.private.us-south.containers.cloud.ibm.com:31097 Pull Secrets: enabled in the default namespace VPCs: r006-3c9ab19c-e8af-4eb3-ab60-b9777c3cce1c Master Status: Ready ( 15 hours ago ) State: deployed Health: normal Version: 1 .18.14_1537 Location: Dallas URL: https://c111.us-south.containers.cloud.ibm.com:31097 You can also check the status of the Application Load Balancer (ALB), $ ibmcloud ks ingress alb ls --cluster $MY_CLUSTER_ID OK ALB ID Enabled State Type Load Balancer Hostname Zone Build Status private-crc031jqqd0rll2ksfg97g-alb1 false disabled private - us-south-1 ingress:/ingress-auth: - public-crc031jqqd0rll2ksfg97g-alb1 true enabled public f128a85f-us-south.lb.appdomain.cloud us-south-1 ingress:0.35.0_869_iks/ingress-auth: - Deploy the Guestbook Application \u00b6 Now the cluster is fully provisioned successfully, you can connect to your cluster and set the current-context of the kubectl , ibmcloud ks cluster config --cluster $MY_CLUSTER_ID kubectl config current-context If you have multiple configuration and contexts, you can easily switch between contexts, kubectl config use-context $MY_CLUSTER_NAME / $MY_CLUSTER_ID Deploy the guestbook application, kubectl create namespace $MY_NAMESPACE kubectl create deployment guestbook --image=ibmcom/guestbook:v1 -n $MY_NAMESPACE kubectl expose deployment guestbook --type=\"LoadBalancer\" --port=3000 --target-port=3000 -n $MY_NAMESPACE List the created service for guestbook, $ kubectl get svc -n $MY_NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook LoadBalancer 172 .21.48.26 7a6a66a7-us-south.lb.appdomain.cloud 3000 :32219/TCP 31m Create environment variables for the public IP address of the LoadBalancer service, the NodePort , and the port, SVC_EXTERNAL_IP = $( kubectl get svc -n $MY_NAMESPACE --output json | jq -r '.items[] | .status.loadBalancer.ingress[0].hostname ' ) echo $SVC_EXTERNAL_IP SVC_NODEPORT = $( kubectl get svc -n $MY_NAMESPACE --output json | jq -r '.items[].spec.ports[] | .nodePort' ) echo $SVC_NODEPORT SVC_PORT = $( kubectl get svc -n $MY_NAMESPACE --output json | jq -r '.items[].spec.ports[] | .port' ) echo $SVC_PORT Note the external IP address is set to the hostname of one of two Load Balancers for VPC . See the Load balancers for VPC in the IBM Cloud VPC Infrastructure listing. Try to send a request to the guestbook application, curl http:// $SVC_EXTERNAL_IP : $SVC_PORT curl: ( 52 ) Empty reply from server Even if you have created a LoadBalancer service with an external IP, the service cannot be reached because the VPC does not include a rule to allow incoming or ingress traffic to the service. Update the Security Group \u00b6 To allow any traffic to applications that are deployed on your cluster's worker nodes, you have to modify the VPC's default security group by ID. Update the security group and add an inbound rule for the NodePort of the service you created when exposing the guestbook deployment. I only want to allow ingress traffic on the NodePort, so I set the minimum and maximum value of allowed inbound ports to the same NodePort value. $ ibmcloud is security-group-rule-add $MY_DEFAULT_SG_ID inbound tcp --port-min $SVC_NODEPORT --port-max $SVC_NODEPORT Creating rule for security group r006-b4f498ea-1e71-489e-95f0-0e64cf8d520f under account Remko de Knikker as user b.newell2@remkoh.dev... ID r006-26b196de-5e3a-4a7d-b5fb-9baf9173feb7 Direction inbound IP version ipv4 Protocol tcp Min destination port 32219 Max destination port 32219 Remote 0 .0.0.0/0 List all the security group rules, $ ibmcloud is security-group-rules $MY_DEFAULT_SG_ID Listing rules of security group r006-b4f498ea-1e71-489e-95f0-0e64cf8d520f under account Remko de Knikker as user b.newell2@remkoh.dev... ID Direction IP version Protocol Remote r006-6c281868-cfbd-46d1-b714-81e085ae2b85 outbound ipv4 all 0.0.0.0/0 r006-b07bbf77-84a0-4e35-90b7-5422f035cf1e inbound ipv4 all compacted-imprison-clinic-support r006-26b196de-5e3a-4a7d-b5fb-9baf9173feb7 inbound ipv4 tcp Ports:Min=32219,Max=32219 0.0.0.0/0 Or add a security group rule to allow inbound TCP traffic on all Kubernetes ports in the range of 30000\u201332767. Try again to reach the guestbook application, curl http://$SVC_EXTERNAL_IP:$SVC_PORT You should now be able to see the HTML response object from the Guestbook application. Open the guestbook URL in a browser to review the web page. echo http://$SVC_EXTERNAL_IP:$SVC_PORT If you want to understand better how the load balancing for VPC works, review the optional extra section Understanding the Load Balancer for VPC You can try removing the inbound rule again to check if the VPC rejects the request again, ibmcloud is security-group-rule-delete $MY_DEFAULT_SG_ID $MY_DEFAULT_SG_RULE_ID Conclusion \u00b6 You are awesome! You secured your Kubernetes cluster with a Virtual Private Cloud (VPC) and started to air-gap the cluster, blocking direct access to your cluster. Security is an important integral part of any software application development and airgapping your cluster by adding a VPC Generation 2 is a first step in securing your cluster, network and containers. Cleanup \u00b6 To conclude, you can choose to delete your Kubernetes resources for this tutorial, $ ibmcloud ks cluster rm --cluster $MY_CLUSTER_NAME Do you want to delete the persistent storage for this cluster? If yes, the data cannot be recovered. If no, you can delete the persistent storage later in your IBM Cloud infrastructure account. [ y/N ] > y After you run this command, the cluster cannot be restored. Remove the cluster remkohdev-iks118-vpc-cluster1? [ y/N ] > y Removing cluster remkohdev-iks118-vpc-cluster1, persistent storage... OK Now the Kubernetes cluster is deleted, we need to first remove the public gateway and load balancers from the subnet, then remove the subnet from the VPC, and delete the VPC. Delete the Gateways, Load Balancers, Network Interfaces, subnet, public gateways, and finally delete the VPC, ibmcloud is security-group-rules $MY_DEFAULT_SG_ID --output json MY_DEFAULT_SG_RULE_ID = $( ibmcloud is security-group-rules $MY_DEFAULT_SG_ID --output json | jq -r --arg SVC_NODEPORT $SVC_NODEPORT '.[] | select( .port_max==($SVC_NODEPORT|tonumber)) | .id ' ) echo $MY_DEFAULT_SG_RULE_ID Delete the rule for the security group, $ ibmcloud is security-group-rule-delete $MY_DEFAULT_SG_ID $MY_DEFAULT_SG_RULE_ID This will delete security group rule r006-4f5f795f-fa90-44e4-b9c4-cb9457a2a421 and cannot be undone. Continue [ y/N ] ?> y Deleting rule r006-4f5f795f-fa90-44e4-b9c4-cb9457a2a421 from security group r006-db96d593-c224-497d-888c-03d84f6d8e98 under account Remko de Knikker as user b.newell2@remkoh.dev... OK Rule r006-4f5f795f-fa90-44e4-b9c4-cb9457a2a421 is deleted. Detach the public gateway from the subnet, $ ibmcloud is subnet-public-gateway-detach $MY_VPC_SUBNET_ID Detaching public gateway from subnet 0717 -57ebaf2d-0de6-4630-af01-6cd84031b679 under account Remko de Knikker as user b.newell2@remkoh.dev... OK Public gateway is detached. Delete the public gateway, ibmcloud is public-gateways PUBLIC_GATEWAY_ID = $( ibmcloud is public-gateways --output json | jq '.[0]' | jq -r '.id' ) echo $PUBLIC_GATEWAY_ID $ ibmcloud is public-gateway-delete $PUBLIC_GATEWAY_ID This will delete public gateway r006-f4603b78-839b-42a5-949c-76403948821a and cannot be undone. Continue [ y/N ] ?> y Deleting public gateway r006-f4603b78-839b-42a5-949c-76403948821a under account Remko de Knikker as user b.newell2@remkoh.dev... OK Public gateway r006-f4603b78-839b-42a5-949c-76403948821a is deleted. Delete the subnet, $ ibmcloud is subnet-delete $MY_VPC_SUBNET_ID This will delete Subnet 0717 -57ebaf2d-0de6-4630-af01-6cd84031b679 and cannot be undone. Continue [ y/N ] ?> y Deleting subnet 0717 -57ebaf2d-0de6-4630-af01-6cd84031b679 under account Remko de Knikker as user b.newell2@remkoh.dev... OK Subnet 0717 -57ebaf2d-0de6-4630-af01-6cd84031b679 is deleted. Delete the Virtual Private Cloud, MY_VPC_ID = $( ibmcloud is vpcs --output json | jq -r '.[] | select( .name==' \\\" $MY_VPC_NAME \\\" ') | .id ' ) echo $MY_VPC_ID $ ibmcloud is vpc-delete $MY_VPC_IDThis will delete vpc r006-3c9ab19c-e8af-4eb3-ab60-b9777c3cce1c and cannot be undone. Continue [ y/N ] ?> y Deleting vpc r006-3c9ab19c-e8af-4eb3-ab60-b9777c3cce1c under account Remko de Knikker as user b.newell2@remkoh.dev... OK vpc r006-3c9ab19c-e8af-4eb3-ab60-b9777c3cce1c is deleted.","title":"VPC Gen2"},{"location":"vpc/vpcgen2/#secure-a-kubernetes-cluster-with-vpc","text":"This tutorial explains how to start to air-gap and secure an OpenShift or Kubernetes cluster and physically isolate your cluster using a Virtual Private Cloud (VPC). Using an air gapped cluster is one of the first things you will do to secure your container deployments. For more information about air-gap , go here . The first step to secure your cluster is to create a Virtual Private Cloud (VPC) and add rules to a Security Group of an Application Load Balancer (ALB) to allow certain inbound traffic. You can add a gateway like API Connect to the VPC and expose the gateway to manage traffic, for instance by using OAuth, rate limiting and API key access control.","title":"Secure a Kubernetes Cluster with VPC"},{"location":"vpc/vpcgen2/#setup","text":"For setup and pre-requisities, go here .","title":"Setup"},{"location":"vpc/vpcgen2/#about","text":"With Red Hat OpenShift Kubernetes (ROKS) or IBM Cloud Kubernetes Service (IKS) on VPC Generation 2 Computeon IBM Cloud, you can create an OpenShift or Kubernetes cluster in Virtual Private Cloud (VPC) infrastructure in a matter of minutes. OpenShift is a more enterprise-ready secure Container Orchestration (CO) platform, but in this tutorial I use a plain managed Kubernetes service because it is more accessible and affordable. If you want however, you can choose to use OpenShift instead. This tutorial uses a free IBM Cloud account, a free Pay-As-You-Go account, a free IBM Cloud Kubernetes Service (IKS) service with Kubernetes v1.18 with 1 worker node, and a free Virtual Private Cloud (VPC) service. This tutorial is based on the official documentation for creating a cluster in your Virtual Private Cloud (VPC) on generation 2 compute . Steps: Setup, Create a VPC Generation 2 Compute using CLI Create a Kubernetes Cluster Deploy the Guestbook Application, Update the Security Group Understanding the Load Balancer Ingress Application Load Balancer (ALB) Cleanup [Optional] Using UI to Create a Cluster with VPC Generation 2 Compute","title":"About"},{"location":"vpc/vpcgen2/#setup-cli","text":"Add the VPC infrastructure service plug-in to the IBM Cloud CLI in the client terminal, ibmcloud plugin install infrastructure-service verify the installed plugins, $ ibmcloud plugin list Listing installed plug-ins... Plugin Name Version Status cloud-functions/wsk/functions/fn 1.0.46 Update Available cloud-object-storage 1.2.1 Update Available container-registry 0.1.494 Update Available container-service/kubernetes-service 1.0.171 Update Available vpc-infrastructure/infrastructure-service 0.7.6 Set the following environment variables to be used in the remainder of the tutorial. Set the USERNAME to a unique value shorter than 10 characters and set the IBMID to the email you used to create the IBM Cloud account. The other variables don't need to be changed but you can choose to rename them if you prefer. USERNAME=<bnewell> IBMID=<b.newell2@remkoh.dev> MY_REGION=us-south MY_ZONE=$MY_REGION-1 MY_VPC_NAME=$USERNAME-vpcgen2-vpc1 MY_VPC_SUBNET_NAME=$USERNAME-vpcsubnet1 MY_PUBLIC_GATEWAY=$USERNAME-public-gateway1 MY_CLUSTER_NAME=$USERNAME-iks118-vpc-cluster1 KS_VERSION=1.18 MY_NAMESPACE=my-guestbook Log in to the IBM Cloud account, if you use a Single Sign-On (SSO) provider, use the --sso flag instead of the username flag. ibmcloud login -u $IBMID [--sso] Target the region where you want to create your VPC environment. The resource group flag is optional, if you happen to know it, you can target it explicitly, but this tutorial does not use it. ibmcloud target -r $MY_REGION [-g <resource_group>] The VPC must be set up in the same multi-zone metro location where you want to create your cluster. Target generation 2 infrastructure. ibmcloud is target --gen 2","title":"Setup CLI"},{"location":"vpc/vpcgen2/#create-a-vpc-generation-2-compute-using-cli","text":"This section uses the IBM Cloud CLI to create a cluster in your VPC on generation 2 compute. Create a VPC, ibmcloud is vpc-create $MY_VPC_NAME ibmcloud is vpcs ID Name Status Classic access Default network ACL Default security group Resource group r006\u20133883b334\u20131f4e-4d6a-b3b7\u2013343a029d81bc remkohdev-vpcgen2-vpc1 available false prowling-prevail-universe-equivocal hatracks-fraying-unloader-prevail default Set an environment variable MY_VPC_ID , MY_VPC_ID=$(ibmcloud is vpcs --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_NAME\\\"') | .id ') echo $MY_VPC_ID Create a subnet ibmcloud is subnet-create $MY_VPC_SUBNET_NAME $MY_VPC_ID --zone $MY_ZONE --ipv4-address-count 256 Set an environment variable MY_VPC_SUBNET_ID , MY_VPC_SUBNET_ID=$(ibmcloud is subnets --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_SUBNET_NAME\\\"') | .id ') echo $MY_VPC_SUBNET_ID Inspect the VPC's default security group, set an environment variable MY_DEFAULT_SG_NAME and MY_DEFAULT_SG_ID , MY_DEFAULT_SG_NAME=$(ibmcloud is vpcs --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_NAME\\\"') | .default_security_group.name ') echo $MY_DEFAULT_SG_NAME MY_DEFAULT_SG_ID=$(ibmcloud is security-groups --output json | jq -r '.[] | select( .name=='\\\"$MY_DEFAULT_SG_NAME\\\"') | .id ') echo $MY_DEFAULT_SG_ID ibmcloud is security-group $MY_DEFAULT_SG_ID --output json { \"created_at\": \"2020-12-18T22:54:04.000Z\", \"crn\": \"crn:v1:bluemix:public:is:us-south:a/e65910fa61ce9072d64902d03f3d4774::security-group:r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/security_groups/r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"id\": \"r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"name\": \"hatracks-fraying-unloader-prevail\", \"network_interfaces\": [ { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/instances/0717_2ba757dd-0d40-48c6-b72e-6fb2ea12584a/network_interfaces/0717-57f73998-6af6-43e1-981f-698f021f4842\", \"id\": \"0717-57f73998-6af6-43e1-981f-698f021f4842\", \"name\": \"unpleased-urethane-recycled-subduing\", \"primary_ipv4_address\": \"10.240.0.4\", \"resource_type\": \"network_interface\" }, { \"deleted\": { \"more_info\": null }, \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/instances/0717_3affef2d-c4b7-49e0-a785-c65ad2f64a35/network_interfaces/0717-0dc82474-6b74-48e7-89e7-536994a45524\", \"id\": \"0717-0dc82474-6b74-48e7-89e7-536994a45524\", \"name\": \"sweep-timpani-unable-stricken\", \"primary_ipv4_address\": \"10.240.0.5\", \"resource_type\": \"network_interface\" } ], \"resource_group\": { \"href\": \"https://resource-controller.cloud.ibm.com/v2/resource_groups/fdd290732f7d47909181a189494e2990\", \"id\": \"fdd290732f7d47909181a189494e2990\", \"name\": \"default\" }, \"rules\": [ { \"direction\": \"outbound\", \"id\": \"r006-c0b88082-748c-47a8-bc75-c9aabb3da4e3\", \"ip_version\": \"ipv4\", \"protocol\": \"all\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } }, { \"direction\": \"inbound\", \"id\": \"r006-4693fcb0-2998-4d73-b62f-9dd80fe1ac56\", \"ip_version\": \"ipv4\", \"protocol\": \"all\", \"remote\": { \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/security_groups/r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"id\": \"r006-e201e996-c849-4d0c-ae41-97e4c303d219\", \"name\": \"hatracks-fraying-unloader-prevail\" } }, { \"direction\": \"inbound\", \"id\": \"r006-b0fb17d4-b858-4a1f-bb8c-24231f527155\", \"ip_version\": \"ipv4\", \"port_max\": 22, \"port_min\": 22, \"protocol\": \"tcp\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } }, { \"direction\": \"inbound\", \"id\": \"r006-87a1f11b-6d30-4d63-84df-1bda1c5e25f2\", \"ip_version\": \"ipv4\", \"protocol\": \"icmp\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" }, \"type\": 8 }, { \"direction\": \"inbound\", \"id\": \"r006-ade5f825-49e0-47bf-b6cf-b020b5fa23b8\", \"ip_version\": \"ipv4\", \"port_max\": 80, \"port_min\": 80, \"protocol\": \"tcp\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } } ], \"targets\": null, \"vpc\": { \"crn\": \"crn:v1:bluemix:public:is:us-south:a/e65910fa61ce9072d64902d03f3d4774::vpc:r006-3883b334-1f4e-4d6a-b3b7-343a029d81bc\", \"href\": \"https://us-south.iaas.cloud.ibm.com/v1/vpcs/r006-3883b334-1f4e-4d6a-b3b7-343a029d81bc\", \"id\": \"r006-3883b334-1f4e-4d6a-b3b7-343a029d81bc\", \"name\": \"remkohdev-vpcgen2-vpc1\" } } Create a public gateway, and update the subnet with the public gateway, this should create a floating IP for your public gateway attached to your subnet, ibmcloud is public-gateway-create $MY_PUBLIC_GATEWAY $MY_VPC_ID $MY_ZONE MY_PUBLIC_GATEWAY_ID=$(ibmcloud is public-gateways --output json | jq -r '.[] | select( .name=='\\\"$MY_PUBLIC_GATEWAY\\\"') | .id') echo $MY_PUBLIC_GATEWAY_ID ibmcloud is subnet-update $MY_VPC_SUBNET_ID --public-gateway-id $MY_PUBLIC_GATEWAY_ID MY_FLOATING_IP=$(ibmcloud is subnet-public-gateway $MY_VPC_SUBNET_ID --output json | jq -r '.floating_ip.address') echo $MY_FLOATING_IP Check that the public gateway is now attached to the subnet by retrieving the public gateway id from the subnet configuration, MY_PUBLIC_GATEWAY_ID2=$(ibmcloud is subnets --output json | jq -r '.[] | select( .name=='\\\"$MY_VPC_SUBNET_NAME\\\"') | .public_gateway.id ') echo $MY_PUBLIC_GATEWAY_ID2 To check the new infrastructure in the IBM Cloud dashboard, go to your subnets for VPC , Click the linked subnet name to view the new subnet details, You should see a public gateway attached with a floating IP.","title":"Create a VPC Generation 2 Compute using CLI"},{"location":"vpc/vpcgen2/#create-a-kubernetes-cluster","text":"Create a cluster in your VPC in the same zone as the subnet. By default, your cluster is created with a public and a private service endpoint. You can use the public service endpoint to access the Kubernetes master, We already defined environment variables for the region, zone and Kubernetes version, but start to check these are valid and available, ibmcloud ks zone ls --provider vpc-gen2 ibmcloud ks versions Create the cluster in the VPC, $ ibmcloud ks cluster create vpc-gen2 --name $MY_CLUSTER_NAME --zone $MY_ZONE --version $KS_VERSION --flavor bx2.2x8 --workers 1 --vpc-id $MY_VPC_ID --subnet-id $MY_VPC_SUBNET_ID Creating cluster... OK Cluster created with ID bvglln7d0e5j0u9lfa80 Review your IBM Cloud account resources, Click the linked cluster name of the cluster you just created. If you do not see the cluster listed yet, wait and refresh the page. Check the status of the new cluster, ibmcloud ks clusters MY_CLUSTER_ID = $( ibmcloud ks clusters --output json --provider vpc-gen2 | jq -r '.[] | select( .name==' \\\" $MY_CLUSTER_NAME \\\" ') | .id ' ) echo $MY_CLUSTER_ID To continue with the next step, the cluster status and the Ingress status must indicate to be available. The cluster might take about 15 minutes to complete. ibmcloud ks cluster get --cluster $MY_CLUSTER_ID Once the cluster is fully provisioned including the Ingress Application Load Balancer (ALB), you should see the Worker nodes status to be 100% Normal , the Ingress subdomain should be populated, among other, Or via the CLI, $ ibmcloud ks cluster get --cluster $MY_CLUSTER_ID Retrieving cluster c031jqqd0rll2ksfg97g... OK Name: bnewell2-iks118-vpc-cluster1 ID: c031jqqd0rll2ksfg97g State: normal Status: All Workers Normal Created: 2021 -01-18 23 :29:47 +0000 ( 16 hours ago ) Resource Group ID: 68af6383f717459686457a6434c4d19f Resource Group Name: Default Pod Subnet: 172 .17.0.0/18 Service Subnet: 172 .21.0.0/16 Workers: 1 Worker Zones: us-south-1 Ingress Subdomain: bnewell2-iks118-vpc-clu-47d7983a425e05fef831e694b7945b16-0000.us-south.containers.appdomain.cloud Ingress Secret: bnewell2-iks118-vpc-clu-47d7983a425e05fef831e694b7945b16-0000 Ingress Status: warning Ingress Message: One or more ALBs are unhealthy. See http://ibm.biz/ingress-alb-check Creator: - Public Service Endpoint URL: https://c111.us-south.containers.cloud.ibm.com:31097 Private Service Endpoint URL: https://c111.private.us-south.containers.cloud.ibm.com:31097 Pull Secrets: enabled in the default namespace VPCs: r006-3c9ab19c-e8af-4eb3-ab60-b9777c3cce1c Master Status: Ready ( 15 hours ago ) State: deployed Health: normal Version: 1 .18.14_1537 Location: Dallas URL: https://c111.us-south.containers.cloud.ibm.com:31097 You can also check the status of the Application Load Balancer (ALB), $ ibmcloud ks ingress alb ls --cluster $MY_CLUSTER_ID OK ALB ID Enabled State Type Load Balancer Hostname Zone Build Status private-crc031jqqd0rll2ksfg97g-alb1 false disabled private - us-south-1 ingress:/ingress-auth: - public-crc031jqqd0rll2ksfg97g-alb1 true enabled public f128a85f-us-south.lb.appdomain.cloud us-south-1 ingress:0.35.0_869_iks/ingress-auth: -","title":"Create a Kubernetes Cluster"},{"location":"vpc/vpcgen2/#deploy-the-guestbook-application","text":"Now the cluster is fully provisioned successfully, you can connect to your cluster and set the current-context of the kubectl , ibmcloud ks cluster config --cluster $MY_CLUSTER_ID kubectl config current-context If you have multiple configuration and contexts, you can easily switch between contexts, kubectl config use-context $MY_CLUSTER_NAME / $MY_CLUSTER_ID Deploy the guestbook application, kubectl create namespace $MY_NAMESPACE kubectl create deployment guestbook --image=ibmcom/guestbook:v1 -n $MY_NAMESPACE kubectl expose deployment guestbook --type=\"LoadBalancer\" --port=3000 --target-port=3000 -n $MY_NAMESPACE List the created service for guestbook, $ kubectl get svc -n $MY_NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook LoadBalancer 172 .21.48.26 7a6a66a7-us-south.lb.appdomain.cloud 3000 :32219/TCP 31m Create environment variables for the public IP address of the LoadBalancer service, the NodePort , and the port, SVC_EXTERNAL_IP = $( kubectl get svc -n $MY_NAMESPACE --output json | jq -r '.items[] | .status.loadBalancer.ingress[0].hostname ' ) echo $SVC_EXTERNAL_IP SVC_NODEPORT = $( kubectl get svc -n $MY_NAMESPACE --output json | jq -r '.items[].spec.ports[] | .nodePort' ) echo $SVC_NODEPORT SVC_PORT = $( kubectl get svc -n $MY_NAMESPACE --output json | jq -r '.items[].spec.ports[] | .port' ) echo $SVC_PORT Note the external IP address is set to the hostname of one of two Load Balancers for VPC . See the Load balancers for VPC in the IBM Cloud VPC Infrastructure listing. Try to send a request to the guestbook application, curl http:// $SVC_EXTERNAL_IP : $SVC_PORT curl: ( 52 ) Empty reply from server Even if you have created a LoadBalancer service with an external IP, the service cannot be reached because the VPC does not include a rule to allow incoming or ingress traffic to the service.","title":"Deploy the Guestbook Application"},{"location":"vpc/vpcgen2/#update-the-security-group","text":"To allow any traffic to applications that are deployed on your cluster's worker nodes, you have to modify the VPC's default security group by ID. Update the security group and add an inbound rule for the NodePort of the service you created when exposing the guestbook deployment. I only want to allow ingress traffic on the NodePort, so I set the minimum and maximum value of allowed inbound ports to the same NodePort value. $ ibmcloud is security-group-rule-add $MY_DEFAULT_SG_ID inbound tcp --port-min $SVC_NODEPORT --port-max $SVC_NODEPORT Creating rule for security group r006-b4f498ea-1e71-489e-95f0-0e64cf8d520f under account Remko de Knikker as user b.newell2@remkoh.dev... ID r006-26b196de-5e3a-4a7d-b5fb-9baf9173feb7 Direction inbound IP version ipv4 Protocol tcp Min destination port 32219 Max destination port 32219 Remote 0 .0.0.0/0 List all the security group rules, $ ibmcloud is security-group-rules $MY_DEFAULT_SG_ID Listing rules of security group r006-b4f498ea-1e71-489e-95f0-0e64cf8d520f under account Remko de Knikker as user b.newell2@remkoh.dev... ID Direction IP version Protocol Remote r006-6c281868-cfbd-46d1-b714-81e085ae2b85 outbound ipv4 all 0.0.0.0/0 r006-b07bbf77-84a0-4e35-90b7-5422f035cf1e inbound ipv4 all compacted-imprison-clinic-support r006-26b196de-5e3a-4a7d-b5fb-9baf9173feb7 inbound ipv4 tcp Ports:Min=32219,Max=32219 0.0.0.0/0 Or add a security group rule to allow inbound TCP traffic on all Kubernetes ports in the range of 30000\u201332767. Try again to reach the guestbook application, curl http://$SVC_EXTERNAL_IP:$SVC_PORT You should now be able to see the HTML response object from the Guestbook application. Open the guestbook URL in a browser to review the web page. echo http://$SVC_EXTERNAL_IP:$SVC_PORT If you want to understand better how the load balancing for VPC works, review the optional extra section Understanding the Load Balancer for VPC You can try removing the inbound rule again to check if the VPC rejects the request again, ibmcloud is security-group-rule-delete $MY_DEFAULT_SG_ID $MY_DEFAULT_SG_RULE_ID","title":"Update the Security\u00a0Group"},{"location":"vpc/vpcgen2/#conclusion","text":"You are awesome! You secured your Kubernetes cluster with a Virtual Private Cloud (VPC) and started to air-gap the cluster, blocking direct access to your cluster. Security is an important integral part of any software application development and airgapping your cluster by adding a VPC Generation 2 is a first step in securing your cluster, network and containers.","title":"Conclusion"},{"location":"vpc/vpcgen2/#cleanup","text":"To conclude, you can choose to delete your Kubernetes resources for this tutorial, $ ibmcloud ks cluster rm --cluster $MY_CLUSTER_NAME Do you want to delete the persistent storage for this cluster? If yes, the data cannot be recovered. If no, you can delete the persistent storage later in your IBM Cloud infrastructure account. [ y/N ] > y After you run this command, the cluster cannot be restored. Remove the cluster remkohdev-iks118-vpc-cluster1? [ y/N ] > y Removing cluster remkohdev-iks118-vpc-cluster1, persistent storage... OK Now the Kubernetes cluster is deleted, we need to first remove the public gateway and load balancers from the subnet, then remove the subnet from the VPC, and delete the VPC. Delete the Gateways, Load Balancers, Network Interfaces, subnet, public gateways, and finally delete the VPC, ibmcloud is security-group-rules $MY_DEFAULT_SG_ID --output json MY_DEFAULT_SG_RULE_ID = $( ibmcloud is security-group-rules $MY_DEFAULT_SG_ID --output json | jq -r --arg SVC_NODEPORT $SVC_NODEPORT '.[] | select( .port_max==($SVC_NODEPORT|tonumber)) | .id ' ) echo $MY_DEFAULT_SG_RULE_ID Delete the rule for the security group, $ ibmcloud is security-group-rule-delete $MY_DEFAULT_SG_ID $MY_DEFAULT_SG_RULE_ID This will delete security group rule r006-4f5f795f-fa90-44e4-b9c4-cb9457a2a421 and cannot be undone. Continue [ y/N ] ?> y Deleting rule r006-4f5f795f-fa90-44e4-b9c4-cb9457a2a421 from security group r006-db96d593-c224-497d-888c-03d84f6d8e98 under account Remko de Knikker as user b.newell2@remkoh.dev... OK Rule r006-4f5f795f-fa90-44e4-b9c4-cb9457a2a421 is deleted. Detach the public gateway from the subnet, $ ibmcloud is subnet-public-gateway-detach $MY_VPC_SUBNET_ID Detaching public gateway from subnet 0717 -57ebaf2d-0de6-4630-af01-6cd84031b679 under account Remko de Knikker as user b.newell2@remkoh.dev... OK Public gateway is detached. Delete the public gateway, ibmcloud is public-gateways PUBLIC_GATEWAY_ID = $( ibmcloud is public-gateways --output json | jq '.[0]' | jq -r '.id' ) echo $PUBLIC_GATEWAY_ID $ ibmcloud is public-gateway-delete $PUBLIC_GATEWAY_ID This will delete public gateway r006-f4603b78-839b-42a5-949c-76403948821a and cannot be undone. Continue [ y/N ] ?> y Deleting public gateway r006-f4603b78-839b-42a5-949c-76403948821a under account Remko de Knikker as user b.newell2@remkoh.dev... OK Public gateway r006-f4603b78-839b-42a5-949c-76403948821a is deleted. Delete the subnet, $ ibmcloud is subnet-delete $MY_VPC_SUBNET_ID This will delete Subnet 0717 -57ebaf2d-0de6-4630-af01-6cd84031b679 and cannot be undone. Continue [ y/N ] ?> y Deleting subnet 0717 -57ebaf2d-0de6-4630-af01-6cd84031b679 under account Remko de Knikker as user b.newell2@remkoh.dev... OK Subnet 0717 -57ebaf2d-0de6-4630-af01-6cd84031b679 is deleted. Delete the Virtual Private Cloud, MY_VPC_ID = $( ibmcloud is vpcs --output json | jq -r '.[] | select( .name==' \\\" $MY_VPC_NAME \\\" ') | .id ' ) echo $MY_VPC_ID $ ibmcloud is vpc-delete $MY_VPC_IDThis will delete vpc r006-3c9ab19c-e8af-4eb3-ab60-b9777c3cce1c and cannot be undone. Continue [ y/N ] ?> y Deleting vpc r006-3c9ab19c-e8af-4eb3-ab60-b9777c3cce1c under account Remko de Knikker as user b.newell2@remkoh.dev... OK vpc r006-3c9ab19c-e8af-4eb3-ab60-b9777c3cce1c is deleted.","title":"Cleanup"},{"location":"vpe/","text":"VPE \u00b6 Links: Recipes > Tutorials Leveraging Virtual Private Endpoint in IBM Cloud VPC to Connect IBM Cloud Object Storage Configuring VPC Subnets About virtual private endpoints gateways https://cloud.ibm.com/docs/vpc?topic=vpc-about-vpe https://cloud.ibm.com/docs/dns-svcs?topic=dns-svcs-frequently-asked-questions https://cloud.ibm.com/docs/vpc?topic=vpc-vpe-iam","title":"VPE"},{"location":"vpe/#vpe","text":"Links: Recipes > Tutorials Leveraging Virtual Private Endpoint in IBM Cloud VPC to Connect IBM Cloud Object Storage Configuring VPC Subnets About virtual private endpoints gateways https://cloud.ibm.com/docs/vpc?topic=vpc-about-vpe https://cloud.ibm.com/docs/dns-svcs?topic=dns-svcs-frequently-asked-questions https://cloud.ibm.com/docs/vpc?topic=vpc-vpe-iam","title":"VPE"}]}